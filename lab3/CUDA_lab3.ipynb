{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":["F9PmBZql0ow4","5gUDpbz5TZml","ygwWcMU9DJmG","IkYKd9J32ewH","SOFMQZAkjlLW"],"mount_file_id":"1PD8axMJPIW19SKS0LZUlg-Ps3b7OlikM","authorship_tag":"ABX9TyO4XcJ+KSBJOq/lj9UOQENk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["---\n","# **LAB 3 - Modello di esecuzione CUDA**\n","---"],"metadata":{"id":"fZYqN0UwVLC_"}},{"cell_type":"markdown","metadata":{"id":"F9PmBZql0ow4"},"source":["# ▶️ CUDA setup"]},{"cell_type":"code","metadata":{"id":"p9RIwaPbVQHV"},"source":["!nvcc --version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n5YlC1IOTlNb"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["GPU computing notebooks download (from github)"],"metadata":{"id":"gcg1GyK5srek"}},{"cell_type":"code","source":["!git clone https://github.com/giulianogrossi/GPUcomputing.git"],"metadata":{"id":"tyHOxci3s3H8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"227eLdP5csN1"},"source":["NVCC Plugin for Jupyter notebook"]},{"cell_type":"code","source":["%cd GPUcomputing/utils/nvcc4jupyter-master/\n","!python3 setup.py install\n","%load_ext nvcc4jupyter\n","%cd /content/"],"metadata":{"id":"4TzxMBFds8aT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5gUDpbz5TZml"},"source":["# ✅ Divergence analysis"]},{"cell_type":"code","metadata":{"id":"RlbVvBaXCHBs"},"source":["%%cuda_group_save --name \"div.cu\" --group \"DIV\"\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <assert.h>\n","#include \"/content/GPUcomputing/utils/common.h\"\n","\n","/*\n"," * Kernel with warp divergence\n"," */\n","__global__ void evenOddDIV(int *c, const ulong N) {\n","\tulong tid = blockIdx.x * blockDim.x + threadIdx.x;\n","\tint a, b;\n","\n","\tif (!(tid % 2))   // branch divergence\n","\t\ta = 2;\n","\telse\n","\t\tb = 1;\n","\n","\t// check index\n","\tif (tid < N)\n","\t\tc[tid] = a + b;\n","}\n","\n","/*\n"," * MAIN\n"," */\n","int main(int argc, char **argv) {\n","\n","\t// set up data size\n","\tint blocksize = 1024;\n","\tulong size = 1024*1024;\n","\n","\tif (argc > 1)\n","\t\tblocksize = atoi(argv[1]);\n","\tif (argc > 2)\n","\t\tsize = atoi(argv[2]);\n","\tulong nBytes = size * sizeof(int);\n","\n","\tprintf(\"Data size: %lu  -- \", size);\n","  printf(\"Data size (bytes): %lu MB\\n\", nBytes/1000000);\n","\n","\t// set up execution configuration\n","\tdim3 block(blocksize, 1);\n","\tdim3 grid((size + block.x - 1) / block.x, 1);\n","\tprintf(\"Execution conf (block %d, grid %d)\\nKernels:\\n\", block.x, grid.x);\n","\n","\t// allocate memory\n","\tint *d_C, *C;\n","\tC = (int *) malloc(nBytes);\n","\tCHECK(cudaMalloc((void** )&d_C, nBytes));\n","\n","\t// run kernel 1\n","\tdouble iStart, iElaps;\n","\tiStart = seconds();\n","\tevenOddDIV<<<grid, block>>>(d_C, size);\n","\tCHECK(cudaDeviceSynchronize());\n","\tiElaps = seconds() - iStart;\n","\tprintf(\"\\tevenOddDIV<<<%d, %d>>> elapsed time %f sec \\n\\n\", grid.x, block.x, iElaps);\n","\tCHECK(cudaGetLastError());\n","\n","  CHECK(cudaMemcpy(C, d_C, nBytes, cudaMemcpyDeviceToHost));\n","\n","\tfree(C);\n","\t// free gpu memory and reset device\n","\tCHECK(cudaFree(d_C));\n","\tCHECK(cudaDeviceReset());\n","\treturn EXIT_SUCCESS;\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kMEGfjJMcX_e"},"source":["# Compilazione ed esecuzione\n","!nvcc -arch=sm_75 src/DIV/div.cu -o div\n","!./div 1024 20000000"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ncu  ./div"],"metadata":{"id":"foUbQ9g49kxg"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PdQqnuH-54Ie"},"source":["# Compilazione ed esecuzione versione di debug\n","!nvcc -arch=sm_75 -g -G src/DIV/div.cu -o div_deb\n","!./div_deb 1024 2000000000"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ncu ./div_deb"],"metadata":{"id":"wk_7bbil9EZQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ↘️ *`TODO...`*"],"metadata":{"id":"_NlWW_V5T1GT"}},{"cell_type":"markdown","source":["Introdurre nuovo kernel che eviti la divergenza a livello di warp.\n","\n","- usare/creare nuova indicizzazione a livello di warp\n","- applicare nuova indicizzazione preservando il risultato finale\n"],"metadata":{"id":"Q4kYsvVbT1GU"}},{"cell_type":"code","source":["%%cuda_group_save --name \"div.cu\" --group \"DIV\"\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <assert.h>\n","#include \"/content/GPUcomputing/utils/common.h\"\n","\n","/*\n"," * Kernel with warp divergence\n"," */\n","__global__ void evenOddDIV(int *c, const ulong N) {\n","\tulong tid = blockIdx.x * blockDim.x + threadIdx.x;\n","\tint a, b;\n","\n","\tif (!(tid % 2))   // branch divergence\n","\t\ta = 2;\n","\telse\n","\t\tb = 1;\n","\n","\t// check index\n","\tif (tid < N)\n","\t\tc[tid] = a + b;\n","}\n","\n","/*\n"," * Kernel without warp divergence\n"," */\n","__global__ void evenOddNODIV(int *c, const int N) {\n","\tint tid = blockIdx.x * blockDim.x + threadIdx.x;\n","\tint a = 0, b = 0;\n","\tunsigned int i, twoWarpSize = 2 * warpSize;\n","\n"," // warp index wid = 0,1,2,3,...\n","\tint wid = tid / warpSize;\n","\tif (!(wid % 2))\n","\t\ta = 2;                  // branch1: thread tid = 0-31, 64-95, ...\n","\telse\n","\t\tb = 1;                  // branch2: thread tid = 32-63, 96-127, ...\n","\n","\t// using wid index for even and odd\n","\tif (!(wid % 2))  // even\n","\t\ti = 2 * (tid % warpSize) + (tid / twoWarpSize) * twoWarpSize;\n","\telse            // odd\n","\t\ti = 2 * (tid % warpSize) + 1 + (tid / twoWarpSize) * twoWarpSize;\n","\n","\t// check index\n","\tif (i < N) {\n","\t\tc[i] = a + b;\n","\t}\n","}\n","\n","/*\n"," * MAIN\n"," */\n","int main(int argc, char **argv) {\n","\n","\t// set up data size\n","\tint blocksize = 1024;\n","\tulong size = 1024*1024;\n","\n","\tif (argc > 1)\n","\t\tblocksize = atoi(argv[1]);\n","\tif (argc > 2)\n","\t\tsize = atoi(argv[2]);\n","\tulong nBytes = size * sizeof(int);\n","\n","\tprintf(\"Data size: %lu  -- \", size);\n","  printf(\"Data size (bytes): %lu MB\\n\", nBytes/1000000);\n","\n","\t// set up execution configuration\n","\tdim3 block(blocksize, 1);\n","\tdim3 grid((size + block.x - 1) / block.x, 1);\n","\tprintf(\"Execution conf (block %d, grid %d)\\nKernels:\\n\", block.x, grid.x);\n","\n","\t// allocate memory\n","\tint *d_C, *C;\n","\tC = (int *) malloc(nBytes);\n","\tCHECK(cudaMalloc((void** )&d_C, nBytes));\n","\n","\t// run kernel 1\n","\tdouble iStart, iElaps;\n","\tiStart = seconds();\n","\tevenOddDIV<<<grid, block>>>(d_C, size);\n","\tCHECK(cudaDeviceSynchronize());\n","\tiElaps = seconds() - iStart;\n","\tprintf(\"\\tevenOddDIV<<<%d, %d>>> elapsed time %f sec \\n\\n\", grid.x, block.x, iElaps);\n","\tCHECK(cudaGetLastError());\n","\n","  CHECK(cudaMemcpy(C, d_C, nBytes, cudaMemcpyDeviceToHost));\n","\n","\n","\t// run kernel 2\n","  CHECK(cudaMemset(d_C, 0.0, nBytes)); // reset memory\n","\tiStart = seconds();\n","\tevenOddNODIV<<<grid, block>>>(d_C, size);\n","\tiElaps = seconds() - iStart;\n","\tprintf(\"\\tevenOddNODIV<<<%d, %d>>> elapsed time %f sec \\n\\n\", grid.x, block.x, iElaps);\n","\tCHECK(cudaGetLastError());\n","\n","\tCHECK(cudaMemcpy(C, d_C, nBytes, cudaMemcpyDeviceToHost));\n","\n","\tfree(C);\n","\t// free gpu memory and reset device\n","\tCHECK(cudaFree(d_C));\n","\tCHECK(cudaDeviceReset());\n","\treturn EXIT_SUCCESS;\n","}\n"],"metadata":{"id":"NEQ9LoktUA3J"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZflE_OqGU75Q"},"source":["# Compilazione ed esecuzione\n","!nvcc -arch=sm_75 src/DIV/div.cu -o div\n","!./div 1024 20000000"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ygwWcMU9DJmG"},"source":["# ✅ Parallel Reduction"]},{"cell_type":"code","metadata":{"id":"WThhkz6GDMsm"},"source":["%%cuda_group_save --name \"preduce.cu\" --group \"PAR\"\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <assert.h>\n","\n","#include \"/content/GPUcomputing/utils/common.h\"\n","\n","\n","/*\n"," *  Block by block parallel implementation with divergence (sequential schema)\n"," */\n","__global__ void blockParReduce1(int *in, int *out, ulong n) {\n","\n","\tuint tid = threadIdx.x;\n","\tulong idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// boundary check\n","\tif (idx >= n)\n","\t\treturn;\n","\n","\t// convert global data pointer to the local pointer of this block\n","\tint *thisBlock = in + blockIdx.x * blockDim.x;\n","\n","\t// in-place reduction in global memory\n","\tfor (int stride = 1; stride < blockDim.x; stride *= 2) {\n","\t\tif ((tid % (2 * stride)) == 0)\n","\t\t\tthisBlock[tid] += thisBlock[tid + stride];\n","\n","\t\t// synchronize within threadblock\n","\t\t__syncthreads();\n","\t}\n","\n","\t// write result for this block to global mem\n","\tif (tid == 0)\n","\t\tout[blockIdx.x] = thisBlock[0];\n","}\n","\n","\n","\n","/*\n"," * MAIN: test on parallel reduction\n"," */\n","int main(void) {\n","\tint *a, *b, *d_a, *d_b;\n","\tint blockSize = 1024;            // block dim 1D\n","\tulong numBlock = 1024*1024;      // grid dim 1D\n","\tulong n = blockSize * numBlock;  // array dim\n","\tlong sum_CPU = 0, sum_GPU;\n","\tlong nByte = n*sizeof(int), mByte = numBlock * sizeof(int);\n","\tdouble start, stopGPU, stopCPU, speedup;\n","\n","\tprintf(\"\\n****  test on parallel reduction  ****\\n\");\n","\n","\t// init\n","\ta = (int *) malloc(nByte);\n","\tb = (int *) malloc(mByte);\n","\tfor (ulong i = 0; i < n; i++) a[i] = 1;\n","\n","\tCHECK(cudaMalloc((void **) &d_a, nByte));\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMalloc((void **) &d_b, mByte));\n","\tCHECK(cudaMemset((void *) d_b, 0, mByte));\n","\n","\t/***********************************************************/\n","\t/*                     CPU reduction                       */\n","\t/***********************************************************/\n","\tprintf(\"  Vector length: %.2f MB\\n\",n/(1024.0*1024.0));\n","\tprintf(\"\\n  CPU procedure...\\n\");\n","\tstart = seconds();\n","\tfor (ulong i = 0; i < n; i++)\n","    sum_CPU += a[i];\n","\tstopCPU = seconds() - start;\n","\tprintf(\"    Elapsed time: %f (sec) \\n\", stopCPU);\n","\tprintf(\"    sum: %lu\\n\",sum_CPU);\n","\n","\tprintf(\"\\n  GPU kernels (mem required %lu bytes)\\n\", nByte);\n","\n","\t/***********************************************************/\n","\t/*         KERNEL blockParReduce1 (divergent)              */\n","\t/***********************************************************/\n","\t// block by block parallel implementation with divergence\n","\tprintf(\"\\n  Launch kernel: blockParReduce1...\\n\");\n","\tstart = seconds();\n","\tblockParReduce1<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaGetLastError());\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n","\n","  // memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\n","  // check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++)\n","\t\tsum_GPU += b[i];\n","\tassert(sum_GPU == n);\n","\n","\t// reset input vector on GPU\n","\tfor (ulong i = 0; i < n; i++) a[i]=1;\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\n","\n","\tcudaFree(d_a);\n","\n","\tCHECK(cudaDeviceReset());\n","\treturn 0;\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pIOextPZMiav"},"source":["#Compilazione ed esecuzione\n","\n","!nvcc -arch=sm_75 src/PAR/preduce.cu -o preduce\n","!./preduce"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ↘️ *`TODO...`*"],"metadata":{"id":"VX7H_0JiBx2j"}},{"cell_type":"markdown","source":["Kernel privo di divergenza:\n","\n","* Usare lo schema che suddivide in blocchi (richiesta sincronizzazione)\n","* Sommare su ogni blocco con parallel reduction (somma parziale)\n","* Utilizzare uno schema interlacciato\n","* Evitare la divergenza nella parallel reduction\n","* Unire le somme parziali dei blocchi\n","\n","\n"],"metadata":{"id":"rMXU-CfpCCep"}},{"cell_type":"code","metadata":{"id":"B5QUNJdYauND"},"source":["%%cuda_group_save --name \"preduce.cu\" --group \"PAR\"\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <assert.h>\n","#include \"/content/GPUcomputing/utils/common.h\"\n","\n","\n","/*\n"," *  Block by block parallel implementation with divergence (sequential schema)\n"," */\n","__global__ void blockParReduce1(int *in, int *out, ulong n) {\n","\n","\tuint tid = threadIdx.x;\n","\tulong idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// boundary check\n","\tif (idx >= n)\n","\t\treturn;\n","\n","\t// convert global data pointer to the local pointer of this block\n","\tint *thisBlock = in + blockIdx.x * blockDim.x;\n","\n","\t// in-place reduction in global memory\n","\tfor (int stride = 1; stride < blockDim.x; stride *= 2) {\n","\t\tif ((tid % (2 * stride)) == 0)\n","\t\t\tthisBlock[tid] += thisBlock[tid + stride];\n","\n","\t\t// synchronize within threadblock\n","\t\t__syncthreads();\n","\t}\n","\n","\t// write result for this block to global mem\n","\tif (tid == 0)\n","\t\tout[blockIdx.x] = thisBlock[0];\n","}\n","\n","/*\n"," *  Block by block parallel implementation without divergence (interleaved schema)\n"," */\n","__global__ void blockParReduce2(int *in, int *out, ulong n) {\n","\n","\tuint tid = threadIdx.x;\n","\tulong idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// boundary check\n","\tif (idx >= n)\n","\t\treturn;\n","\n","\t// convert global data pointer to the local pointer of this block\n","\tint *thisBlock = in + blockIdx.x * blockDim.x;\n","\n","\t// in-place reduction in global memory\n","\tfor (int stride = blockDim.x / 2; stride > 0; stride >>= 1)  {\n","\t\tif (tid < stride)\n","\t\t\tthisBlock[tid] += thisBlock[tid + stride];\n","\n","\t\t// synchronize within threadblock\n","\t\t__syncthreads();\n","\t}\n","\n","\t// write result for this block to global mem\n","\tif (tid == 0)\n","\t\tout[blockIdx.x] = thisBlock[0];\n","}\n","\n","\n","/*\n"," * MAIN: test on parallel reduction\n"," */\n","int main(void) {\n","\tint *a, *b, *d_a, *d_b;\n","\tint blockSize = 1024;            // block dim 1D\n","\tulong numBlock = 1024*1024;      // grid dim 1D\n","\tulong n = blockSize * numBlock;  // array dim\n","\tlong sum_CPU = 0, sum_GPU;\n","\tlong nByte = n*sizeof(int), mByte = numBlock * sizeof(int);\n","\tdouble start, stopGPU, stopCPU, speedup;\n","\n","\tprintf(\"\\n****  test on parallel reduction  ****\\n\");\n","\n","\t// init\n","\ta = (int *) malloc(nByte);\n","\tb = (int *) malloc(mByte);\n","\tfor (ulong i = 0; i < n; i++) a[i] = 1;\n","\n","\tCHECK(cudaMalloc((void **) &d_a, nByte));\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMalloc((void **) &d_b, mByte));\n","\tCHECK(cudaMemset((void *) d_b, 0, mByte));\n","\n","\t/***********************************************************/\n","\t/*                     CPU reduction                       */\n","\t/***********************************************************/\n","\tprintf(\"  Vector length: %.2f MB\\n\",n/(1024.0*1024.0));\n","\tprintf(\"\\n  CPU procedure...\\n\");\n","\tstart = seconds();\n","\tfor (ulong i = 0; i < n; i++)\n","    sum_CPU += a[i];\n","\tstopCPU = seconds() - start;\n","\tprintf(\"    Elapsed time: %f (sec) \\n\", stopCPU);\n","\tprintf(\"    sum: %lu\\n\",sum_CPU);\n","\n","\tprintf(\"\\n  GPU kernels (mem required %lu bytes)\\n\", nByte);\n","\n","\t/***********************************************************/\n","\t/*         KERNEL blockParReduce1 (divergent)              */\n","\t/***********************************************************/\n","\t// block by block parallel implementation with divergence\n","\tprintf(\"\\n  Launch kernel: blockParReduce1...\\n\");\n","\tstart = seconds();\n","\tblockParReduce1<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaGetLastError());\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n","\n","  // memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\n","  // check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++)\n","\t\tsum_GPU += b[i];\n","\tassert(sum_GPU == n);\n","\n","\t// reset input vector on GPU\n","\tfor (ulong i = 0; i < n; i++) a[i]=1;\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\n","\t/***********************************************************/\n","\t/*        KERNEL blockParReduce2  (non divergent)          */\n","\t/***********************************************************/\n","\t// block by block parallel implementation without divergence\n","\tprintf(\"\\n  Launch kernel: blockParReduce2...\\n\");\n","\tstart = seconds();\n","\tblockParReduce2<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n","\tCHECK(cudaGetLastError());\n","\n","  // memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\n","  // check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++) {\n","\t\tsum_GPU += b[i];\n","  //\t\tprintf(\"b[%d] = %d\\n\",i,b[i]);\n","\t}\n","\tassert(sum_GPU == n);\n","\n","  // reset input vector on GPU\n","\tfor (ulong i = 0; i < n; i++) a[i] = 1;\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\n","\t// check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++)\n","\t\tsum_GPU += b[i];\n","\tassert(sum_GPU == n);\n","\n","\tcudaFree(d_a);\n","\n","\tCHECK(cudaDeviceReset());\n","\treturn 0;\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Is7vbYHauNM"},"source":["#Compilazione ed esecuzione\n","\n","!nvcc -arch=sm_75 src/PAR/preduce.cu -o preduce\n","!./preduce"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IkYKd9J32ewH"},"source":["# ✅ Istogramma di un'immagine BMP"]},{"cell_type":"markdown","source":["Calcolare l'istogramma di un aimmagine BMP con uso di `atomicAdd`"],"metadata":{"id":"vfkxkSqmCmAD"}},{"cell_type":"markdown","source":["### ↘️ *`TODO...`*"],"metadata":{"id":"Dg9BHTaWClBj"}},{"cell_type":"code","metadata":{"id":"G18uZY3t2kFp"},"source":["%%cuda_group_save --name \"hist.cu\" --group \"HST\"\n","\n","#include <cuda_runtime.h>\n","#include <stdio.h>\n","#include <time.h>\n","#include <limits.h>\n","#include \"/content/GPUcomputing/utils/common.h\"\n","#include \"/content/GPUcomputing/utils/BMP/ImageStuff.h\"\n","#include \"/content/GPUcomputing/utils/BMP/bmpUtil.h\"\n","\n","/*\n"," * Kernel 1D that computes histogram on GPU\n"," */\n","__global__ void histogramBMP(uint *bins, const pel *imgSrc, const uint W, const uint N, const uint M) {\n","\t// ** pixel granularity **\n","\tuint x = blockDim.x * blockIdx.x + threadIdx.x; // 1D pixel linear index over [0:W*H)\n","\n","\t// num of rows to skip\n","\tuint nrows = x / W;\n","\n","\t// offset (= col) within current row\n","\tuint off = x % W;\n","\n","  // pixel out of range\n","\tif (x >= N)\n","\t\treturn;\n","\n","\t// byte position of the pixel\n","\tuint p = M * nrows + 3*off;\n","\tpel R = imgSrc[p];\n","\tpel G = imgSrc[p+1];\n","\tpel B = imgSrc[p+2];\n","\n","\t// use atomic\n","\tatomicAdd(&bins[R], 1);\n","\tatomicAdd(&bins[G+256], 1);\n","\tatomicAdd(&bins[B+512], 1);\n","}\n","\n","/*\n"," * Function that computes histogram on CPU\n"," */\n","void hist_CPU(uint *bins, const pel *imgSrc, const uint W, const uint H, const uint M) {\n","\tfor (int i = 0; i < W*H; i++) {\n","\t\tuint r = i / W;              // row of the source pixel\n","\t\tuint off = i - r * W;        // col of the source pixel\n","\n","\t\t// byte granularity\n","\t\tuint p = M * r + 3*off;      // src byte position of the pixel\n","\t\tpel R = imgSrc[p];\n","\t\tpel G = imgSrc[p+1];\n","\t\tpel B = imgSrc[p+2];\n","\t\tbins[R] += 1;\n","\t\tbins[G+256] += 1;\n","\t\tbins[B+512] += 1;\n","\t}\n","}\n","\n","int main(int argc, char **argv) {\n","\n","\tuint dimBlock = 1024;\n","\tpel *imgBMP_CPU;     // Where images are stored in CPU\n","\tpel *imgBMP_GPU;\t // Where images are stored in GPU\n","\n","\tuint *binsRGB_CPU, *binsRGB_GPU, *binsRGB_GPU2CPU;\n","\tuint N_bins = 3*256;\n","\tuint bin_size = N_bins*sizeof(uint);\n","\n","\tif (argc > 2)\n","\t\tdimBlock = atoi(argv[2]);\n","\telse if (argc < 2) {\n","\t\tprintf(\"\\n\\nUsage:  hist InputFilename dimBlock\\n\");\n","\t\texit(EXIT_FAILURE);\n","\t}\n","\n","\t// bins for CPU & GPU\n","\tbinsRGB_CPU = (uint*) calloc(N_bins, sizeof(uint));\n","\tbinsRGB_GPU2CPU = (uint*) malloc(bin_size);\n","\tCHECK(cudaMalloc((void**) &binsRGB_GPU, bin_size));\n","\n","\t// Create CPU memory to store the input image\n","\timgBMP_CPU = ReadBMPlin(argv[1]);\n","\tif (imgBMP_CPU == NULL) {\n","\t\tprintf(\"Cannot allocate memory for the input image...\\n\");\n","\t\texit(EXIT_FAILURE);\n","\t}\n","\n","\t// Allocate GPU buffer for image and bins\n","\tCHECK(cudaMalloc((void**) &imgBMP_GPU, IMAGESIZE));\n","\n","\t// Copy input vectors from host memory to GPU buffers.\n","\tCHECK(cudaMemcpy(imgBMP_GPU, imgBMP_CPU, IMAGESIZE, cudaMemcpyHostToDevice));\n","\n","\t// CPU histogram\n","\tdouble start = seconds();   // start time\n","\thist_CPU(binsRGB_CPU, imgBMP_CPU, WIDTH, HEIGHT, WIDTHB);\n","\tdouble stop = seconds();   // elapsed time\n","\tprintf(\"\\nCPU elapsed time %f sec \\n\\n\", stop - start);\n","\n","\t// invoke kernels (define grid and block sizes)\n","\tuint nPixels = WIDTH*HEIGHT;\n","\tint dimGrid = (nPixels + dimBlock - 1) / dimBlock;\n","\tprintf(\"\\ndimGrid = %d   dimBlock = %d\\n\",dimGrid,dimBlock);\n","\n","\tstart = seconds();   // start time\n","\thistogramBMP<<<dimGrid, dimBlock>>>(binsRGB_GPU, imgBMP_GPU, WIDTH, nPixels, WIDTHB);\n","\tCHECK(cudaDeviceSynchronize());\n","\tstop = seconds();   // elapsed time\n","\tprintf(\"\\nGPU elapsed time %f sec \\n\\n\", stop - start);\n","\n","\t// Copy output (results) from GPU buffer to host (CPU) memory.\n","\tCHECK(cudaMemcpy(binsRGB_GPU2CPU, binsRGB_GPU, bin_size, cudaMemcpyDeviceToHost));\n","\n","\tfor (int i = 0; i < N_bins/3; i++)\n","\t\tprintf(\"bin_GPU[%d] = \\t%d\\t%d\\t%d\\t -- bin_CPU[%d] = \\t%d\\t%d\\t%d\\n\", i,\n","\t\t\t\tbinsRGB_GPU2CPU[i],binsRGB_GPU2CPU[i+256],binsRGB_GPU2CPU[i+512],\n","\t\t\t\ti,binsRGB_CPU[i],binsRGB_CPU[i+256],binsRGB_CPU[i+512]);\n","\n","\t// Deallocate GPU memory\n","\tcudaFree(imgBMP_GPU);\n","\tcudaFree(binsRGB_GPU);\n","\n","\t// tracing tools spel as Parallel Nsight and Visual Profiler to show complete traces.\n","\tCHECK(cudaDeviceReset());\n","\n","\treturn (EXIT_SUCCESS);\n","}\n","\n","/*\n"," *  Read a 24-bit/pixel BMP file into a 1D linear array.\n"," *  Allocate memory to store the 1D image and return its pointer\n"," */\n","pel *ReadBMPlin(char* fn) {\n","\tstatic pel *Img;\n","\tFILE* f = fopen(fn, \"rb\");\n","\tif (f == NULL) {\n","\t\tprintf(\"\\n\\n%s NOT FOUND\\n\\n\", fn);\n","\t\texit(EXIT_FAILURE);\n","\t}\n","\n","\tpel HeaderInfo[54];\n","\tsize_t nByte = fread(HeaderInfo, sizeof(pel), 54, f); // read the 54-byte header\n","\t// extract image height and width from header\n","\tint width = *(int*) &HeaderInfo[18];\n","\timg.width = width;\n","\tint height = *(int*) &HeaderInfo[22];\n","\timg.height = height;\n","\tint RowBytes = (width * 3 + 3) & (~3);  // row is multiple of 4 pixel\n","\timg.rowByte = RowBytes;\n","\t//save header for re-use\n","\tmemcpy(img.headInfo, HeaderInfo, 54);\n","\tprintf(\"\\n Input File name: %5s  (%d x %d)   File Size=%lu\", fn, img.width, img.height, IMAGESIZE);\n","\n","\t// allocate memory to store the main image (1 Dimensional array)\n","\tImg = (pel *) malloc(IMAGESIZE);\n","\tif (Img == NULL)\n","\t\treturn Img;      // Cannot allocate memory\n","\t// read the image from disk\n","\tsize_t out = fread(Img, sizeof(pel), IMAGESIZE, f);\n","\tfclose(f);\n","\treturn Img;\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FvvbqpV-3gLH"},"source":["# Compilazione ed esecuzione\n","\n","!nvcc -arch=sm_75 src/HST/hist.cu /content/GPUcomputing/utils/BMP/ImageStuff.c -o hist\n","!./hist /content/GPUcomputing/images/dog.bmp 256"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SOFMQZAkjlLW"},"source":["# ✅ Prodotto MQDB CUDA"]},{"cell_type":"markdown","source":["### ↘️ *`TODO...`*"],"metadata":{"id":"9QunXrnuC8AA"}},{"cell_type":"markdown","source":["Calcolare il prodotto di matrici MQDB con kernel CUDA"],"metadata":{"id":"lY5KTfZaC82S"}},{"cell_type":"code","metadata":{"id":"QVQVpcvKjkIk"},"source":["%%cuda_group_save --name \"mqdb_prod.cu\" --group \"MQDB\"\n","\n","#include \"/content/GPUcomputing/utils/common.h\"\n","#include \"/content/GPUcomputing/utils/MQDB/mqdb.h\"\n","\n","#define BLOCK_SIZE 16     // block size\n","\n","struct tms {\n","\tdouble CPUtms;\n","\tdouble GPUtmsNaive;\n","\tdouble GPUtmsMQDB;\n","\tfloat density;\n","};\n","\n","/*\n"," * Kernel for standard (naive) matrix product\n"," */\n","__global__ void matProd(mqdb A, mqdb B, mqdb C, int n) {\n","\t// row & col indexes\n","\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n","\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// each thread computes an entry of the product matrix\n","\tif ((row < n) && (col < n)) {\n","\t\tfloat val = 0;\n","\t\tfor (int k = 0; k < n; k++)\n","\t\t\tval += A.elem[row * n + k] * B.elem[k * n + col];\n","\t\tC.elem[row * n + col] = val;\n","\t}\n","}\n","\n","/*\n"," * Kernel for block sub-matrix product of mqdb\n"," */\n","__global__ void mqdbBlockProd(mqdb A, mqdb B, mqdb C, int sdim, int d, int n) {\n","\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n","\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// jump to the right block sub-matrix\n","\tint  offset = (n+1)*sdim;\n","\n","\t// each thread computes an entry of the product matrix\n","\tif ((row < d) && (col < d)) {\n","\t\tfloat val = 0;\n","\n","\t\tfor (int k = 0; k < d; k++)\n","\t\t\tval += A.elem[row * n + k + offset] * B.elem[k * n + col + offset];\n","\t\tC.elem[row * n + col + offset] = val;\n","\t}\n","}\n","\n","/*\n"," * Test on MQDB kernels\n"," */\n","void testKernelsMQDB(uint n, uint k, struct tms* times) {\n","\n","\t// mqdb host matrices\n","\tmqdb A, B, C, C1;\n","\n","\t// mqdb device matrices\n","\tmqdb d_A, d_B, d_C;\n","\n","\t// fill in\n","\tA = mqdbConst(n, k, 10, 1);\n","\tB = mqdbConst(n, k, 10, 1);\n","\tC = mqdbConst(n, k, 10, 1);\n","\tC1 = mqdbConst(n, k, 10, 1);\n","\n","\tulong nBytes = n * n * sizeof(float);\n","\tulong kBytes = k * sizeof(uint);\n","\tprintf(\"        Memory size required = %.1f (MB)\\n\",(float)nBytes/(1024.0*1024.0));\n","\n","\t// malloc and copy on device memory\n","\td_A.nBlocks = A.nBlocks;\n","\tCHECK(cudaMalloc((void**)&d_A.blkSize, kBytes));\n","\tCHECK(cudaMemcpy(d_A.blkSize, A.blkSize, kBytes, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMalloc((void**)&d_A.elem, nBytes));\n","\tCHECK(cudaMemcpy(d_A.elem, A.elem, nBytes, cudaMemcpyHostToDevice));\n","\td_B.nBlocks = B.nBlocks;\n","\tCHECK(cudaMalloc((void**)&d_B.blkSize, kBytes));\n","\tCHECK(cudaMemcpy(d_B.blkSize, B.blkSize, kBytes, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMalloc((void**)&d_B.elem, nBytes));\n","\tCHECK(cudaMemcpy(d_B.elem, B.elem, nBytes, cudaMemcpyHostToDevice));\n","\td_C.nBlocks = C.nBlocks;\n","\tCHECK(cudaMalloc((void**)&d_C.blkSize, kBytes));\n","\tCHECK(cudaMemcpy(d_C.blkSize, C.blkSize, kBytes, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMalloc((void**)&d_C.elem, nBytes));\n","\tCHECK(cudaMemset(d_C.elem, 0.0, nBytes));\n","\n","\t/***********************************************************/\n","\t/*                    CPU MQDB product                     */\n","\t/***********************************************************/\n","\tprintf(\"\\nCPU MQDB product...\\n\");\n","\tdouble start = seconds();\n","\tmqdbProd(A,B,C);\n","\tdouble CPUTime = seconds() - start;\n","\tprintf(\"   CPU elapsed time: %.5f (sec)\\n\\n\", CPUTime);\n","\n","\t/***********************************************************/\n","\t/*                     GPU mat product                     */\n","\t/***********************************************************/\n","\tprintf(\"Kernel (naive) mat product...\\n\");\n","\tdim3 block(BLOCK_SIZE, BLOCK_SIZE);\n","\tdim3 grid((n + block.x - 1) / block.x, (n + block.y - 1) / block.y);\n","\tstart = seconds();\n","\tmatProd<<<grid, block>>>(d_A, d_B, d_C, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tdouble GPUtime1 = seconds() - start;\n","\tprintf(\"   elapsed time:                %.4f (sec)\\n\", GPUtime1);\n","\tprintf(\"   speedup vs CPU MQDB product: %.4f\\n\", CPUTime/GPUtime1);\n","\tCHECK(cudaMemcpy(C1.elem, d_C.elem, nBytes, cudaMemcpyDeviceToHost));\n","\tCHECK(cudaMemset(d_C.elem, 0.0, nBytes));\n","\tcheckResult(C,C1);\n","\t//\tmqdbDisplay(C1);\n","\n","\t/***********************************************************/\n","\t/*                     GPU MQDB product                    */\n","\t/***********************************************************/\n","\tprintf(\"Kernel MQDB product...\\n\");\n","\tuint sdim = 0;\n","\tstart = seconds();\n","\tfor (uint i = 0; i < k; i++ ) {\n","\t\tuint d = A.blkSize[i];\n","\t\tmqdbBlockProd<<<grid, block>>>(d_A, d_B, d_C, sdim, d, n);\n","\t\tsdim += d;\n","\t}\n","\tCHECK(cudaDeviceSynchronize());\n","\tdouble GPUtime2 = seconds() - start;\n","\tprintf(\"   elapsed time:                    %.4f (sec)\\n\", GPUtime2);\n","\tprintf(\"   speedup vs CPU MQDB product:     %.4f\\n\", CPUTime/GPUtime2);\n","\tprintf(\"   speedup vs GPU std mat product:  %.4f\\n\", GPUtime1/GPUtime2);\n","\t// copy the array 'C' back from the GPU to the CPU\n","\tCHECK(cudaMemcpy(C1.elem, d_C.elem, nBytes, cudaMemcpyDeviceToHost));\n","\tCHECK(cudaMemset(d_C.elem, 0.0, nBytes));\n","\tcheckResult(C,C1);\n","\n","\tCHECK(cudaFree(d_A.elem));\n","\tCHECK(cudaFree(d_B.elem));\n","\tCHECK(cudaFree(d_C.elem));\n","\n","\t// collect times\n","\ttimes->CPUtms = CPUTime;\n","\ttimes->GPUtmsNaive = GPUtime1;\n","\ttimes->GPUtmsMQDB = GPUtime2;\n","\n","\tfloat den = 0;\n","\tfor (uint j = 0; j < k; j++)\n","\t\tden += A.blkSize[j]*A.blkSize[j];\n","\ttimes->density = den/(n*n);\n","}\n","\n","/*\n"," * main function\n"," */\n","int main(int argc, char *argv[]) {\n","\tuint n = 8*1024;      // matrix size\n","\tuint min_k = 30;       // max num of blocks\n","\tuint max_k = 30;       // max num of blocks\n","\n","\tstruct tms times[max_k-min_k+1];\n","\n","\t// multiple tests on kernels\n","\tfor (uint k = min_k; k <= max_k; k++) {\n","\t\tprintf(\"\\n*****   k = %d --- (avg block size = %f)\\n\",k,(float)n/k);\n","\t\ttestKernelsMQDB(n, k, &times[k-min_k]);\n","\t}\n","\n","\tFILE *fd;\n","\tfd = fopen(\"res.csv\", \"w\");\n","\tif (fd == NULL) {\n","\t\tperror(\"file error!\\n\");\n","\t\texit(1);\n","\t}\n","\n","\t// write results on file\n","\tfprintf(fd,\"num blocks,\");\n","\t\tfor (uint j = 0; j <= max_k-min_k; j++)\n","\t\t\tfprintf(fd,\"%d,\",j+min_k);\n","\n","\tfprintf(fd,\"\\nCPU MQDB product,\");\n","\tfor (uint j = 0; j <= max_k-min_k; j++)\n","\t\tfprintf(fd,\"%.6f,\",times[j].CPUtms);\n","\n","\tfprintf(fd,\"\\nKernel mat product naive,\");\n","\tfor (uint j = 0; j <= max_k-min_k; j++)\n","\t\tfprintf(fd,\"%.6f,\",times[j].GPUtmsNaive);\n","\n","\tfprintf(fd,\"\\nKernel MQDB product,\");\n","\tfor (uint j = 0; j <= max_k-min_k; j++)\n","\t\tfprintf(fd,\"%.6f,\",times[j].GPUtmsMQDB);\n","\n","\tfprintf(fd,\"\\ndensity,\");\n","\tfor (uint j = 0; j <= max_k-min_k; j++)\n","\t\tfprintf(fd,\"%.6f,\",times[j].density);\n","\n","\tfclose(fd);\n","\n","\treturn 0;\n","}\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wLxZjCx8bT3s"},"source":["# Compilazione ed esecuzione\n","\n","!nvcc -arch=sm_75 src/MQDB/mqdb_prod.cu /content/GPUcomputing/utils/MQDB/mqdb.cpp  -o mqdb_prod\n","!./mqdb_prod"],"execution_count":null,"outputs":[]}]}