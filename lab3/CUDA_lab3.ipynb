{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CUDA_lab3.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":["_cGSqZovVkxK","zs_a5Vuimily","5gUDpbz5TZml","ygwWcMU9DJmG","IkYKd9J32ewH","Dg9BHTaWClBj","9QunXrnuC8AA"],"mount_file_id":"1PD8axMJPIW19SKS0LZUlg-Ps3b7OlikM","authorship_tag":"ABX9TyNXMsVrcZV3lS+Fqh9+TT/T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["---\n","# **LAB 3 - Modello di esecuzione CUDA**\n","---"],"metadata":{"id":"fZYqN0UwVLC_"}},{"cell_type":"markdown","metadata":{"id":"WoJbB3T5Vkw-"},"source":["# ‚ñ∂Ô∏è CUDA setup"]},{"cell_type":"code","metadata":{"id":"Fht2Wy8wVkxJ"},"source":["!nvcc --version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0jP2H_YJVkxJ"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## [GPU Compute Capability](https://developer.nvidia.com/cuda-gpus)"],"metadata":{"id":"VKbaxH9wWosO"}},{"cell_type":"markdown","metadata":{"id":"_cGSqZovVkxK"},"source":["## NVCC Plugin for Jupyter notebook\n","\n","*Usage*:\n","\n","\n","*   Load Extension `%load_ext nvcc_plugin`\n","*   Mark a cell to be treated as cuda cell\n","`%%cuda --name example.cu --compile false`\n","\n","**NOTE**: The cell must contain either code or comments to be run successfully. It accepts 2 arguments. `-n | --name` - which is the name of either CUDA source or Header. The name parameter must have extension `.cu` or `.h`. Second argument -c | --compile; default value is false. The argument is a flag to specify if the cell will be compiled and run right away or not. It might be usefull if you're playing in the main function\n","\n","*  We are ready to run CUDA C/C++ code right in your Notebook. For this we need explicitly say to the interpreter, that we want to use the extension by adding `%%cu` at the beginning of each cell with CUDA code. \n","\n","\n"]},{"cell_type":"code","metadata":{"id":"RCVhMkqYVkxK"},"source":["!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e6PDOytTVkxK"},"source":["%load_ext nvcc_plugin"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Bash and data setup"],"metadata":{"id":"cReFlD-VRfZe"}},{"cell_type":"code","source":["#@title Bash setup\n","%%writefile /root/.bashrc\n","\n","# If not running interactively, don't do anything\n","[ -z \"$PS1\" ] && return\n","\n","# don't put duplicate lines in the history. See bash(1) for more options\n","# ... or force ignoredups and ignorespace\n","HISTCONTROL=ignoredups:ignorespace\n","\n","# append to the history file, don't overwrite it\n","shopt -s histappend\n","\n","# for setting history length see HISTSIZE and HISTFILESIZE in bash(1)\n","HISTSIZE=10000\n","HISTFILESIZE=20000\n","\n","# check the window size after each command and, if necessary,\n","# update the values of LINES and COLUMNS.\n","shopt -s checkwinsize\n","\n","# make less more friendly for non-text input files, see lesspipe(1)\n","[ -x /usr/bin/lesspipe ] && eval \"$(SHELL=/bin/sh lesspipe)\"\n","\n","PS1='\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ '\n","\n","# enable color support of ls and also add handy aliases\n","if [ -x /usr/bin/dircolors ]; then\n","    test -r ~/.dircolors && eval \"$(dircolors -b ~/.dircolors)\" || eval \"$(dircolors -b)\"\n","    alias ls='ls --color=auto'\n","    #alias dir='dir --color=auto'\n","    #alias vdir='vdir --color=auto'\n","\n","    alias grep='grep --color=auto'\n","    alias fgrep='fgrep --color=auto'\n","    alias egrep='egrep --color=auto'\n","fi\n","\n","# some more ls aliases\n","alias ll='ls -lF'\n","alias la='ls -A'\n","alias l='ls -CF'\n","\n","# path setup\n","export PATH=\"./:/usr/local/cuda/bin:$PATH\""],"metadata":{"cellView":"form","id":"O8ICSyy8_GEq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!source /root/.bashrc"],"metadata":{"id":"QxIfKO3Ghf7g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Clone GPUcomputing site on github..."],"metadata":{"id":"IYG8Cv4bTzyI"}},{"cell_type":"code","source":["!git clone https://github.com/giulianogrossi/GPUcomputing.git"],"metadata":{"id":"E7jZmHjCT0vu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define some paths..."],"metadata":{"id":"ZarLje6wR_Og"}},{"cell_type":"code","source":["# path setup\n","%cd /content/GPUcomputing/lab3\n","!mkdir -p divergence\n","!mkdir -p histogram\n","!mkdir -p MQDB-CUDA\n","!mkdir -p parReduce"],"metadata":{"id":"tC-AaOJlkLOO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ‚ñ∂Ô∏è VS Code on Colab"],"metadata":{"id":"zs_a5Vuimily"}},{"cell_type":"code","source":["#@title Colab-ssh tunnel\n","#@markdown Execute this cell to open the ssh tunnel. Check [colab-ssh documentation](https://github.com/WassimBenzarti/colab-ssh) for more details.\n","\n","# Install colab_ssh on google colab\n","!pip install colab_ssh --upgrade\n","\n","from colab_ssh import launch_ssh_cloudflared, init_git_cloudflared\n","ssh_tunnel_password = \"gpu\" #@param {type: \"string\"}\n","launch_ssh_cloudflared(password=ssh_tunnel_password)\n","\n","# Optional: if you want to clone a Github or Gitlab repository\n","repository_url=\"https://github.com/giulianogrossi/GPUcomputing\" #@param {type: \"string\"}\n","init_git_cloudflared(repository_url)"],"metadata":{"id":"BCf9JxqphHAp","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5gUDpbz5TZml"},"source":["# ‚úÖ Divergence analysis"]},{"cell_type":"code","metadata":{"id":"RlbVvBaXCHBs"},"source":["%%writefile divergence/div.cu\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <assert.h>\n","#include \"../../utils/common.h\"\n","\n","/*\n"," * Kernel with warp divergence\n"," */\n","__global__ void evenOddDIV(int *c, const ulong N) {\n","\tulong tid = blockIdx.x * blockDim.x + threadIdx.x;\n","\tint a, b;\n","\n","\tif (!(tid % 2))   // branch divergence\n","\t\ta = 2;                  \n","\telse\n","\t\tb = 1;                  \n","\n","\t// check index\n","\tif (tid < N)\n","\t\tc[tid] = a + b;\n","}\n","\n","/*\n"," * Kernel without warp divergence\n"," */\n","__global__ void evenOddNODIV(int *c, const int N) {\n","\tint tid = blockIdx.x * blockDim.x + threadIdx.x;\n","\tint a = 0, b = 0;\n","\tunsigned int i, twoWarpSize = 2 * warpSize;\n","\n","\tint wid = tid / warpSize; \t// warp index wid = 0,1,2,3,...\n","\tif (!(wid % 2))\n","\t\ta = 2;                  // branch1: thread tid = 0-31, 64-95, ...\n","\telse\n","\t\tb = 1;                  // branch2: thread tid = 32-63, 96-127, ...\n","\n","\t// right index\n","\tif (!(wid % 2))  // even\n","\t\ti = 2 * (tid % warpSize) + (tid / twoWarpSize) * twoWarpSize;\n","\telse            // odd\n","\t\ti = 2 * (tid % warpSize) + 1 + (tid / twoWarpSize) * twoWarpSize;\n","\n","\t// check index\n","\tif (i < N) {\n","\t\tc[i] = a + b;\n","\t}\n","}\n","\n","/*\n"," * MAIN\n"," */\n","int main(int argc, char **argv) {\n","\n","\t// set up data size\n","\tint blocksize = 1024;\n","\tulong size = 1024*1024;\n","\n","\tif (argc > 1)\n","\t\tblocksize = atoi(argv[1]);\n","\tif (argc > 2)\n","\t\tsize = atoi(argv[2]);\n","\tulong nBytes = size * sizeof(int);\n","\n","\tprintf(\"Data size: %lu  -- \", size);\n","  printf(\"Data size (bytes): %lu MB\\n\", nBytes/1000000);\n","\n","\t// set up execution configuration\n","\tdim3 block(blocksize, 1);\n","\tdim3 grid((size + block.x - 1) / block.x, 1);\n","\tprintf(\"Execution conf (block %d, grid %d)\\nKernels:\\n\", block.x, grid.x);\n","\n","\t// allocate memory\n","\tint *d_C, *C;\n","\tC = (int *) malloc(nBytes);\n","\tCHECK(cudaMalloc((void** )&d_C, nBytes));\n","\n","\t// run kernel 1\n","\tdouble iStart, iElaps;\n","\tiStart = seconds();\n","\tevenOddDIV<<<grid, block>>>(d_C, size);\n","\tCHECK(cudaDeviceSynchronize());\n","\tiElaps = seconds() - iStart;\n","\tprintf(\"\\tevenOddDIV<<<%d, %d>>> elapsed time %f sec \\n\\n\", grid.x, block.x, iElaps);\n","\tCHECK(cudaGetLastError());\n","  \n","  CHECK(cudaMemcpy(C, d_C, nBytes, cudaMemcpyDeviceToHost));\n","\n","\n","\t// run kernel 2\n","  CHECK(cudaMemset(d_C, 0.0, nBytes)); // reset memory\n","\tiStart = seconds();\n","\tevenOddNODIV<<<grid, block>>>(d_C, size);\n","\tiElaps = seconds() - iStart;\n","\tprintf(\"\\tevenOddNODIV<<<%d, %d>>> elapsed time %f sec \\n\\n\", grid.x, block.x, iElaps);\n","\tCHECK(cudaGetLastError());\n","\n","\tCHECK(cudaMemcpy(C, d_C, nBytes, cudaMemcpyDeviceToHost));\n","\n","\tfree(C);\n","\t// free gpu memory and reset device\n","\tCHECK(cudaFree(d_C));\n","\tCHECK(cudaDeviceReset());\n","\treturn EXIT_SUCCESS;\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kMEGfjJMcX_e"},"source":["# Compilazione ed esecuzione\n","!nvcc -arch=sm_37 divergence/div.cu -o div \n","!./div 1024 20000000"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PdQqnuH-54Ie"},"source":["# Compilazione ed esecuzione versione di debug \n","!nvcc -arch=sm_37 -g -G divergence/div.cu -o div_deb\n","!./div_deb 1024 2000000000"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["deadlock example..."],"metadata":{"id":"ZQMlPu1lUaTu"}},{"cell_type":"code","source":["/*\n"," * Kernel with sync barriers\n"," */\n","__global__ void deadlock(char kind) {\n","    \n","\tint tid = blockIdx.x * blockDim.x + threadIdx.x;\n","\t\n","\tif (!(tid % 2) && (kind == 'b' || kind == 'e'))      // branch divergence\n","    printf(\"hello! I'm tid = %d\", tid);\n","\t\t__syncthreads();   // barrier for even threads           \n","\telse if (kind == 'b' || kind == 'o')\n","    printf(\"hello! I'm tid = %d\", tid);\n","\t\t__syncthreads();   // barrier for odd threads     \n","}\n","\n","/*\n"," * MAIN\n"," */\n","int main(int argc, char **argv) {\n","\n","  char kind = 'b';   // 'b' stands for both, 'e' for even 'o' for odd only \n","\tiStart = seconds();\n","\tdeadlock<<<1, 32>>>(kind);\n","\tiElaps = seconds() - iStart;\n","\tprintf(\"deadlock<<<%d, %d>>> elapsed time %f sec \\n\\n\", grid.x, block.x, iElaps);\n","\tCHECK(cudaGetLastError());\n","\t"],"metadata":{"id":"_EFvcTsoTcAg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ygwWcMU9DJmG"},"source":["# ‚úÖ Parallel Reduction"]},{"cell_type":"code","metadata":{"id":"WThhkz6GDMsm"},"source":["%%writefile /content/parReduce/preduce.cu\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <assert.h>\n","\n","#include \"../../utils/common.h\"\n","\n","/*\n"," *  Block by block parallel implementation with divergence (sequential schema)\n"," */\n","__global__ void blockParReduce1(int *in, int *out, ulong n) {\n","\n","\tuint tid = threadIdx.x;\n","\tulong idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// boundary check\n","\tif (idx >= n)\n","\t\treturn;\n","\n","\t// convert global data pointer to the local pointer of this block\n","\tint *thisBlock = in + blockIdx.x * blockDim.x;\n","\n","\t// in-place reduction in global memory\n","\tfor (int stride = 1; stride < blockDim.x; stride *= 2) {\n","\t\tif ((tid % (2 * stride)) == 0)\n","\t\t\tthisBlock[tid] += thisBlock[tid + stride];\n","\n","\t\t// synchronize within threadblock\n","\t\t__syncthreads();\n","\t}\n","\n","\t// write result for this block to global mem\n","\tif (tid == 0)\n","\t\tout[blockIdx.x] = thisBlock[0];\n","}\n","\n","/*\n"," *  Block by block parallel implementation without divergence (interleaved schema)\n"," */\n","__global__ void blockParReduce2(int *in, int *out, ulong n) {\n","\n","\tuint tid = threadIdx.x;\n","\tulong idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// boundary check\n","\tif (idx >= n)\n","\t\treturn;\n","\n","\t// convert global data pointer to the local pointer of this block\n","\tint *thisBlock = in + blockIdx.x * blockDim.x;\n","\n","\t// in-place reduction in global memory\n","\tfor (int stride = blockDim.x / 2; stride > 0; stride >>= 1)  {\n","\t\tif (tid < stride)\n","\t\t\tthisBlock[tid] += thisBlock[tid + stride];\n","\n","\t\t// synchronize within threadblock\n","\t\t__syncthreads();\n","\t}\n","\n","\t// write result for this block to global mem\n","\tif (tid == 0)\n","\t\tout[blockIdx.x] = thisBlock[0];\n","}\n","\n","\n","/*\n"," * MAIN: test on parallel reduction\n"," */\n","int main(void) {\n","\tint *a, *b, *d_a, *d_b;\n","\tint blockSize = 1024;            // block dim 1D\n","\tulong numBlock = 1024*1024;      // grid dim 1D\n","\tulong n = blockSize * numBlock;  // array dim\n","\tlong sum_CPU = 0, sum_GPU;\n","\tlong nByte = n*sizeof(int), mByte = numBlock * sizeof(int);\n","\tdouble start, stopGPU, stopCPU, speedup;\n","\n","\tprintf(\"\\n****  test on parallel reduction  ****\\n\");\n","\n","\t// init\n","\ta = (int *) malloc(nByte);\n","\tb = (int *) malloc(mByte);\n","\tfor (ulong i = 0; i < n; i++) a[i] = 1;\n","\n","\tCHECK(cudaMalloc((void **) &d_a, nByte));\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMalloc((void **) &d_b, mByte));\n","\tCHECK(cudaMemset((void *) d_b, 0, mByte));\n","\n","\t/***********************************************************/\n","\t/*                     CPU reduction                       */\n","\t/***********************************************************/\n","\tprintf(\"  Vector length: %.2f MB\\n\",n/(1024.0*1024.0));\n","\tprintf(\"\\n  CPU procedure...\\n\");\n","\tstart = seconds();\n","\tfor (ulong i = 0; i < n; i++) \n","    sum_CPU += a[i];\n","\tstopCPU = seconds() - start;\n","\tprintf(\"    Elapsed time: %f (sec) \\n\", stopCPU);\n","\tprintf(\"    sum: %lu\\n\",sum_CPU);\n","\n","\tprintf(\"\\n  GPU kernels (mem required %lu bytes)\\n\", nByte);\n","\n","\t/***********************************************************/\n","\t/*         KERNEL blockParReduce1 (divergent)              */\n","\t/***********************************************************/\n","\t// block by block parallel implementation with divergence\n","\tprintf(\"\\n  Launch kernel: blockParReduce1...\\n\");\n","\tstart = seconds();\n","\tblockParReduce1<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaGetLastError());\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n","\t\n","  // memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\t\n","  // check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++)\n","\t\tsum_GPU += b[i];\n","\tassert(sum_GPU == n);\n","\n","\t// reset input vector on GPU\n","\tfor (ulong i = 0; i < n; i++) a[i]=1;\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\n","\t/***********************************************************/\n","\t/*        KERNEL blockParReduce2  (non divergent)          */\n","\t/***********************************************************/\n","\t// block by block parallel implementation without divergence\n","\tprintf(\"\\n  Launch kernel: blockParReduce2...\\n\");\n","\tstart = seconds();\n","\tblockParReduce2<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n","\tCHECK(cudaGetLastError());\n","\t\n","  // memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\t\n","  // check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++) {\n","\t\tsum_GPU += b[i];\n","  //\t\tprintf(\"b[%d] = %d\\n\",i,b[i]);\n","\t}\n","\tassert(sum_GPU == n);\n","\t\n","  // reset input vector on GPU\n","\tfor (ulong i = 0; i < n; i++) a[i] = 1;\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\n","\t// check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++)\n","\t\tsum_GPU += b[i];\n","\tassert(sum_GPU == n);\n","\n","\tcudaFree(d_a);\n","\n","\tCHECK(cudaDeviceReset());\n","\treturn 0;\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pIOextPZMiav"},"source":["#Compilazione ed esecuzione\n","\n","!nvcc -arch=sm_37 parReduce/preduce.cu -o preduce\n","!./preduce"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üî¥ TODO"],"metadata":{"id":"VX7H_0JiBx2j"}},{"cell_type":"markdown","source":["Analizzare le prestazioni usando \n","* `nvprof --events branch,divergent_branch`\n","* `nvprof --metrics achieved_occupancy`"],"metadata":{"id":"rMXU-CfpCCep"}},{"cell_type":"markdown","metadata":{"id":"IkYKd9J32ewH"},"source":["# ‚úÖ Istogramma di un'immagine BMP"]},{"cell_type":"markdown","source":["Calcolare l'istogramma di un aimmagine BMP con uso di `atomicAdd`"],"metadata":{"id":"vfkxkSqmCmAD"}},{"cell_type":"markdown","source":["# üî¥ TODO"],"metadata":{"id":"Dg9BHTaWClBj"}},{"cell_type":"code","metadata":{"id":"G18uZY3t2kFp"},"source":["%%writefile histogram/hist.cu\n","/**\n"," * hist.cu\n"," */\n","#include <cuda_runtime.h>\n","#include <stdio.h>\n","#include <time.h>\n","#include <limits.h>\n","\n","#include \"../../utils/common.h\"\n","#include \"../../utils/BMP/ImageStuff.h\"\n","#include \"../../utils/BMP/bmpUtil.h\"\n","\n","/*\n"," * Kernel 1D that computes histogram on GPU\n"," */\n","__global__ void histogramBMP(uint *bins, const pel *imgSrc, const uint W, const uint N, const uint M) {\n","\t// ** pixel granularity **\n","\tuint x = blockDim.x * blockIdx.x + threadIdx.x; // 1D pixel linear index over [0:W*H)\n","\tuint nrows = x / W;                             // num of rows to skip\n","\tuint off = x % W;                               // offset (= col) within current row\n","\n","\tif (x >= N)                        // pixel out of range\n","\t\treturn;\n","\n","\t//  ** byte granularity **\n","\tuint p = M * nrows + 3*off;        // src byte position of the pixel\n","\tpel R = imgSrc[p];\n","\tpel G = imgSrc[p+1];\n","\tpel B = imgSrc[p+2];\n","\tatomicAdd(&bins[R], 1);\n","\tatomicAdd(&bins[G+256], 1);\n","\tatomicAdd(&bins[B+512], 1);\n","}\n","\n","/*\n"," * Function that computes histogram on CPU\n"," */\n","void hist_CPU(uint *bins, const pel *imgSrc, const uint W, const uint H, const uint M) {\n","\tfor (int i = 0; i < W*H; i++) {\n","\t\tuint r = i / W;              // row of the source pixel\n","\t\tuint off = i - r * W;        // col of the source pixel\n","\n","\t\t//  ** byte granularity **\n","\t\tuint p = M * r + 3*off;      // src byte position of the pixel\n","\t\tpel R = imgSrc[p];\n","\t\tpel G = imgSrc[p+1];\n","\t\tpel B = imgSrc[p+2];\n","\t\tbins[R] += 1;\n","\t\tbins[G+256] += 1;\n","\t\tbins[B+512] += 1;\n","\t}\n","}\n","\n","int main(int argc, char **argv) {\n","\n","\tuint dimBlock = 1024;\n","\tpel *imgBMP_CPU;     // Where images are stored in CPU\n","\tpel *imgBMP_GPU;\t // Where images are stored in GPU\n","\n","\tuint *binsRGB_CPU, *binsRGB_GPU, *binsRGB_GPU2CPU;\n","\tuint N_bins = 3*256;\n","\tuint bin_size = N_bins*sizeof(uint);\n","\n","\tif (argc > 2)\n","\t\tdimBlock = atoi(argv[2]);\n","\telse if (argc < 2) {\n","\t\tprintf(\"\\n\\nUsage:  hist InputFilename dimBlock\\n\");\n","\t\texit(EXIT_FAILURE);\n","\t}\n","\n","\t// bins for CPU & GPU\n","\tbinsRGB_CPU = (uint*) calloc(N_bins, sizeof(uint));\n","\tbinsRGB_GPU2CPU = (uint*) malloc(bin_size);\n","\tCHECK(cudaMalloc((void**) &binsRGB_GPU, bin_size));\n","\n","\t// Create CPU memory to store the input image\n","\timgBMP_CPU = ReadBMPlin(argv[1]);\n","\tif (imgBMP_CPU == NULL) {\n","\t\tprintf(\"Cannot allocate memory for the input image...\\n\");\n","\t\texit(EXIT_FAILURE);\n","\t}\n","\n","\t// Allocate GPU buffer for image and bins\n","\tCHECK(cudaMalloc((void**) &imgBMP_GPU, IMAGESIZE));\n","\n","\t// Copy input vectors from host memory to GPU buffers.\n","\tCHECK(cudaMemcpy(imgBMP_GPU, imgBMP_CPU, IMAGESIZE, cudaMemcpyHostToDevice));\n","\n","\t// CPU histogram\n","\tdouble start = seconds();   // start time\n","\thist_CPU(binsRGB_CPU, imgBMP_CPU, WIDTH, HEIGHT, WIDTHB);\n","\tdouble stop = seconds();   // elapsed time\n","\tprintf(\"\\nCPU elapsed time %f sec \\n\\n\", stop - start);\n","\n","\t// invoke kernels (define grid and block sizes)\n","\tuint nPixels = WIDTH*HEIGHT;\n","\tint dimGrid = (nPixels + dimBlock - 1) / dimBlock;\n","\tprintf(\"\\ndimGrid = %d   dimBlock = %d\\n\",dimGrid,dimBlock);\n","\n","\tstart = seconds();   // start time\n","\thistogramBMP<<<dimGrid, dimBlock>>>(binsRGB_GPU, imgBMP_GPU, WIDTH, nPixels, WIDTHB);\n","\tCHECK(cudaDeviceSynchronize());\n","\tstop = seconds();   // elapsed time\n","\tprintf(\"\\nGPU elapsed time %f sec \\n\\n\", stop - start);\n","\n","\t// Copy output (results) from GPU buffer to host (CPU) memory.\n","\tCHECK(cudaMemcpy(binsRGB_GPU2CPU, binsRGB_GPU, bin_size, cudaMemcpyDeviceToHost));\n","\n","\tfor (int i = 0; i < N_bins/3; i++)\n","\t\tprintf(\"bin_GPU[%d] = \\t%d\\t%d\\t%d\\t -- bin_CPU[%d] = \\t%d\\t%d\\t%d\\n\", i,\n","\t\t\t\tbinsRGB_GPU2CPU[i],binsRGB_GPU2CPU[i+256],binsRGB_GPU2CPU[i+512],\n","\t\t\t\ti,binsRGB_CPU[i],binsRGB_CPU[i+256],binsRGB_CPU[i+512]);\n","\n","\t// Deallocate GPU memory\n","\tcudaFree(imgBMP_GPU);\n","\tcudaFree(binsRGB_GPU);\n","\n","\t// tracing tools spel as Parallel Nsight and Visual Profiler to show complete traces.\n","\tCHECK(cudaDeviceReset());\n","\n","\treturn (EXIT_SUCCESS);\n","}\n","\n","/*\n"," *  Read a 24-bit/pixel BMP file into a 1D linear array.\n"," *  Allocate memory to store the 1D image and return its pointer\n"," */\n","pel *ReadBMPlin(char* fn) {\n","\tstatic pel *Img;\n","\tFILE* f = fopen(fn, \"rb\");\n","\tif (f == NULL) {\n","\t\tprintf(\"\\n\\n%s NOT FOUND\\n\\n\", fn);\n","\t\texit(EXIT_FAILURE);\n","\t}\n","\n","\tpel HeaderInfo[54];\n","\tsize_t nByte = fread(HeaderInfo, sizeof(pel), 54, f); // read the 54-byte header\n","\t// extract image height and width from header\n","\tint width = *(int*) &HeaderInfo[18];\n","\timg.width = width;\n","\tint height = *(int*) &HeaderInfo[22];\n","\timg.height = height;\n","\tint RowBytes = (width * 3 + 3) & (~3);  // row is multiple of 4 pixel\n","\timg.rowByte = RowBytes;\n","\t//save header for re-use\n","\tmemcpy(img.headInfo, HeaderInfo, 54);\n","\tprintf(\"\\n Input File name: %5s  (%d x %d)   File Size=%lu\", fn, img.width, img.height, IMAGESIZE);\n","\n","\t// allocate memory to store the main image (1 Dimensional array)\n","\tImg = (pel *) malloc(IMAGESIZE);\n","\tif (Img == NULL)\n","\t\treturn Img;      // Cannot allocate memory\n","\t// read the image from disk\n","\tsize_t out = fread(Img, sizeof(pel), IMAGESIZE, f);\n","\tfclose(f);\n","\treturn Img;\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FvvbqpV-3gLH"},"source":["# Compilazione ed esecuzione\n","\n","!nvcc -arch=sm_37 histogram/hist.cu ../utils/BMP/ImageStuff.c -o hist\n","!./hist /content/GPUcomputing/images/dog.bmp 256"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SOFMQZAkjlLW"},"source":["# ‚úÖ Prodotto MQDB CUDA"]},{"cell_type":"code","source":["%%writefile /content/MQDB-CUDA/mqdb.h\n","\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <string.h>\n","#include <math.h>\n","\n","#ifndef MQDB_H\n","#define MQDB_H\n","\n","#define randu() ((float)rand() / (float) RAND_MAX)\n","#define abs(x) ((x)<0 ? (-x) : (x))\n","\n","typedef unsigned long ulong;\n","typedef unsigned int uint;\n","\n","typedef struct MQDB {\n","\tchar desc[100];   // description\n","\tint nBlocks;      // num. of blocks\n","\tint *blkSize;     // block dimensions\n","\tfloat *elem;       // elements in row-major order\n","\tulong nElems;     // actual number of elements\n","} mqdb;\n","\n","// function prototypes\n","int genRandDims(mqdb*, uint, uint, int);\n","int genRandDimsUnified(mqdb*, uint, uint, int);\n","void fillBlocks(mqdb*, uint, uint, char, float);\n","void fillBlocksUnified(mqdb*, uint, uint, char, float);\n","mqdb mqdbConst(uint, uint, uint, float);\n","void mqdbProd(mqdb, mqdb, mqdb);\n","void matProd(mqdb, mqdb, mqdb);\n","void checkResult(mqdb, mqdb);\n","void mqdbDisplay(mqdb*);\n","\n","#endif\n"],"metadata":{"id":"1fdW1VHR_CGg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile /content/MQDB-CUDA/mqdb.cpp\n","#include \"mqdb.h\"\n","\n","/**\n"," * random generate block dimensions\n"," */\n","int genRandDims(mqdb *M, uint n, uint k, int seed) {\n","\n","\tif (n == 0 || k == 0 || k > n) {\n","\t\tprintf(\"error: n and k must be positive and n > k!\\n\");\n","\t\treturn(-1);\n","\t}\n","\tsrand(seed);\n","\tM->nBlocks = k;\n","\t// random generation of block sizes\n","\tM->blkSize = (int *) malloc(k * sizeof(int));\n","\tint sum = 0;\n","\tint r;\n","\tfloat mu = 2.0f * (float) n / (float) k;\n","\tfor (int i = 0; i < k - 1; i++) {\n","\t\t// expected value E[block_size] = n/k\n","\t\twhile ((r = round(mu * randu())) > n - sum - k + i + 1);\n","\t\tif (!r)\n","\t\t\tr += 1;\n","\t\tM->blkSize[i] = r;\n","\t\tsum += r;\n","\t}\n","\tM->blkSize[k - 1] = n - sum;\n","\treturn(0);\n","}\n","\n","/**\n"," * fill blocks either random or constant\n"," */\n","void fillBlocks(mqdb *M, uint n, uint k, char T, float c) {\n","\t//mat size n*n\n","\tM->elem = (float *) calloc(n * n, sizeof(float));\n","\tM->nElems = 0;\n","\tint offset = 0;\n","\t// loop on blocks\n","\tfor (int i = 0; i < k; i++) {\n","\t\tfor (int j = 0; j < M->blkSize[i]; j++)\n","\t\t\tfor (int k = 0; k < M->blkSize[i]; k++) {\n","\t\t\t\tif (T == 'C')  \t    // const fill mat entries\n","\t\t\t\t\tM->elem[offset * n + j * n + k + offset] = c;\n","\t\t\t\telse if (T == 'R') \t// random fill mat entries\n","\t\t\t\t\tM->elem[offset * n + j * n + k + offset] = c*randu();\n","\t\t\t\t//printf(\"M->[%d] = %f\\n\", offset * n + j * n + k + offset, M->elem[offset * n + j * n + k + offset]);\n","\t}\n","\t\toffset += M->blkSize[i];\n","\t\tM->nElems += M->blkSize[i]*M->blkSize[i];\n","\t}\n","\t// set description\n","\tsprintf(M->desc, \"Random mqdb:  mat. size = %d, num. blocks = %d\",n,k);\n","}\n","\n","\n","/**\n"," * random generate block dimensions - using CUDA Unified Memory\n"," */\n","int genRandDimsUnified(mqdb *M, uint n, uint k, int seed) {\n","\n","\tif (n == 0 || k == 0 || k > n) {\n","\t\tprintf(\"error: n and k must be positive and n > k!\\n\");\n","\t\treturn(-1);\n","\t}\n","\tsrand(seed);\n","\tM->nBlocks = k;\n","\tint sum = 0;\n","\tint r;\n","\tfloat mu = 2.0f * (float) n / (float) k;\n","\tfor (int i = 0; i < k - 1; i++) {\n","\t\t// expected value E[block_size] = n/k\n","\t\twhile ((r = round(mu * randu())) > n - sum - k + i + 1);\n","\t\tif (!r)\n","\t\t\tr += 1;\n","\t\tM->blkSize[i] = r;\n","\t\tsum += r;\n","\t}\n","\tM->blkSize[k - 1] = n - sum;\n","\treturn(0);\n","}\n","\n","/**\n"," * fill blocks either random or constant - using CUDA Unified Memory\n"," */\n","void fillBlocksUnified(mqdb *M, uint n, uint k, char T, float c) {\n","\t//mat size n*n\n","\tM->nElems = 0;\n","\tint offset = 0;\n","\t// loop on blocks\n","\tfor (int i = 0; i < k; i++) {\n","\t\tfor (int j = 0; j < M->blkSize[i]; j++)\n","\t\t\tfor (int k = 0; k < M->blkSize[i]; k++) {\n","\t\t\t\tif (T == 'C')  \t    // const fill mat entries\n","\t\t\t\t\tM->elem[offset * n + j * n + k + offset] = c;\n","\t\t\t\telse if (T == 'R') \t// random fill mat entries\n","\t\t\t\t\tM->elem[offset * n + j * n + k + offset] = c*randu();\n","\t\t\t\t//printf(\"M->[%d] = %f\\n\", offset * n + j * n + k + offset, M->elem[offset * n + j * n + k + offset]);\n","\t}\n","\t\toffset += M->blkSize[i];\n","\t\tM->nElems += M->blkSize[i]*M->blkSize[i];\n","\t}\n","\t// set description\n","\tsprintf(M->desc, \"Random mqdb:  mat. size = %d, num. blocks = %d\",n,k);\n","}\n","\n","/**\n"," * rand_gen_mqdb: mqdb  type returned\n"," *                n     square matrix size\n"," *                k     number of blocks\n"," *                seed  seed for random generator\n"," */\n","mqdb genRandMat(unsigned n, unsigned k, unsigned seed) {\n","\tmqdb M;\n","\tgenRandDims(&M, n, k, seed);\n","\n","\t// random fill mat entries\n","\tfillBlocks(&M, n, k, 'R', 1.0);\n","\n","\treturn M;\n","}\n","\n","/**\n"," * const_mqdb: mqdb  is the type returned\n"," *                n     is the square matrix size\n"," *                k     is the number of blocks\n"," *                seed  is the seed for random generator\n"," *                c   \tis the constant value assigned\n"," */\n","mqdb mqdbConst(uint n, uint k, uint seed, float c) {\n","\tmqdb M;\n","\tgenRandDims(&M, n, k, seed);\n","\n","\t// fill mat entries with a constant\n","\tfillBlocks(&M, n, k, 'C', c);\n","\n","\treturn M;\n","}\n","\n","/*\n"," * product between mqdb matrices restricted to blocks\n"," */\n","void mqdbProd(mqdb A, mqdb B, mqdb C) {\n","\tuint n = 0;\n","\tfor (uint i = 0; i < A.nBlocks; i++)\n","\t\tn += A.blkSize[i];                  // mat dim\n","\tint k = A.nBlocks;                      // num blks\n","\tint dl = 0;                             // blk left bound\n","\tint dr = 0;                             // blk left bound\n","\tfor (uint i = 0; i < k; i++) {           // loop on blks\n","\t\tdr += A.blkSize[i];                 // blk right bound\n","\t\tfor (uint r = dl; r < dr; r++) {     // scan block rows\n","\t\t\tfor (uint c = dl; c < dr; c++) { // scan block cols\n","\t\t\t\tfloat s = 0;\n","\t\t\t\tfor (uint l = dl; l < dr; l++)\n","\t\t\t\t\ts += A.elem[r*n + l] * B.elem[c + l * n];\n","\t\t\t\tC.elem[r*n + c] = s;\n","\t\t\t}\n","\t\t}\n","\t\tdl = dr;\n","\t}\n","}\n","\n","/*\n"," * standard (naive) matrix product on host\n"," */\n","void matProd(mqdb A, mqdb B, mqdb C) {\n","\tint n = 0;\n","\tfor (uint i = 0; i < A.nBlocks; i++)\n","\t\tn += A.blkSize[i];\n","\n","\tfor (uint r = 0; r < n; r++)\n","\t\tfor (uint c = 0; c < n; c++) {\n","\t\t\tdouble sum = 0;\n","\t\t\tfor (uint l = 0; l < n; l++){\n","\t\t\t\tdouble a = A.elem[r * n + l];\n","\t\t\t\tdouble b = B.elem[l * n + c];\n","\t\t\t\tsum += a*b;\n","\t\t\t}\n","\t\t\tC.elem[r * n + c] = (float)sum;\n","\t\t}\n","}\n","\n","/*\n"," * elementwise comparison between two mqdb\n"," */\n","void checkResult(mqdb A, mqdb B) {\n","\tdouble epsilon = 1.0E-8;\n","\tbool match = 1;\n","\tint n = 0;\n","\tfor (int i = 0; i < A.nBlocks; i++)\n","\t\tn += A.blkSize[i];\n","\tfor (int i = 0; i < n * n; i++) {\n","\t\tif (abs(A.elem[i] - B.elem[i]) > epsilon) {\n","\t\t\tmatch = 0;\n","\t\t\tprintf(\"   * Arrays do not match!\\n\");\n","\t\t\tprintf(\"     gpu: %2.2f,  host: %2.2f at current %d\\n\", A.elem[i],\n","\t\t\t\t\tB.elem[i], i);\n","\t\t\tbreak;\n","\t\t}\n","\t}\n","\tif (match)\n","\t\tprintf(\"   Arrays match\\n\\n\");\n","}\n","/*\n"," * print mqdb\n"," */\n","void mqdbDisplay(mqdb *M) {\n","\tint k = M->nBlocks;\n","\tint n = 0;\n","\tprintf(\"%s\\n\", M->desc);\n","\tprintf(\"Block sizes [%d]: \", k);\n","\tfor (int j = 0; j < k; j++) {\n","\t\tn += M->blkSize[j];\n","\t\tprintf(\"%d  \", M->blkSize[j]);\n","\t}\n","\n","\tprintf(\"\\nElements: \\n\");\n","\tfor (int i = 0; i < n; i++) {\n","\t\tfor (int j = 0; j < n; j++) {\n","\t\t\tif (M->elem[i*n + j] == 0)\n","\t\t\t\tprintf(\"------\");\n","\t\t\telse\n","\t\t\t\tprintf(\"%5.2f \", M->elem[i*n + j]);\n","\t\t}\n","\t\tprintf(\"\\n\");\n","\t}\n","\tprintf(\"\\n\");\n","}"],"metadata":{"id":"lAUf46d5_hLE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üî¥ TODO"],"metadata":{"id":"9QunXrnuC8AA"}},{"cell_type":"markdown","source":["Calcolare il prodotto di matrici MQDB con kernel CUDA "],"metadata":{"id":"lY5KTfZaC82S"}},{"cell_type":"code","metadata":{"id":"QVQVpcvKjkIk"},"source":["%%writefile /content/MQDB-CUDA/mqdb_prod.cu\n","\n","#include \"../MQDB-CUDA/mqdb.h\"\n","#include \"../utils/common.h\"\n","\n","#define BLOCK_SIZE 16     // block size\n","\n","struct tms {\n","\tdouble CPUtms;\n","\tdouble GPUtmsNaive;\n","\tdouble GPUtmsMQDB;\n","\tfloat density;\n","};\n","\n","/*\n"," * Kernel for standard (naive) matrix product\n"," */\n","__global__ void matProd(mqdb A, mqdb B, mqdb C, int n) {\n","\t// row & col indexes\n","\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n","\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// each thread computes an entry of the product matrix\n","\tif ((row < n) && (col < n)) {\n","\t\tfloat val = 0;\n","\t\tfor (int k = 0; k < n; k++)\n","\t\t\tval += A.elem[row * n + k] * B.elem[k * n + col];\n","\t\tC.elem[row * n + col] = val;\n","\t}\n","}\n","\n","/*\n"," * Kernel for block sub-matrix product of mqdb\n"," */\n","__global__ void mqdbBlockProd(mqdb A, mqdb B, mqdb C, int sdim, int d, int n) {\n","\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n","\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// jump to the right block sub-matrix\n","\tint  offset = (n+1)*sdim;\n","\n","\t// each thread computes an entry of the product matrix\n","\tif ((row < d) && (col < d)) {\n","\t\tfloat val = 0;\n","\n","\t\tfor (int k = 0; k < d; k++)\n","\t\t\tval += A.elem[row * n + k + offset] * B.elem[k * n + col + offset];\n","\t\tC.elem[row * n + col + offset] = val;\n","\t}\n","}\n","\n","/*\n"," * Test on MQDB kernels\n"," */\n","void testKernelsMQDB(uint n, uint k, struct tms* times) {\n","\n","\t// mqdb host matrices\n","\tmqdb A, B, C, C1;\n","\n","\t// mqdb device matrices\n","\tmqdb d_A, d_B, d_C;\n","\n","\t// fill in\n","\tA = mqdbConst(n, k, 10, 1);\n","\tB = mqdbConst(n, k, 10, 1);\n","\tC = mqdbConst(n, k, 10, 1);\n","\tC1 = mqdbConst(n, k, 10, 1);\n","\n","\tulong nBytes = n * n * sizeof(float);\n","\tulong kBytes = k * sizeof(uint);\n","\tprintf(\"Memory size required = %.1f (MB)\\n\",(float)nBytes/(1024.0*1024.0));\n","\n","\t// malloc and copy on device memory\n","\td_A.nBlocks = A.nBlocks;\n","\tCHECK(cudaMalloc((void**)&d_A.blkSize, kBytes));\n","\tCHECK(cudaMemcpy(d_A.blkSize, A.blkSize, kBytes, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMalloc((void**)&d_A.elem, nBytes));\n","\tCHECK(cudaMemcpy(d_A.elem, A.elem, nBytes, cudaMemcpyHostToDevice));\n","\td_B.nBlocks = B.nBlocks;\n","\tCHECK(cudaMalloc((void**)&d_B.blkSize, kBytes));\n","\tCHECK(cudaMemcpy(d_B.blkSize, B.blkSize, kBytes, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMalloc((void**)&d_B.elem, nBytes));\n","\tCHECK(cudaMemcpy(d_B.elem, B.elem, nBytes, cudaMemcpyHostToDevice));\n","\td_C.nBlocks = C.nBlocks;\n","\tCHECK(cudaMalloc((void**)&d_C.blkSize, kBytes));\n","\tCHECK(cudaMemcpy(d_C.blkSize, C.blkSize, kBytes, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMalloc((void**)&d_C.elem, nBytes));\n","\tCHECK(cudaMemset(d_C.elem, 0.0, nBytes));\n","\n","\t/***********************************************************/\n","\t/*                    CPU MQDB product                     */\n","\t/***********************************************************/\n","\tprintf(\"CPU MQDB product...\\n\");\n","\tdouble start = seconds();\n","\tmqdbProd(A,B,C);\n","\tdouble CPUTime = seconds() - start;\n","\tprintf(\"   CPU elapsed time: %.5f (sec)\\n\\n\", CPUTime);\n","\n","\t/***********************************************************/\n","\t/*                     GPU mat product                     */\n","\t/***********************************************************/\n","\tprintf(\"Kernel (naive) mat product...\\n\");\n","\tdim3 block(BLOCK_SIZE, BLOCK_SIZE);\n","\tdim3 grid((n + block.x - 1) / block.x, (n + block.y - 1) / block.y);\n","\tstart = seconds();\n","\tmatProd<<<grid, block>>>(d_A, d_B, d_C, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tdouble GPUtime1 = seconds() - start;\n","\tprintf(\"   elapsed time:                %.2f (sec)\\n\", GPUtime1);\n","\tprintf(\"   speedup vs CPU MQDB product: %.2f\\n\", CPUTime/GPUtime1);\n","\tCHECK(cudaMemcpy(C1.elem, d_C.elem, nBytes, cudaMemcpyDeviceToHost));\n","\tCHECK(cudaMemset(d_C.elem, 0.0, nBytes));\n","\tcheckResult(C,C1);\n","\t//\tmqdbDisplay(C1);\n","\n","\t/***********************************************************/\n","\t/*                     GPU MQDB product                    */\n","\t/***********************************************************/\n","\tprintf(\"Kernel MQDB product...\\n\");\n","\tuint sdim = 0;\n","\tstart = seconds();\n","\tfor (uint i = 0; i < k; i++ ) {\n","\t\tuint d = A.blkSize[i];\n","\t\tmqdbBlockProd<<<grid, block>>>(d_A, d_B, d_C, sdim, d, n);\n","\t\tsdim += d;\n","\t}\n","\tCHECK(cudaDeviceSynchronize());\n","\tdouble GPUtime2 = seconds() - start;\n","\tprintf(\"   elapsed time:                    %.2f (sec)\\n\", GPUtime2);\n","\tprintf(\"   speedup vs CPU MQDB product:     %.2f\\n\", CPUTime/GPUtime2);\n","\tprintf(\"   speedup vs GPU std mat product:  %.2f\\n\", GPUtime1/GPUtime2);\n","\t// copy the array 'C' back from the GPU to the CPU\n","\tCHECK(cudaMemcpy(C1.elem, d_C.elem, nBytes, cudaMemcpyDeviceToHost));\n","\tCHECK(cudaMemset(d_C.elem, 0.0, nBytes));\n","\tcheckResult(C,C1);\n","\n","\tCHECK(cudaFree(d_A.elem));\n","\tCHECK(cudaFree(d_B.elem));\n","\tCHECK(cudaFree(d_C.elem));\n","\n","\t// collect times\n","\ttimes->CPUtms = CPUTime;\n","\ttimes->GPUtmsNaive = GPUtime1;\n","\ttimes->GPUtmsMQDB = GPUtime2;\n","\t\n","\tfloat den = 0;\n","\tfor (uint j = 0; j < k; j++)\n","\t\tden += A.blkSize[j]*A.blkSize[j];\n","\ttimes->density = den/(n*n);\n","}\n","\n","/*\n"," * main function\n"," */\n","int main(int argc, char *argv[]) {\n","\tuint n = 2*1024;      // matrix size\n","\tuint min_k = 30;       // max num of blocks\n","\tuint max_k = 30;       // max num of blocks\n","\n","\tstruct tms times[max_k-min_k+1];\n","\n","\t// multiple tests on kernels\n","\tfor (uint k = min_k; k <= max_k; k++) {\n","\t\tprintf(\"\\n*****   k = %d --- (avg block size = %f)\\n\",k,(float)n/k);\n","\t\ttestKernelsMQDB(n, k, &times[k-min_k]);\n","\t}\n","\n","\tFILE *fd;\n","\tfd = fopen(\"res.csv\", \"w\");\n","\tif (fd == NULL) {\n","\t\tperror(\"file error!\\n\");\n","\t\texit(1);\n","\t}\n","\n","\t// write results on file\n","\tfprintf(fd,\"num blocks,\");\n","\t\tfor (uint j = 0; j <= max_k-min_k; j++)\n","\t\t\tfprintf(fd,\"%d,\",j+min_k);\n","\n","\tfprintf(fd,\"\\nCPU MQDB product,\");\n","\tfor (uint j = 0; j <= max_k-min_k; j++)\n","\t\tfprintf(fd,\"%.4f,\",times[j].CPUtms);\n","\n","\tfprintf(fd,\"\\nKernel mat product naive,\");\n","\tfor (uint j = 0; j <= max_k-min_k; j++)\n","\t\tfprintf(fd,\"%.4f,\",times[j].GPUtmsNaive);\n","\n","\tfprintf(fd,\"\\nKernel MQDB product,\");\n","\tfor (uint j = 0; j <= max_k-min_k; j++)\n","\t\tfprintf(fd,\"%.4f,\",times[j].GPUtmsMQDB);\n","\n","\tfprintf(fd,\"\\ndensity,\");\n","\tfor (uint j = 0; j <= max_k-min_k; j++)\n","\t\tfprintf(fd,\"%.4f,\",times[j].density);\n","\n","\tfclose(fd);\n","\n","\treturn 0;\n","}\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wLxZjCx8bT3s"},"source":["# Compilazione ed esecuzione\n","\n","!nvcc -arch=sm_37 MQDB-CUDA/mqdb_prod.cu MQDB-CUDA/mqdb.cpp  -o mqdb_prod\n","!./mqdb_prod"],"execution_count":null,"outputs":[]}]}