{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1wWMjVMdtfif5IjU502FHjWP5yWCWN65-","timestamp":1679914540076}],"collapsed_sections":["F9PmBZql0ow4","vKG_y-BUiE5G","iUYP4kCJhEIx","OHR7Zs3dNs1N"],"mount_file_id":"1mk0QXjREp-k_J-pdFfmgoC7-EVmgPyAo","authorship_tag":"ABX9TyP0uBNHKIppmnRvZwt4usOY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["---\n","# **LAB 5 - Shared memory (SMEM)**\n","---"],"metadata":{"id":"fZYqN0UwVLC_"}},{"cell_type":"markdown","metadata":{"id":"F9PmBZql0ow4"},"source":["# ‚ñ∂Ô∏è CUDA setup"]},{"cell_type":"code","metadata":{"id":"p9RIwaPbVQHV"},"source":["!nvcc --version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n5YlC1IOTlNb"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## [GPU Compute Capability](https://developer.nvidia.com/cuda-gpus)"],"metadata":{"id":"tMDwal1zXNio"}},{"cell_type":"markdown","metadata":{"id":"iVV0CidyVeqU"},"source":["## NVCC Plugin for Jupyter notebook\n","\n","*Usage*:\n","\n","\n","*   Load Extension `%load_ext nvcc_plugin`\n","*   Mark a cell to be treated as cuda cell\n","`%%cuda --name example.cu --compile false`\n","\n","**NOTE**: The cell must contain either code or comments to be run successfully. It accepts 2 arguments. `-n | --name` - which is the name of either CUDA source or Header. The name parameter must have extension `.cu` or `.h`. Second argument -c | --compile; default value is false. The argument is a flag to specify if the cell will be compiled and run right away or not. It might be usefull if you're playing in the main function\n","\n","*  We are ready to run CUDA C/C++ code right in your Notebook. For this we need explicitly say to the interpreter, that we want to use the extension by adding `%%cu` at the beginning of each cell with CUDA code. \n","\n","\n"]},{"cell_type":"code","metadata":{"id":"X1EeyR1jBnWR"},"source":["!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h7HcKDuAB-CO"},"source":["%load_ext nvcc_plugin"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plugin for cpp sintax highlighting \n","\n","!wget -O cpp_plugin.py https://gist.github.com/akshaykhadse/7acc91dd41f52944c6150754e5530c4b/raw/cpp_plugin.py\n","%load_ext cpp_plugin"],"metadata":{"id":"4zGi-1hA7qdM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Clone GPUcomputing site on github..."],"metadata":{"id":"ojEBtkAGT4SU"}},{"cell_type":"code","source":["!git clone https://github.com/giulianogrossi/GPUcomputing.git"],"metadata":{"id":"2OKGY3EFT41B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ‚ñ∂Ô∏è VS Code on Colab"],"metadata":{"id":"vKG_y-BUiE5G"}},{"cell_type":"code","source":["#@title Colab-ssh tunnel\n","#@markdown Execute this cell to open the ssh tunnel. Check [colab-ssh documentation](https://github.com/WassimBenzarti/colab-ssh) for more details.\n","\n","# Install colab_ssh on google colab\n","!pip install colab_ssh --upgrade --quiet\n","\n","from colab_ssh import launch_ssh_cloudflared, init_git_cloudflared\n","ssh_tunnel_password = \"gpu\" #@param {type: \"string\"}\n","launch_ssh_cloudflared(password=ssh_tunnel_password)\n","\n","# Optional: if you want to clone a Github or Gitlab repository\n","repository_url=\"https://github.com/giulianogrossi/GPUcomputing\" #@param {type: \"string\"}\n","init_git_cloudflared(repository_url)"],"metadata":{"cellView":"form","id":"B2BcP2EciE5G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define some paths..."],"metadata":{"id":"lSqPQ40yiE5H"}},{"cell_type":"code","source":["# path setup\n","!mkdir -p /content/GPUcomputing/lab2\n","%cd /content/GPUcomputing/lab2\n","!mkdir -p src\n"],"metadata":{"id":"vpnw20UGiE5H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iUYP4kCJhEIx"},"source":["# ‚ñ∂Ô∏è DeviceQuery"]},{"cell_type":"code","metadata":{"id":"kW9b_Yuxi7id"},"source":["# DeviceQuery dell'attuale device (su Colab!)\n","!nvcc GPUcomputing/utils/deviceQuery.cu -o deviceQuery\n","!./deviceQuery"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ‚úÖ Parallel reduction con SMEM\n"],"metadata":{"id":"OHR7Zs3dNs1N"}},{"cell_type":"code","source":["%%cuda --name preduceSMEM.cu\n","\n","#include \"../GPUcomputing/utils/common.h\"\n","#include <cuda_runtime.h>\n","#include <stdio.h>\n","#define DIM 1024\n","\n","/*\n"," * An example of using shared memory to optimize performance of a parallel\n"," * reduction by constructing partial results for a thread block in shared memory\n"," * before flushing to global memory.\n"," */\n","\n","extern __shared__ int dsmem[];\n","\n","//# Recursive Implementation of Interleaved Pair Approach\n","int recursiveReduce(int *data, int const size) {\n","  if (size == 1) return data[0];\n","\n","  int const stride = size / 2;\n","\n","  for (int i = 0; i < stride; i++)\n","    data[i] += data[i + stride];\n","\n","  return recursiveReduce(data, stride);\n","}\n","\n","__device__ void warpReduce(volatile int *smem, unsigned int tid) {\n","  smem[tid] = min(smem[tid],smem[tid+32]);\n","  smem[tid] = min(smem[tid],smem[tid+16]);\n","  smem[tid] = min(smem[tid],smem[tid+8]);\n","  smem[tid] = min(smem[tid],smem[tid+4]);\n","  smem[tid] = min(smem[tid],smem[tid+2]);\n","  smem[tid] = min(smem[tid],smem[tid+1]);\n","}\n","\n","//# unroll4 + complete unroll for loop + gmem\n","__global__ void reduceGmem(int *g_idata, int *g_odata, unsigned int n) {\n","    //# set thread ID\n","    unsigned int tid = threadIdx.x;\n","    int *idata = g_idata + blockIdx.x * blockDim.x;\n","\n","    //# boundary check\n","    unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if (idx >= n) return;\n","\n","    //# in-place reduction in global memory\n","    if (blockDim.x >= 1024 && tid < 512) idata[tid] += idata[tid + 512];\n","\n","    __syncthreads();\n","\n","    if (blockDim.x >= 512 && tid < 256) idata[tid] += idata[tid + 256];\n","\n","    __syncthreads();\n","\n","    if (blockDim.x >= 256 && tid < 128) idata[tid] += idata[tid + 128];\n","\n","    __syncthreads();\n","\n","    if (blockDim.x >= 128 && tid < 64) idata[tid] += idata[tid + 64];\n","\n","    __syncthreads();\n","\n","    // unrolling warp\n","    if (tid < 32) \n","      warpReduce(idata, tid);\n","\n","    // write result for this block to global mem\n","    if (tid == 0) g_odata[blockIdx.x] = idata[0];\n","}\n","\n","__global__ void reduceSmem(int *g_idata, int *g_odata, unsigned int n) {\n","    __shared__ int smem[DIM];\n","\n","    // set thread ID\n","    unsigned int tid = threadIdx.x;\n","\n","    // boundary check\n","    unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if (idx >= n) return;\n","\n","    // convert global data pointer to the local pointer of this block\n","    int *idata = g_idata + blockIdx.x * blockDim.x;\n","\n","    // set to smem by each threads\n","    smem[tid] = idata[tid];\n","    __syncthreads();\n","\n","    // in-place reduction in shared memory\n","    if (blockDim.x >= 1024 && tid < 512) smem[tid] += smem[tid + 512];\n","\n","    __syncthreads();\n","\n","    if (blockDim.x >= 512 && tid < 256) smem[tid] += smem[tid + 256];\n","\n","    __syncthreads();\n","\n","    if (blockDim.x >= 256 && tid < 128) smem[tid] += smem[tid + 128];\n","\n","    __syncthreads();\n","\n","    if (blockDim.x >= 128 && tid < 64)  smem[tid] += smem[tid + 64];\n","\n","    __syncthreads();\n","\n","    // unrolling warp\n","    if (tid < 32)\n","      warpReduce(smem, tid);\n","\n","    // write result for this block to global mem\n","    if (tid == 0) g_odata[blockIdx.x] = smem[0];\n","}\n","\n","__global__ void reduceSmemDyn(int *g_idata, int *g_odata, unsigned int n) {\n","    extern __shared__ int smem[];\n","\n","    // set thread ID\n","    unsigned int tid = threadIdx.x;\n","    int *idata = g_idata + blockIdx.x * blockDim.x;\n","\n","    // set to smem by each threads\n","    smem[tid] = idata[tid];\n","    __syncthreads();\n","\n","    // in-place reduction in global memory\n","    if (blockDim.x >= 1024 && tid < 512)  smem[tid] += smem[tid + 512];\n","\n","    __syncthreads();\n","\n","    if (blockDim.x >= 512 && tid < 256)  smem[tid] += smem[tid + 256];\n","\n","    __syncthreads();\n","\n","    if (blockDim.x >= 256 && tid < 128) smem[tid] += smem[tid + 128];\n","\n","    __syncthreads();\n","\n","    if (blockDim.x >= 128 && tid < 64) smem[tid] += smem[tid + 64];\n","\n","    __syncthreads();\n","\n","    // unrolling warp\n","    if (tid < 32) \n","      warpReduce(smem, tid);\n","\n","    // write result for this block to global mem\n","    if (tid == 0) g_odata[blockIdx.x] = smem[0];\n","}\n","\n","//# unroll4 + complete unroll for loop + gmem\n","__global__ void reduceGmemUnroll(int *g_idata, int *g_odata, unsigned int n) {\n","  // set thread ID\n","  unsigned int tid = threadIdx.x;\n","  unsigned int idx = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n","\n","  // convert global data pointer to the local pointer of this block\n","  int *idata = g_idata + blockIdx.x * blockDim.x * 4;\n","\n","  // unrolling 4\n","  if (idx + 3 * blockDim.x < n) {\n","    int a1 = g_idata[idx];\n","    int a2 = g_idata[idx + blockDim.x];\n","    int a3 = g_idata[idx + 2 * blockDim.x];\n","    int a4 = g_idata[idx + 3 * blockDim.x];\n","    g_idata[idx] = a1 + a2 + a3 + a4;\n","  }\n","\n","  __syncthreads();\n","\n","  // in-place reduction in global memory\n","  if (blockDim.x >= 1024 && tid < 512) idata[tid] += idata[tid + 512];\n","\n","  __syncthreads();\n","\n","  if (blockDim.x >= 512 && tid < 256) idata[tid] += idata[tid + 256];\n","\n","  __syncthreads();\n","\n","  if (blockDim.x >= 256 && tid < 128) idata[tid] += idata[tid + 128];\n","\n","  __syncthreads();\n","\n","  if (blockDim.x >= 128 && tid < 64) idata[tid] += idata[tid + 64];\n","\n","  __syncthreads();\n","\n","  // unrolling warp\n","  if (tid < 32)\n","    warpReduce(idata, tid);\n","\n","  // write result for this block to global mem\n","  if (tid == 0) g_odata[blockIdx.x] = idata[0];\n","}\n","\n","__global__ void reduceSmemUnroll(int *g_idata, int *g_odata, unsigned int n) {\n","    // static shared memory\n","    __shared__ int smem[DIM];\n","\n","    // set thread ID\n","    unsigned int tid = threadIdx.x;\n","\n","    // global index, 4 blocks of input data processed at a time\n","    unsigned int idx = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n","\n","    // unrolling 4 blocks\n","    int tmpSum = 0;\n","\n","    // boundary check\n","    if (idx + 4 * blockDim.x <= n) {\n","      int a1 = g_idata[idx];\n","      int a2 = g_idata[idx + blockDim.x];\n","      int a3 = g_idata[idx + 2 * blockDim.x];\n","      int a4 = g_idata[idx + 3 * blockDim.x];\n","      tmpSum = a1 + a2 + a3 + a4;\n","    }\n","\n","    smem[tid] = tmpSum;\n","    __syncthreads();\n","\n","    // in-place reduction in shared memory\n","    if (blockDim.x >= 1024 && tid < 512) smem[tid] += smem[tid + 512];\n","\n","    __syncthreads();\n","\n","    if (blockDim.x >= 512 && tid < 256)  smem[tid] += smem[tid + 256];\n","\n","    __syncthreads();\n","\n","    if (blockDim.x >= 256 && tid < 128)  smem[tid] += smem[tid + 128];\n","\n","    __syncthreads();\n","\n","    if (blockDim.x >= 128 && tid < 64)   smem[tid] += smem[tid + 64];\n","\n","    __syncthreads();\n","\n","    // unrolling warp\n","    if (tid < 32) \n","      warpReduce(smem, tid);\n","\n","    // write result for this block to global mem\n","    if (tid == 0) g_odata[blockIdx.x] = smem[0];\n","}\n","\n","__global__ void reduceSmemUnrollDyn(int *g_idata, int *g_odata, unsigned int n) {\n","    extern __shared__ int smem[];\n","\n","    // set thread ID\n","    unsigned int tid = threadIdx.x;\n","    unsigned int idx = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n","\n","    // unrolling 4\n","    int tmpSum = 0;\n","\n","    if (idx + 3 * blockDim.x < n) {\n","      int a1 = g_idata[idx];\n","      int a2 = g_idata[idx + blockDim.x];\n","      int a3 = g_idata[idx + 2 * blockDim.x];\n","      int a4 = g_idata[idx + 3 * blockDim.x];\n","      tmpSum = a1 + a2 + a3 + a4;\n","    }\n","\n","    smem[tid] = tmpSum;\n","    __syncthreads();\n","\n","    // in-place reduction in global memory\n","    if (blockDim.x >= 1024 && tid < 512)  smem[tid] += smem[tid + 512];\n","\n","    __syncthreads();\n","\n","    if (blockDim.x >= 512 && tid < 256)  smem[tid] += smem[tid + 256];\n","\n","    __syncthreads();\n","\n","    if (blockDim.x >= 256 && tid < 128) smem[tid] += smem[tid + 128];\n","\n","    __syncthreads();\n","\n","    if (blockDim.x >= 128 && tid < 64) smem[tid] += smem[tid + 64];\n","\n","    __syncthreads();\n","\n","    // unrolling warp\n","    if (tid < 32) \n","      warpReduce(smem, tid);\n","\n","    // write result for this block to global mem\n","    if (tid == 0) g_odata[blockIdx.x] = smem[0];\n","}\n","\n","__global__ void reduceNeighboredGmem(int *g_idata, int *g_odata, unsigned int  n) {\n","    // set thread ID\n","    unsigned int tid = threadIdx.x;\n","    unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    // convert global data pointer to the local pointer of this block\n","    int *idata = g_idata + blockIdx.x * blockDim.x;\n","\n","    // boundary check\n","    if (idx >= n) return;\n","\n","    // in-place reduction in global memory\n","    for (int stride = 1; stride < blockDim.x; stride *= 2) {\n","      if ((tid % (2 * stride)) == 0)\n","        idata[tid] += idata[tid + stride];\n","\n","      // synchronize within threadblock\n","      __syncthreads();\n","    }\n","\n","    // write result for this block to global mem\n","    if (tid == 0) g_odata[blockIdx.x] = idata[0];\n","}\n","\n","__global__ void reduceNeighboredSmem(int *g_idata, int *g_odata, unsigned int  n) {\n","    __shared__ int smem[DIM];\n","\n","    // set thread ID\n","    unsigned int tid = threadIdx.x;\n","    unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    // convert global data pointer to the local pointer of this block\n","    int *idata = g_idata + blockIdx.x * blockDim.x;\n","\n","    // boundary check\n","    if (idx >= n) return;\n","\n","    smem[tid] = idata[tid];\n","    __syncthreads();\n","\n","    // in-place reduction in global memory\n","    for (int stride = 1; stride < blockDim.x; stride *= 2)\n","    {\n","        if ((tid % (2 * stride)) == 0)\n","        {\n","            smem[tid] += smem[tid + stride];\n","        }\n","\n","        // synchronize within threadblock\n","        __syncthreads();\n","    }\n","\n","    // write result for this block to global mem\n","    if (tid == 0) g_odata[blockIdx.x] = smem[0];\n","}\n","\n","int main(int argc, char **argv) {\n","  // set up device\n","  int dev = 0;\n","  cudaDeviceProp deviceProp;\n","  CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n","  printf(\"%s starting reduction at \", argv[0]);\n","  printf(\"device %d: %s \", dev, deviceProp.name);\n","  CHECK(cudaSetDevice(dev));\n","\n","  bool bResult = false;\n","\n","  // initialization\n","  int power = 10;\n","\n","  // execution configuration\n","  int blocksize = DIM;   // initial block size\n","\n","  if (argc >= 2) \n","      blocksize = atoi(argv[1]);\n","\n","  if (argc >= 3)\n","      power = atoi(argv[2]);\n","\n","  int size = 1 << power; // total number of elements to reduce\n","  printf(\"    with array size %d  \", size);\n","\n","  dim3 block (blocksize, 1);\n","  dim3 grid  ((size + block.x - 1) / block.x, 1);\n","  printf(\"grid %d block %d\\n\", grid.x, block.x);\n","\n","  // allocate host memory\n","  size_t bytes = size * sizeof(int);\n","  int *h_idata = (int *) malloc(bytes);\n","  int *h_odata = (int *) malloc(grid.x * sizeof(int));\n","  int *tmp     = (int *) malloc(bytes);\n","\n","  // initialize the array\n","  for (int i = 0; i < size; i++)\n","    h_idata[i] = (int)( rand() & 0xFF );\n","\n","  memcpy (tmp, h_idata, bytes);\n","\n","  int gpu_sum = 0;\n","\n","  // allocate device memory\n","  int *d_idata = NULL;\n","  int *d_odata = NULL;\n","  CHECK(cudaMalloc((void **) &d_idata, bytes));\n","  CHECK(cudaMalloc((void **) &d_odata, grid.x * sizeof(int)));\n","\n","  // cpu reduction\n","  int cpu_sum = recursiveReduce (tmp, size);\n","  printf(\"cpu reduce          : %d\\n\", cpu_sum);\n","\n","  // reduce gmem\n","  CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n","  reduceNeighboredGmem<<<grid.x, block>>>(d_idata, d_odata, size);\n","  CHECK(cudaGetLastError());\n","  CHECK(cudaMemcpy(h_odata, d_odata, grid.x * sizeof(int), cudaMemcpyDeviceToHost));\n","  gpu_sum = 0;\n","\n","  for (int i = 0; i < grid.x; i++) \n","    gpu_sum += h_odata[i];\n","\n","  printf(\"reduceNeighboredGmem: %d <<<grid %d block %d>>>\\n\", gpu_sum, grid.x, block.x);\n","\n","  // reduce gmem\n","  CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n","  reduceNeighboredSmem<<<grid.x, block>>>(d_idata, d_odata, size);\n","  CHECK(cudaGetLastError());\n","  CHECK(cudaMemcpy(h_odata, d_odata, grid.x * sizeof(int), cudaMemcpyDeviceToHost));\n","  gpu_sum = 0;\n","\n","  for (int i = 0; i < grid.x; i++) gpu_sum += h_odata[i];\n","\n","  printf(\"reduceNeighboredSmem: %d <<<grid %d block %d>>>\\n\", gpu_sum, grid.x, block.x);\n","\n","  // reduce gmem\n","  CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n","  reduceGmem<<<grid.x, block>>>(d_idata, d_odata, size);\n","  CHECK(cudaGetLastError());\n","  CHECK(cudaMemcpy(h_odata, d_odata, grid.x * sizeof(int), cudaMemcpyDeviceToHost));\n","  gpu_sum = 0;\n","\n","  for (int i = 0; i < grid.x; i++) gpu_sum += h_odata[i];\n","\n","  printf(\"reduceGmem          : %d <<<grid %d block %d>>>\\n\", gpu_sum, grid.x, block.x);\n","\n","  // reduce smem\n","  CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n","  reduceSmem<<<grid.x, block>>>(d_idata, d_odata, size);\n","  CHECK(cudaGetLastError());\n","  CHECK(cudaMemcpy(h_odata, d_odata, grid.x * sizeof(int), cudaMemcpyDeviceToHost));\n","  gpu_sum = 0;\n","\n","  for (int i = 0; i < grid.x; i++) gpu_sum += h_odata[i];\n","\n","  printf(\"reduceSmem          : %d <<<grid %d block %d>>>\\n\", gpu_sum, grid.x, block.x);\n","\n","  // reduce smem\n","  CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n","  reduceSmemDyn<<<grid.x, block, blocksize*sizeof(int)>>>(d_idata, d_odata,\n","          size);\n","  CHECK(cudaGetLastError());\n","  CHECK(cudaMemcpy(h_odata, d_odata, grid.x * sizeof(int), cudaMemcpyDeviceToHost));\n","  gpu_sum = 0;\n","\n","  for (int i = 0; i < grid.x; i++) gpu_sum += h_odata[i];\n","\n","  printf(\"reduceSmemDyn       : %d <<<grid %d block %d>>>\\n\", gpu_sum, grid.x, block.x);\n","\n","  // reduce gmem\n","  CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n","  reduceGmemUnroll<<<grid.x / 4, block>>>(d_idata, d_odata, size);\n","  CHECK(cudaGetLastError());\n","  CHECK(cudaMemcpy(h_odata, d_odata, grid.x / 4 * sizeof(int), cudaMemcpyDeviceToHost));\n","  gpu_sum = 0;\n","\n","  for (int i = 0; i < grid.x / 4; i++) gpu_sum += h_odata[i];\n","\n","  printf(\"reduceGmemUnroll4   : %d <<<grid %d block %d>>>\\n\", gpu_sum, grid.x / 4, block.x);\n","\n","  // reduce smem\n","  CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n","  reduceSmemUnroll<<<grid.x / 4, block>>>(d_idata, d_odata, size);\n","  CHECK(cudaGetLastError());\n","  CHECK(cudaMemcpy(h_odata, d_odata, grid.x / 4 * sizeof(int), cudaMemcpyDeviceToHost));\n","  gpu_sum = 0;\n","\n","  for (int i = 0; i < grid.x / 4; i++) gpu_sum += h_odata[i];\n","\n","  printf(\"reduceSmemUnroll4   : %d <<<grid %d block %d>>>\\n\", gpu_sum, grid.x / 4, block.x);\n","\n","  // reduce smem\n","  CHECK(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));\n","  reduceSmemUnrollDyn<<<grid.x / 4, block, DIM*sizeof(int)>>>(d_idata, d_odata, size);\n","  CHECK(cudaGetLastError());\n","  CHECK(cudaMemcpy(h_odata, d_odata, grid.x / 4 * sizeof(int), cudaMemcpyDeviceToHost));\n","  gpu_sum = 0;\n","\n","  for (int i = 0; i < grid.x / 4; i++) gpu_sum += h_odata[i];\n","\n","  printf(\"reduceSmemDynUnroll4: %d <<<grid %d block %d>>>\\n\", gpu_sum, grid.x / 4, block.x);\n","\n","  // free host memory\n","  free(h_idata);\n","  free(h_odata);\n","\n","  // free device memory\n","  CHECK(cudaFree(d_idata));\n","  CHECK(cudaFree(d_odata));\n","\n","  // reset device\n","  CHECK(cudaDeviceReset());\n","\n","  // check the results\n","  bResult = (gpu_sum == cpu_sum);\n","\n","  if(!bResult) printf(\"Test failed!\\n\");\n","\n","  return EXIT_SUCCESS;\n","}\n"],"metadata":{"id":"aF_XWzGyNmBL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvcc -arch=sm_75  src/preduceSMEM.cu -o preduce\n","!./preduce 1024 20"],"metadata":{"id":"HCyY1Y8_sB0S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvcc -arch=sm_75 --ptxas-options=-v src/preduceSMEM.cu -o preduce\n","!./preduce 1024 20"],"metadata":{"id":"sCBstFF3U1Ya"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vXUIQkZLCTcG"},"source":["# ‚úÖ Moltiplicazione matriciale con SMEM\n"]},{"cell_type":"markdown","source":["**Prodotto di matrici con SMEM**\n","\n","Scrivere un programma CUDA per prodotto matrici $C = A*B$ che usi la SMEM e riduca cos√¨ il 'traffico' in global mem\n","\n","**passi:**\n","1. Definire la SMEM per ogni blocco della matrice $C$\n","2. Svolgere un ciclo sui blocchi per caricare la SMEM da global mem\n","3. Sincronizzare -1-\n","4. Nel ciclo effettuare localmente all‚Äôinterno di ogni blocco il calcolo del prodotto riga-colonna e caricare su registro\n","5. sincronizzare -2-\n","6. Scrivere il risultato finale su matrice prodotto in global mem\n"],"metadata":{"id":"Kdv736GrlSf1"}},{"cell_type":"markdown","source":["# üî¥ TODO"],"metadata":{"id":"8B0w12MS4xtP"}},{"cell_type":"code","metadata":{"id":"-Y52R0d3CA50"},"source":["%%cuda --name matmulSMEM.cu\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include \"../GPUcomputing/utils/common.h\"\n","\n","#define IDX(i,j,n) (i*n+j)\n","#define ABS(x,y) (x-y>=0?x-y:y-x)\n","#define N 1024\n","#define P 1024\n","#define M 1024\n","#define BLOCK_SIZE 32\n","\n","\n","/*\n"," * Kernel for matrix product with static SMEM\n"," *      C  =  A  *  B\n"," *    (NxM) (MxP) (PxM)\n"," */\n","__global__ void matmulSMEMstatic(float* A, float* B, float* C) {\n","\t\n","\t//# TODO\n","\t\n","}\n","\n","/*\n"," * Kernel for matrix product using dynamic SMEM\n"," */\n","__global__ void matmulSMEMdynamic(float* A, float* B, float* C, const uint SMEMsize) {\n","\t\n","\t//# TODO\n","}\n","\n","/*\n"," * Kernel for naive matrix product\n"," */\n","__global__ void matmul(float* A, float* B, float* C) {\n","\t// indexes\n","\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n","\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// each thread computes an entry of the product matrix\n","\tif ((row < N) && (col < M)) {\n","\t\tfloat sum = 0;\n","\t\tfor (int k = 0; k < P; k++)\n","\t\t\tsum += A[IDX(row, k, P)] * B[IDX(k, col, M)];\n","\t\tC[IDX(row, col, M)] = sum;\n","\t}\n","}\n","\n","/*\n"," *  matrix product on CPU\n"," */\n","void matmulCPU(float* A, float* B, float* C) {\n","\tfor (int row = 0; row < N; row++)\n","\t\tfor (int col = 0; col < M; col++) {\n","\t\t\tfloat sum = 0;\n","\t\t\tfor (int k = 0; k < P; k++)\n","\t\t\t\tsum += A[IDX(row, k, P)] * B[IDX(k, col, M)];\n","\t\t\tC[IDX(row, col, M)] = sum;\n","\t\t}\n","}\n","\n","/*\n"," * Test the device\n"," */\n","unsigned long testCUDADevice(void) {\n","\tint dev = 0;\n","\n","\tcudaDeviceSetCacheConfig (cudaFuncCachePreferEqual);\n","\tcudaDeviceProp deviceProp;\n","\tcudaSetDevice(dev);\n","\tcudaGetDeviceProperties(&deviceProp, dev);\n","\tprintf(\"Device %d: \\\"%s\\\"\\n\", dev, deviceProp.name);\n","\tprintf(\"Total amount of shared memory available per block: %lu KB\\n\",\n","\t\t\tdeviceProp.sharedMemPerBlock / 1024);\n","\treturn deviceProp.sharedMemPerBlock;\n","}\n","\n","/*\n"," * elementwise comparison between two mqdb\n"," */\n","void checkResult(float *A, float *B) {\n","\tdouble epsilon = 1.0E-8;\n","\tbool match = 1;\n","\tfor (int i = 0; i < N*M; i++)\n","\t\tif (ABS(A[i], B[i]) > epsilon) {\n","\t\t\tmatch = 0;\n","\t\t\tprintf(\"   * Arrays do not match!\\n\");\n","\t\t\tbreak;\n","\t\t}\n","\tif (match)\n","\t\tprintf(\"   Arrays match\\n\\n\");\n","}\n","\n","/*\n"," * MAIN\n"," */\n","int main(void) {\n","\t // Kernels for matrix product\n","\t //      C  =  A  *  B\n","\t //    (NxM) (NxP) (PxM)\n","\tprintf(\"N = %d, M = %d, K = %d\\n\", N, M, P);\n","\tuint rowA = N, rowB = P;\n","\tuint colA = P, colB = M;\n","\tuint rowC = N, colC = M;\n","\tfloat *A, *B, *C, *C1;\n","\tfloat *dev_A, *dev_B, *dev_C;\n","\n","\t// dims\n","\tunsigned long Asize = rowA * colA * sizeof(float);\n","\tunsigned long Bsize = rowB * colB * sizeof(float);\n","\tunsigned long Csize = rowC * colC * sizeof(float);\n","\t\n","\t// malloc host memory\n","\tA = (float*) malloc(Asize);\n","\tB = (float*) malloc(Bsize);\n","\tC = (float*) malloc(Csize);\n","\tC1 = (float*) malloc(Csize);\n","\n","\t// device SMEM available/ test device shared memory\n","\tunsigned long maxSMEMbytes = testCUDADevice();\n","\t\n","\n","\t// malloc device memory\n","\tCHECK(cudaMalloc((void** )&dev_A, Asize));\n","\tCHECK(cudaMalloc((void** )&dev_B, Bsize));\n","\tCHECK(cudaMalloc((void** )&dev_C, Csize));\n","\tprintf(\"Total amount of allocated memory on GPU %.2f MB\\n\\n\", (float)(Asize + Bsize + Csize)/(1024.0*1024.0));\n","\n","\t// fill the matrices A and B\n","\tfor (int i = 0; i < N * P; i++) A[i] = 1.0;\n","\tfor (int i = 0; i < P * M; i++) B[i] = 1.0;\n","\n","\t/***********************************************************/\n","\t/*                       CPU matmul                       */\n","\t/***********************************************************/\n","\tprintf(\"\\n   *** CPU & NAIVE KERNEL ***\\n\\n\");\n","\tdouble start = seconds();\n","\tmatmulCPU(A, B, C);\n","\tprintf(\"   matmul elapsed time CPU = %f\\n\\n\", seconds() - start);\n","\n","\n","\t// copy matrices A and B to the GPU\n","\tCHECK(cudaMemcpy(dev_A, A, Asize, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMemcpy(dev_B, B, Bsize, cudaMemcpyHostToDevice));\n","\n","\t/***********************************************************/\n","\t/*                    GPU naive matmul                     */\n","\t/***********************************************************/\n","\t// grid block dims = shared mem dims = BLOCK_SIZE\n","\tdim3 block(BLOCK_SIZE, BLOCK_SIZE);\n","\tdim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n","\tstart = seconds();\n","\tmatmul<<<grid, block>>>(dev_A, dev_B, dev_C);\n","\tCHECK(cudaDeviceSynchronize());\n","\tprintf(\"   Kernel naive matmul elapsed time GPU = %f\\n\", seconds() - start);\n","\n","\t// copy the array 'C' back from the GPU to the CPU\n","\tCHECK(cudaMemcpy(C1, dev_C, Csize, cudaMemcpyDeviceToHost));\n","\tcheckResult(C,C1);\n","\n","\t/***********************************************************/\n","\t/*              GPU matmulSMEM static SMEM               */\n","\t/***********************************************************/\n","\t// grid block dims = shared mem dims = BLOCK_SIZE\n","\tprintf(\"\\n   *** USING STATIC SMEM ***\\n\\n\");\n","\tstart = seconds();\n","\tmatmulSMEMstatic<<<grid, block>>>(dev_A, dev_B, dev_C);\n","\tCHECK(cudaDeviceSynchronize());\n","\tprintf(\"   Kernel matmulSMEM static elapsed time GPU = %f\\n\", seconds() - start);\n","\t\n","\t// amount of SMEM used\n","\tuint SMEMsize = 2 * BLOCK_SIZE * BLOCK_SIZE * sizeof(float);\n","\t\tif (maxSMEMbytes < SMEMsize)\n","\t\t\tprintf(\"   Shared memory usage WARNING: available: %lu, required: %d bytes\\n\",\tmaxSMEMbytes, SMEMsize);\n","\t\telse\n","\t\t\tprintf(\"   Total amount of shared memory required per block %.1f KB\\n\", (float) SMEMsize / (float) 1024);\n","\t\n","\t// copy the array 'C' back from the GPU to the CPU\n","\tCHECK(cudaMemcpy(C1, dev_C, Csize, cudaMemcpyDeviceToHost));\n","\tcheckResult(C,C1);\n","\t\n","\t/***********************************************************/\n","\t/*            GPU matmulSMEMD dynamic SMEM                */\n","\t/***********************************************************/\n","\t// set cache size\n","\tcudaDeviceSetCacheConfig (cudaFuncCachePreferShared);\n","\tprintf(\"\\n   *** USING DYNAMIC SMEM ***\\n\\n\");\n","\n","\t// try with various SMEM sizes\n","\tuint sizes[] = {8, 16, 32};\n","\tfor (int i = 0; i < 3; i++) {\n","\t\tuint blockSize = sizes[i];\n","\t\tblock.x = blockSize;\n","\t\tblock.y = blockSize;\n","\t\tgrid.x = (M + block.x - 1) / block.x;\n","\t\tgrid.y = (N + block.y - 1) / block.y;\n","\t\tuint SMEMsize = blockSize * blockSize;\n","\t\tuint SMEMbyte = 2 * SMEMsize * sizeof(float);\n","\t\tstart = seconds();\n","\t\tmatmulSMEMdynamic<<< grid, block, SMEMbyte >>>(dev_A, dev_B, dev_C, SMEMsize);\n","\t\tCHECK(cudaDeviceSynchronize());\n","\t\tprintf(\"   Kernel matmulSMEM dynamic (SMEM size %d) elapsed time GPU = %f\\n\", blockSize, seconds() - start);\n","\n","\t\t// amount of SMEM used\n","\t\tif (maxSMEMbytes < SMEMbyte)\n","\t\t\tprintf(\"   Shared memory usage WARNING: available: %lu, required: %d bytes\\n\",\tmaxSMEMbytes, SMEMbyte);\n","\t\telse\n","\t\t\tprintf(\"   Total amount of shared memory required per block %.1f KB\\n\", (float) SMEMbyte / (float) 1024);\n","\n","\t\t// copy the array 'C' back from the GPU to the CPU\n","\t\tCHECK(cudaMemcpy(C1, dev_C, Csize, cudaMemcpyDeviceToHost));\n","\t\tcheckResult(C,C1);\n","\t}\n","\n","\t// free the memory allocated on the GPU\n","\tcudaFree(dev_A);\n","\tcudaFree(dev_B);\n","\tcudaFree(dev_C);\n","\n","\tcudaDeviceReset();\n","\treturn EXIT_SUCCESS;\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0PSc9B9PDTWt"},"source":["# Compilazione ed esecuzione\n","\n","!nvcc -arch=sm_70 src/matmulSMEM.cu  -o matmulSMEM\n","!matmulSMEM"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mfR471rze2p-"},"source":["!ls -la"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SOFMQZAkjlLW"},"source":["# ‚úÖ Convoluzione con SMEM"]},{"cell_type":"code","metadata":{"id":"v9nRkLgeB10A"},"source":["%%cuda --name conv1D.cu\n","\n","#include <stdlib.h>\n","#include <stdio.h>\n","#include \"../GPUcomputing/utils/common.h\"\n","\n","#define MASK_RADIUS  5\n","#define MASK_SIZE    2 * MASK_RADIUS + 1\n","#define BLOCK_SIZE   128\n","#define TILE_WIDTH   BLOCK_SIZE + MASK_SIZE - 1\n","\n","__device__ __constant__ float d_mask[MASK_SIZE];\n","\n","void initialData(float*, int);\n","void movingAverage(float*, int n);\n","void printData(float*, const int);\n","void convolutionHost(float*, float*, float*, const int);\n","void checkResult(float*, float*, int);\n","\n","/*\n"," *# kernel for 1D convolution: it holds only if MASK_RADIUS < BLOCK_SIZE\n"," */\n","__global__ void convolution1D(float *result, float *data, int n) {\n","\tunsigned int i = blockDim.x * blockIdx.x + threadIdx.x;\n","\n","\t//# shared memory size = BLOCK_SIZE + MASK\n","\t__shared__ float tile[TILE_WIDTH];\n","\n","\t//# boundary\n","\tint left = blockIdx.x * blockDim.x - MASK_RADIUS;\n","\tint right = (blockIdx.x + 1) * blockDim.x;\n","\n","  //# left halo\n","\tif (threadIdx.x < MASK_RADIUS)                      \n","\t\ttile[threadIdx.x] = left < 0 ? 0 : data[left + threadIdx.x];\n","\n","  //# center\n","\ttile[threadIdx.x + MASK_RADIUS] = data[i];\n","\n","  //# right halo  \n","\tif (threadIdx.x >= blockDim.x - MASK_RADIUS)  \n","\t\ttile[threadIdx.x + MASK_SIZE - 1] = right >= n ? 0 :\n","\t\t\t\tdata[right + threadIdx.x - blockDim.x + MASK_RADIUS];\n","\n","\t__syncthreads();\n","\n","\t//# convolution: tile * mask\n","\tfloat sum = 0;\n","\tfor (int i = -MASK_RADIUS; i <= MASK_RADIUS; i++)\n","\t\tsum += tile[threadIdx.x + MASK_RADIUS + i] * d_mask[i + MASK_RADIUS];\n","\n","\t//# final result\n","\tresult[i] = sum;\n","}\n","\n","/*\n"," * MAIN: convolution 1D host & device\n"," */\n","int main(int argc, char **argv) {\n","\t// set up device\n","\tint dev = 0;\n","\tcudaDeviceProp deviceProp;\n","\tCHECK(cudaGetDeviceProperties(&deviceProp, dev));\n","\tprintf(\"starting conv1D at device %d: %s\\n\", dev, deviceProp.name);\n","\tCHECK(cudaSetDevice(dev));\n","\n","\t// set up array size\n","\tint n = 1 << 24;\n","\tint N = MASK_SIZE;\n","\n","\tprintf(\"Array of size = %.1f MB\\n\", n/(1024.0*1024.0));\n","\n","\t// mem sizes\n","\tsize_t nBytes = n * sizeof(float);\n","\tsize_t nBytes_mask = N * sizeof(float);\n","\n","\t// grid configuration\n","\tdim3 block(BLOCK_SIZE);\n","\tdim3 grid((n + BLOCK_SIZE - 1) / BLOCK_SIZE);\n","\n","\t// allocate host memory\n","\tfloat *h_data = (float *) malloc(nBytes);\n","\tfloat *h_result = (float *) malloc(nBytes);\n","\tfloat *result = (float *) malloc(nBytes);\n","\tfloat *h_mask = (float *) malloc(nBytes_mask);\n","\n","\t//  initialize host array\n","\tmovingAverage(h_mask, N);\n","\tinitialData(h_data, n);\n","\n","\t// convolution on host\n","\tdouble start = seconds();\n","\tconvolutionHost(h_data, result, h_mask, n);\n","\tdouble hostElaps = seconds() - start;\n","\n","\t// allocate device memory\n","\tfloat *d_data, *d_result;\n","\tCHECK(cudaMalloc((void**)&d_data, nBytes));\n","\tCHECK(cudaMalloc((void**)&d_result, nBytes));\n","\n","\t// copy data from host to device\n","\tCHECK(cudaMemcpy(d_data, h_data, nBytes, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMemcpyToSymbol(d_mask, h_mask, nBytes_mask));\n","\n","\tstart = seconds();\n","\tconvolution1D<<<grid, block>>>(d_result, d_data, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tdouble devElaps = seconds() - start;\n","  printf(\"Times:\\n\");\n","\tprintf(\"   - CPU elapsed time = %f\\n\", hostElaps);\n","  printf(\"   - GPU elapsed time = %f\\n\", devElaps);\n","  printf(\"   - Speed-up (ratio) = %f\\n\", hostElaps / devElaps);\n","\n","\tCHECK(cudaMemcpy(h_result, d_result, nBytes, cudaMemcpyDeviceToHost));\n","\n","\t// check result\n","\tcheckResult(h_result, result, n);\n","\n","\t// free host and device memory\n","\tCHECK(cudaFree(d_result));\n","\tCHECK(cudaFree(d_data));\n","\tfree(h_data);\n","\tfree(h_mask);\n","\tfree(h_result);\n","\tfree(result);\n","\n","\t// reset device\n","\tCHECK(cudaDeviceReset());\n","\treturn EXIT_SUCCESS;\n","}\n","\n","void initialData(float *h_data, int n) {\n","\t// initialize the data\n","\tfor (int i = 0; i < n; i++)\n","\t\th_data[i] = 10.0;\n","}\n","\n","void movingAverage(float *h_mask, int n) {\n","\t// initialize mask moving average\n","\tfor (int i = 0; i < n; i++)\n","\t\th_mask[i] = 1.0 / ((float) n);\n","\treturn;\n","}\n","\n","void printData(float *a, const int size) {\n","\tprintf(\"\\n\");\n","\tfor (int i = 0; i < size; i++)\n","\t\tprintf(\"%.2f \", a[i]);\n","\tprintf(\"\\n\");\n","\treturn;\n","}\n","\n","void convolutionHost(float *data, float *result, float *mask, const int n) {\n","\tfor (int i = 0; i < n; i++) {\n","\t\tfloat sum = 0;\n","\t\tfor (int j = 0; j < MASK_SIZE; j++) {\n","\t\t\tint idx = i - MASK_RADIUS + j;\n","\t\t\tif (idx >= 0 && idx < n)\n","\t\t\t\tsum += data[idx] * mask[j];\n","\t\t}\n","\t\tresult[i] = sum;\n","\t}\n","}\n","\n","void checkResult(float *d_result, float *h_result, int n) {\n","\tdouble epsilon = 1.0E-8;\n","\n","\tfor (int i = 0; i < n; i++)\n","\t\tif (abs(h_result[i] - d_result[i]) > epsilon) {\n","\t\t\tprintf(\"different on entry (%d) |h_result - d_result| >  %f\\n\", i,\n","\t\t\t\t\tepsilon);\n","\t\t\tbreak;\n","\t\t}\n","}\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wLxZjCx8bT3s"},"source":["# Compilazione ed esecuzione\n","!nvcc -arch=sm_70  src/conv1D.cu -o conv1D\n","!./conv1D"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üî¥ TODO"],"metadata":{"id":"hI7kdGlx72HO"}},{"cell_type":"code","metadata":{"id":"bwcTDn6ehJr_"},"source":["%%cuda --name conv2D.cu\n","#include <stdlib.h>\n","#include <string.h>\n","\n","#include \"../GPUcomputing/utils/common.h\"\n","\n","#define DATA_WIDTH   (20*1024)\n","#define DATA_HEIGHT  (20*1024)\n","#define BLOCK_SIZE   8\n","#define MASK_RADIUS  2\n","#define MASK_SIZE    (2 * MASK_RADIUS + 1)\n","#define TILE_WIDTH   (BLOCK_SIZE + MASK_SIZE - 1)\n","#define DEBUG 0\n","\n","// constant mem\n","__constant__ float M_dev[MASK_SIZE*MASK_SIZE];\n","\n","/*\n"," * kernel for convolution 2D (it holds only if MASK_RADIUS < BLOCK_SIZE)\n"," */\n","__global__ void conv2D(float *A, float *B) {\n","\t\n","\t//# TODO\n","}\n","\n","/*\n"," * Average filter\n"," */\n","void Avg_mask(float *mask) {\n","\tint n = MASK_SIZE;\n","\tfor (int i = 0; i < n*n; i++)\n","\t\tmask[i] = (float) 1.0f / (n * n);\n","}\n","\n","\n","/*\n"," * main\n"," */\n","int main(void) {\n","\n","  // check params\n","  if (MASK_RADIUS >= BLOCK_SIZE) {\n","    printf(\"ERROR: it holds only if MASK_RADIUS < BLOCK_SIZE!\\n\");\n","    return 1;\n","  }\n","\n","\tint nW = DATA_WIDTH;\n","  int nH = DATA_HEIGHT;\n","\tint b = BLOCK_SIZE;\n","\n","\tfloat M[MASK_SIZE*MASK_SIZE]; // const size\n","\tfloat *A, *B, *A_dev, *B_dev;\n","\tint datasize = nW * nH * sizeof(float);\n","  int masksize = MASK_SIZE*MASK_SIZE * sizeof(float);\n","\n","  printf(\"Data size: %.2f (MB)\\n\", (float)datasize/(1024.0*1024.0));\n","\tprintf(\"Initializing data...\\n\");\n","\tA = (float *) malloc(datasize);\n","\tB = (float *) malloc(datasize);\n","\n","\t// initialize data\n","\tfor (int i = 0; i < nH; i++)\n","\t\tfor (int j = 0; j < nW; j++)\n","\t\t\tA[i*nW+j] = rand()%10;\n","\n","  // initialize mask \n","\tAvg_mask(M);\n","\n","#if DEBUG\n","\t// print data\n","\tprintf(\"Print matrix A...\\n\");\n","\tfor (int i = 0; i < nH; i++) {\n","    if (i%8 == 0 && i>0)\n","      printf(\"\\n\");\n","\n","\t\tfor (int j = 0; j < nW; j++)\n","      if (j%8 == 0 && j>0)\n","\t\t\t  printf(\" %0.0f \", A[i*nW+j]);\n","      else\n","        printf(\"%0.0f \", A[i*nW+j]);\n","\t\tprintf(\"\\n\");\n","\t}\n","\n","\tprintf(\"Print matrix M ...\\n\");\n","\tfor (int i = 0; i < MASK_SIZE; i++) {\n","\t\tfor (int j = 0; j < MASK_SIZE; j++)\n","\t\t\t  printf(\" %1.2f \", M[i * MASK_SIZE + j]);\n","\t\tprintf(\"\\n\");\n","\t}\n","#endif\n","\n","\t// cuda allocation \n","\tCHECK(cudaMemcpyToSymbol(M_dev, M, masksize));\n","\tCHECK(cudaMalloc((void **) &A_dev, datasize));\n","\tCHECK(cudaMalloc((void **) &B_dev, datasize));\n","\tCHECK(cudaMemcpy(A_dev, A, datasize, cudaMemcpyHostToDevice));\n","\t\n","\t// block, grid dims, kernel\n","\tdim3 block(b, b);\n","\tdim3 grid((nW+b-1)/b, (nH+b-1)/b);\n","  double iStart, iElaps;\n","\tiStart = seconds();\n","\tconv2D<<<grid, block>>>(A_dev, B_dev);\n","  cudaDeviceSynchronize();\n","  iElaps = seconds() - iStart;\n","\tprintf(\"\\nconv2D<<<(%d,%d), (%d,%d)>>> elapsed time %f sec \\n\\n\", grid.x, grid.y, block.x, block.y, iElaps);\n","\tCHECK(cudaGetLastError());\n","\n","\tCHECK(cudaMemcpy(B, B_dev, datasize, cudaMemcpyDeviceToHost));\n","\n","#if DEBUG\n","\t// print out data\n","\tprintf(\"Print results...\\n\");\n","\tfor (int i = 0; i < nH; i++) {\n","    if (i%8 == 0 && i>0)\n","      printf(\"\\n\");\n","\t\tfor (int j = 0; j < nW; j++)\n","      if (j%8 == 0 && j>0)\n","\t\t\t  printf(\" %0.2f \", B[i*nW+j]);\n","      else\n","        printf(\"%0.2f \", B[i*nW+j]);\n","\t\tprintf(\"\\n\");\n","\t}\n","#endif\n","\n","\tcudaFree(A_dev);\n","\tcudaFree(B_dev);\n","  cudaDeviceReset();\n","\tfree(A);\n","\tfree(B);\n","\treturn 0;\n","}\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yiun00TE2wcE"},"source":["# Compilazione ed esecuzione\n","!nvcc -arch=sm_70  src/conv2D.cu -o conv2D\n","!./conv2D"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EZ6L97JrnNEl"},"execution_count":null,"outputs":[]}]}