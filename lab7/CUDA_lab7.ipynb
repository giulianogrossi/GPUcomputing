{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CUDA_lab7.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":["WoJbB3T5Vkw-","zs_a5Vuimily","iUYP4kCJhEIx","DvmApCF76YD0","vXUIQkZLCTcG","R1V3b-a3RjL3","SOFMQZAkjlLW","cqQnULvlE2fu","SbQlRthtJTQE"],"mount_file_id":"1mk0QXjREp-k_J-pdFfmgoC7-EVmgPyAo","authorship_tag":"ABX9TyOvp+g2QI3oHsQOCJbkCbmN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["---\n","# **LAB 7 - Global memory (GMEM)**\n","---"],"metadata":{"id":"fZYqN0UwVLC_"}},{"cell_type":"markdown","metadata":{"id":"WoJbB3T5Vkw-"},"source":["# ‚ñ∂Ô∏è CUDA setup"]},{"cell_type":"code","metadata":{"id":"Fht2Wy8wVkxJ"},"source":["!nvcc --version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0jP2H_YJVkxJ"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## [GPU Compute Capability](https://developer.nvidia.com/cuda-gpus)"],"metadata":{"id":"VKbaxH9wWosO"}},{"cell_type":"markdown","metadata":{"id":"_cGSqZovVkxK"},"source":["## NVCC Plugin for Jupyter notebook\n","\n","*Usage*:\n","\n","\n","*   Load Extension `%load_ext nvcc_plugin`\n","*   Mark a cell to be treated as cuda cell\n","`%%cuda --name example.cu --compile false`\n","\n","**NOTE**: The cell must contain either code or comments to be run successfully. It accepts 2 arguments. `-n | --name` - which is the name of either CUDA source or Header. The name parameter must have extension `.cu` or `.h`. Second argument -c | --compile; default value is false. The argument is a flag to specify if the cell will be compiled and run right away or not. It might be usefull if you're playing in the main function\n","\n","*  We are ready to run CUDA C/C++ code right in your Notebook. For this we need explicitly say to the interpreter, that we want to use the extension by adding `%%cu` at the beginning of each cell with CUDA code. \n","\n","\n"]},{"cell_type":"code","metadata":{"id":"RCVhMkqYVkxK"},"source":["!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e6PDOytTVkxK"},"source":["%load_ext nvcc_plugin"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Bash and data setup"],"metadata":{"id":"cReFlD-VRfZe"}},{"cell_type":"code","source":["#@title Bash setup\n","%%writefile /root/.bashrc\n","\n","# If not running interactively, don't do anything\n","[ -z \"$PS1\" ] && return\n","\n","# don't put duplicate lines in the history. See bash(1) for more options\n","# ... or force ignoredups and ignorespace\n","HISTCONTROL=ignoredups:ignorespace\n","\n","# append to the history file, don't overwrite it\n","shopt -s histappend\n","\n","# for setting history length see HISTSIZE and HISTFILESIZE in bash(1)\n","HISTSIZE=10000\n","HISTFILESIZE=20000\n","\n","# check the window size after each command and, if necessary,\n","# update the values of LINES and COLUMNS.\n","shopt -s checkwinsize\n","\n","# make less more friendly for non-text input files, see lesspipe(1)\n","[ -x /usr/bin/lesspipe ] && eval \"$(SHELL=/bin/sh lesspipe)\"\n","\n","PS1='\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ '\n","\n","# enable color support of ls and also add handy aliases\n","if [ -x /usr/bin/dircolors ]; then\n","    test -r ~/.dircolors && eval \"$(dircolors -b ~/.dircolors)\" || eval \"$(dircolors -b)\"\n","    alias ls='ls --color=auto'\n","    #alias dir='dir --color=auto'\n","    #alias vdir='vdir --color=auto'\n","\n","    alias grep='grep --color=auto'\n","    alias fgrep='fgrep --color=auto'\n","    alias egrep='egrep --color=auto'\n","fi\n","\n","# some more ls aliases\n","alias ll='ls -lF'\n","alias la='ls -A'\n","alias l='ls -CF'\n","\n","# path setup\n","export PATH=\"./:/usr/local/cuda/bin:$PATH\""],"metadata":{"cellView":"form","id":"O8ICSyy8_GEq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!source /root/.bashrc"],"metadata":{"id":"QxIfKO3Ghf7g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Clone GPUcomputing site on github..."],"metadata":{"id":"IYG8Cv4bTzyI"}},{"cell_type":"code","source":["!git clone https://github.com/giulianogrossi/GPUcomputing.git"],"metadata":{"id":"E7jZmHjCT0vu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define some paths..."],"metadata":{"id":"ZarLje6wR_Og"}},{"cell_type":"code","source":["# path setup\n","!mkdir -p /content/GPUcomputing/lab7\n","%cd /content/GPUcomputing/lab7\n","!mkdir -p mems\n","!mkdir -p unified\n","!mkdir -p transpose\n","!mkdir -p struct"],"metadata":{"id":"tC-AaOJlkLOO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ‚ñ∂Ô∏è VS Code on Colab"],"metadata":{"id":"zs_a5Vuimily"}},{"cell_type":"code","source":["#@title Colab-ssh tunnel\n","#@markdown Execute this cell to open the ssh tunnel. Check [colab-ssh documentation](https://github.com/WassimBenzarti/colab-ssh) for more details.\n","\n","# Install colab_ssh on google colab\n","!pip install colab_ssh --upgrade\n","\n","from colab_ssh import launch_ssh_cloudflared, init_git_cloudflared\n","ssh_tunnel_password = \"gpu\" #@param {type: \"string\"}\n","launch_ssh_cloudflared(password=ssh_tunnel_password)\n","\n","# Optional: if you want to clone a Github or Gitlab repository\n","repository_url=\"https://github.com/giulianogrossi/GPUcomputing\" #@param {type: \"string\"}\n","init_git_cloudflared(repository_url)"],"metadata":{"id":"BCf9JxqphHAp","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iUYP4kCJhEIx"},"source":["# ‚ñ∂Ô∏è DeviceQuery"]},{"cell_type":"code","metadata":{"id":"kW9b_Yuxi7id"},"source":["# DeviceQuery dell'attuale device (su Colab!)\n","!nvcc /content/GPUcomputing/utils/deviceQuery.cu -o deviceQuery\n","!./deviceQuery"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ‚úÖ Static, pinned, zero-copy  memory"],"metadata":{"id":"DvmApCF76YD0"}},{"cell_type":"markdown","source":["**Pinned memory**\n","\n","An example of using CUDA's memory copy API to transfer data to and from the device. In this case, `cudaMalloc` is used to allocate memory on the GPU and `cudaMemcpy` is used to transfer the contents of host memory to an array allocated using `cudaMalloc`. Host memory is allocated using `cudaMallocHost` to create a page-locked host array."],"metadata":{"id":"4ApFq6kF6Olh"}},{"cell_type":"code","source":["%%writefile mems/pinMemTransfer.cu\n","\n","#include \"../../utils/common.h\"\n","#include <cuda_runtime.h>\n","#include <stdio.h>\n","\n","\n","int main(int argc, char **argv) {\n","  // set up device\n","  int dev = 0;\n","  CHECK(cudaSetDevice(dev));\n","\n","  // memory size\n","  unsigned int isize = 1 << 24;\n","  unsigned int nbytes = isize * sizeof(float);\n","\n","  // get device information\n","  cudaDeviceProp deviceProp;\n","  CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n","\n","  if (!deviceProp.canMapHostMemory) {\n","      printf(\"Device %d does not support mapping CPU host memory!\\n\", dev);\n","      CHECK(cudaDeviceReset());\n","      exit(EXIT_SUCCESS);\n","  }\n","\n","  printf(\"%s starting at \", argv[0]);\n","  printf(\"device %d: %s memory size %d nbyte %5.2fMB canMap %d\\n\", dev,\n","          deviceProp.name, isize, nbytes / (1024.0f * 1024.0f),\n","          deviceProp.canMapHostMemory);\n","\n","  float *h_a;\n","  // allocate the host memory   \n","  //h_a = (float *)malloc(nbytes);\n","\n","  // allocate pinned host memory\n","  CHECK(cudaMallocHost ((float **)&h_a, nbytes));\n","\n","  // allocate device memory\n","  float *d_a;\n","  CHECK(cudaMalloc((float **)&d_a, nbytes));\n","\n","  for (int i = 0; i < isize; i++) \n","    h_a[i] = 100.10f;\n","\n","  // transfer data from the host to the device\n","  CHECK(cudaMemcpy(d_a, h_a, nbytes, cudaMemcpyHostToDevice));\n","\n","  // transfer data from the device to the host\n","  CHECK(cudaMemcpy(h_a, d_a, nbytes, cudaMemcpyDeviceToHost));\n","\n","  // free memory\n","  CHECK(cudaFree(d_a));\n","  CHECK(cudaFreeHost(h_a));\n","\n","  // reset device\n","  CHECK(cudaDeviceReset());\n","  return EXIT_SUCCESS;\n","}\n"],"metadata":{"id":"XObjkBa16Wx2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvcc -O3 -arch=sm_37 mems/pinMemTransfer.cu -o pinMemTransfer\n","!nvprof ./pinMemTransfer\n"],"metadata":{"id":"arU-6PkD7V4A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vXUIQkZLCTcG"},"source":["# ‚úÖ Unified memory\n"]},{"cell_type":"code","source":["%%writefile unified/sumMatrix.cu\n","\n","#include <stdio.h>\n","#include \"../../utils/common.h\"\n","\n","void initialData(float *ip, const int size) {\n","  int i;\n","\n","  for (i = 0; i < size; i++)\n","    ip[i] = (float)( rand() & 0xFF ) / 10.0f;\n","  return;\n","}\n","\n","void sumMatrixOnHost(float *A, float *B, float *C, const int nx, const int ny) {\n","  float *ia = A;\n","  float *ib = B;\n","  float *ic = C;\n","\n","  for (int iy = 0; iy < ny; iy++) {\n","    for (int ix = 0; ix < nx; ix++)\n","      ic[ix] = ia[ix] + ib[ix];\n","\n","    ia += nx;\n","    ib += nx;\n","    ic += nx;\n","  }\n","  return;\n","}\n","\n","void checkResult(float *hostRef, float *gpuRef, const int N) {\n","  double epsilon = 1.0E-8;\n","  bool match = 1;\n","\n","  for (int i = 0; i < N; i++) {\n","    if (abs(hostRef[i] - gpuRef[i]) > epsilon) {\n","      match = 0;\n","      printf(\"host %f gpu %f\\n\", hostRef[i], gpuRef[i]);\n","      break;\n","    }\n","  }\n","\n","  if (!match)\n","    printf(\"Arrays do not match.\\n\\n\");\n","}\n","\n","// grid 2D block 2D\n","__global__ void sumMatrixGPU(float *MatA, float *MatB, float *MatC, int nx, int ny) {\n","  unsigned int ix = threadIdx.x + blockIdx.x * blockDim.x;\n","  unsigned int iy = threadIdx.y + blockIdx.y * blockDim.y;\n","  unsigned int idx = iy * nx + ix;\n","\n","  if (ix < nx && iy < ny)\n","    MatC[idx] = MatA[idx] + MatB[idx];\n","}\n","\n","int main(int argc, char **argv) {\n","    printf(\"%s Starting \", argv[0]);\n","\n","    // set up device\n","    int dev = 0;\n","    cudaDeviceProp deviceProp;\n","    CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n","    printf(\"using Device %d: %s\\n\", dev, deviceProp.name);\n","    CHECK(cudaSetDevice(dev));\n","\n","    // set up data size of matrix\n","    int nx, ny;\n","    int ishift = 12;\n","\n","    if  (argc > 1) ishift = atoi(argv[1]);\n","\n","    nx = ny = 1 << ishift;\n","\n","    int nxy = nx * ny;\n","    int nBytes = nxy * sizeof(float);\n","    printf(\"Matrix size: nx %d ny %d\\n\", nx, ny);\n","\n","    // malloc host memory\n","    float *h_A, *h_B, *hostRef, *gpuRef;\n","    h_A = (float *)malloc(nBytes);\n","    h_B = (float *)malloc(nBytes);\n","    hostRef = (float *)malloc(nBytes);\n","    gpuRef = (float *)malloc(nBytes);\n","\n","    // initialize data at host side\n","    double iStart = seconds();\n","    initialData(h_A, nxy);\n","    initialData(h_B, nxy);\n","    double iElaps = seconds() - iStart;\n","\n","    printf(\"initialization: \\t %f sec\\n\", iElaps);\n","\n","    memset(hostRef, 0, nBytes);\n","    memset(gpuRef, 0, nBytes);\n","\n","    // add matrix at host side for result checks\n","    iStart = seconds();\n","    sumMatrixOnHost(h_A, h_B, hostRef, nx, ny);\n","    iElaps = seconds() - iStart;\n","    printf(\"sumMatrix on host:\\t %f sec\\n\", iElaps);\n","\n","    // malloc device global memory\n","    float *d_MatA, *d_MatB, *d_MatC;\n","    CHECK(cudaMalloc((void **)&d_MatA, nBytes));\n","    CHECK(cudaMalloc((void **)&d_MatB, nBytes));\n","    CHECK(cudaMalloc((void **)&d_MatC, nBytes));\n","\n","    // invoke kernel at host side\n","    int dimx = 32;\n","    int dimy = 32;\n","    dim3 block(dimx, dimy);\n","    dim3 grid((nx + block.x - 1) / block.x, (ny + block.y - 1) / block.y);\n","\n","    // init device data to 0.0f, then warm-up kernel to obtain accurate timing\n","    // result\n","    CHECK(cudaMemset(d_MatA, 0.0f, nBytes));\n","    CHECK(cudaMemset(d_MatB, 0.0f, nBytes));\n","    sumMatrixGPU<<<grid, block>>>(d_MatA, d_MatB, d_MatC, 1, 1);\n","\n","\n","    // transfer data from host to device\n","    CHECK(cudaMemcpy(d_MatA, h_A, nBytes, cudaMemcpyHostToDevice));\n","    CHECK(cudaMemcpy(d_MatB, h_B, nBytes, cudaMemcpyHostToDevice));\n","\n","    iStart =  seconds();\n","    sumMatrixGPU<<<grid, block>>>(d_MatA, d_MatB, d_MatC, nx, ny);\n","\n","    CHECK(cudaDeviceSynchronize());\n","    iElaps = seconds() - iStart;\n","    printf(\"sumMatrix on gpu :\\t %f sec <<<(%d,%d), (%d,%d)>>> \\n\", iElaps,\n","            grid.x, grid.y, block.x, block.y);\n","\n","    CHECK(cudaMemcpy(gpuRef, d_MatC, nBytes, cudaMemcpyDeviceToHost));\n","\n","    // check kernel error\n","    CHECK(cudaGetLastError());\n","\n","    // check device results\n","    checkResult(hostRef, gpuRef, nxy);\n","\n","    // free device global memory\n","    CHECK(cudaFree(d_MatA));\n","    CHECK(cudaFree(d_MatB));\n","    CHECK(cudaFree(d_MatC));\n","\n","    // free host memory\n","    free(h_A);\n","    free(h_B);\n","    free(hostRef);\n","    free(gpuRef);\n","\n","    // reset device\n","    CHECK(cudaDeviceReset());\n","\n","    return (0);\n","}\n"],"metadata":{"id":"Stv28L2PU-Kg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compilazione ed esecuzione\n","\n","!nvcc -arch=sm_37 unified/sumMatrix.cu  -o sumMatrix\n","!./sumMatrix 14"],"metadata":{"id":"LezrfqF5VPEw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# profilazione\n","\n","!nvprof ./sumMatrix 14"],"metadata":{"id":"iRqExSDZViHG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üî¥ TODO"],"metadata":{"id":"R1V3b-a3RjL3"}},{"cell_type":"code","metadata":{"id":"-Y52R0d3CA50"},"source":["%%writefile unified/sumMatrixUni.cu\n","\n","#include \"../../utils/common.h\"\n","\n","void initialData(float *ip, const int size) {\n","  int i;\n","\n","  for (i = 0; i < size; i++)\n","    ip[i] = (float)( rand() & 0xFF ) / 10.0f;\n","  return;\n","}\n","\n","void sumMatrixOnHost(float *A, float *B, float *C, const int nx, const int ny) {\n","  float *ia = A;\n","  float *ib = B;\n","  float *ic = C;\n","\n","  for (int iy = 0; iy < ny; iy++) {\n","    for (int ix = 0; ix < nx; ix++)\n","      ic[ix] = ia[ix] + ib[ix];\n","\n","    ia += nx;\n","    ib += nx;\n","    ic += nx;\n","  }\n","  return;\n","}\n","\n","void checkResult(float *hostRef, float *gpuRef, const int N) {\n","  double epsilon = 1.0E-8;\n","  bool match = 1;\n","\n","  for (int i = 0; i < N; i++) {\n","    if (abs(hostRef[i] - gpuRef[i]) > epsilon) {\n","      match = 0;\n","      printf(\"host %f gpu %f\\n\", hostRef[i], gpuRef[i]);\n","      break;\n","    }\n","  }\n","\n","  if (!match)\n","    printf(\"Arrays do not match.\\n\\n\");\n","}\n","\n","// grid 2D block 2D\n","__global__ void sumMatrixGPU(float *MatA, float *MatB, float *MatC, int nx, int ny) {\n","  unsigned int ix = threadIdx.x + blockIdx.x * blockDim.x;\n","  unsigned int iy = threadIdx.y + blockIdx.y * blockDim.y;\n","  unsigned int idx = iy * nx + ix;\n","\n","  if (ix < nx && iy < ny)\n","    MatC[idx] = MatA[idx] + MatB[idx];\n","}\n","\n","int main(int argc, char **argv) {\n","  printf(\"%s Starting \", argv[0]);\n","\n","  // set up device\n","  int dev = 0;\n","  cudaDeviceProp deviceProp;\n","  CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n","  printf(\"using Device %d: %s\\n\", dev, deviceProp.name);\n","  CHECK(cudaSetDevice(dev));\n","\n","  // set up data size of matrix\n","  int nx, ny;\n","  int ishift = 14;\n","\n","  if  (argc > 1) ishift = atoi(argv[1]);\n","\n","  nx = ny = 1 << ishift;\n","\n","  int nxy = nx * ny;\n","  int nBytes = nxy * sizeof(float);\n","  printf(\"Matrix size: nx %d ny %d\\n\", nx, ny);\n","\n","  // malloc host memory\n","  float *A, *B, *hostRef, *gpuRef;\n","  CHECK(cudaMallocManaged((void **)&A, nBytes));\n","  CHECK(cudaMallocManaged((void **)&B, nBytes));\n","  CHECK(cudaMallocManaged((void **)&gpuRef,  nBytes);  );\n","  CHECK(cudaMallocManaged((void **)&hostRef, nBytes););\n","\n","  // initialize data at host side\n","  double iStart = seconds();\n","  initialData(A, nxy);\n","  initialData(B, nxy);\n","  double iElaps = seconds() - iStart;\n","  printf(\"initialization: \\t %f sec\\n\", iElaps);\n","\n","  memset(hostRef, 0, nBytes);\n","  memset(gpuRef, 0, nBytes);\n","\n","  // add matrix at host side for result checks\n","  iStart = seconds();\n","  sumMatrixOnHost(A, B, hostRef, nx, ny);\n","  iElaps = seconds() - iStart;\n","  printf(\"sumMatrix on host:\\t %f sec\\n\", iElaps);\n","\n","  // invoke kernel at host side\n","  int dimx = 32;\n","  int dimy = 32;\n","  dim3 block(dimx, dimy);\n","  dim3 grid((nx + block.x - 1) / block.x, (ny + block.y - 1) / block.y);\n","\n","  // warm-up kernel, with unified memory all pages will migrate from host to device\n","  sumMatrixGPU<<<grid, block>>>(A, B, gpuRef, 1, 1);\n","\n","  // after warm-up, time with unified memory\n","  iStart = seconds();\n","\n","  sumMatrixGPU<<<grid, block>>>(A, B, gpuRef, nx, ny);\n","\n","  CHECK(cudaDeviceSynchronize());\n","  iElaps = seconds() - iStart;\n","  printf(\"sumMatrix on gpu :\\t %f sec <<<(%d,%d), (%d,%d)>>> \\n\", iElaps,\n","          grid.x, grid.y, block.x, block.y);\n","\n","  // check kernel error\n","  CHECK(cudaGetLastError());\n","\n","  // check device results\n","  checkResult(hostRef, gpuRef, nxy);\n","\n","  // free device global memory\n","  CHECK(cudaFree(A));\n","  CHECK(cudaFree(B));\n","  CHECK(cudaFree(hostRef));\n","  CHECK(cudaFree(gpuRef));\n","\n","  // reset device\n","  CHECK(cudaDeviceReset());\n","\n","  return (0);\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0PSc9B9PDTWt"},"source":["# Compilazione ed esecuzione\n","\n","!nvcc -arch=sm_37 unified/sumMatrixUni.cu  -o sumMatrixUni\n","!./sumMatrixUni 14"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mfR471rze2p-"},"source":["# profilazione\n","\n","!nvprof ./sumMatrixUni"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mykWw4i6-PTq"},"source":["%%writefile unified_mem/sumMatrixGPUManual.cu\n","\n","#include \"../../utils/common.h\"\n","\n","void initialData(float *ip, const int size) {\n","  int i;\n","\n","  for (i = 0; i < size; i++)\n","    ip[i] = (float)( rand() & 0xFF ) / 10.0f;\n","  return;\n","}\n","\n","void sumMatrixOnHost(float *A, float *B, float *C, const int nx, const int ny) {\n","  float *ia = A;\n","  float *ib = B;\n","  float *ic = C;\n","\n","  for (int iy = 0; iy < ny; iy++) {\n","    for (int ix = 0; ix < nx; ix++)\n","      ic[ix] = ia[ix] + ib[ix];\n","\n","    ia += nx;\n","    ib += nx;\n","    ic += nx;\n","  }\n","  return;\n","}\n","\n","void checkResult(float *hostRef, float *gpuRef, const int N) {\n","  double epsilon = 1.0E-8;\n","  bool match = 1;\n","\n","  for (int i = 0; i < N; i++) {\n","    if (abs(hostRef[i] - gpuRef[i]) > epsilon) {\n","      match = 0;\n","      printf(\"host %f gpu %f\\n\", hostRef[i], gpuRef[i]);\n","      break;\n","    }\n","  }\n","\n","  if (!match)\n","    printf(\"Arrays do not match.\\n\\n\");\n","}\n","\n","// grid 2D block 2D\n","__global__ void sumMatrixGPU(float *MatA, float *MatB, float *MatC, int nx, int ny) {\n","  unsigned int ix = threadIdx.x + blockIdx.x * blockDim.x;\n","  unsigned int iy = threadIdx.y + blockIdx.y * blockDim.y;\n","  unsigned int idx = iy * nx + ix;\n","\n","  if (ix < nx && iy < ny)\n","    MatC[idx] = MatA[idx] + MatB[idx];\n","}\n","\n","int main(int argc, char **argv) {\n","    printf(\"%s Starting \", argv[0]);\n","\n","    // set up device\n","    int dev = 0;\n","    cudaDeviceProp deviceProp;\n","    CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n","    printf(\"using Device %d: %s\\n\", dev, deviceProp.name);\n","    CHECK(cudaSetDevice(dev));\n","\n","    // set up data size of matrix\n","    int nx, ny;\n","    int ishift = 12;\n","\n","    if  (argc > 1) ishift = atoi(argv[1]);\n","\n","    nx = ny = 1 << ishift;\n","\n","    int nxy = nx * ny;\n","    int nBytes = nxy * sizeof(float);\n","    printf(\"Matrix size: nx %d ny %d\\n\", nx, ny);\n","\n","    // malloc host memory\n","    float *h_A, *h_B, *hostRef, *gpuRef;\n","    h_A = (float *)malloc(nBytes);\n","    h_B = (float *)malloc(nBytes);\n","    hostRef = (float *)malloc(nBytes);\n","    gpuRef = (float *)malloc(nBytes);\n","\n","    // initialize data at host side\n","    double iStart = seconds();\n","    initialData(h_A, nxy);\n","    initialData(h_B, nxy);\n","    double iElaps = seconds() - iStart;\n","\n","    printf(\"initialization: \\t %f sec\\n\", iElaps);\n","\n","    memset(hostRef, 0, nBytes);\n","    memset(gpuRef, 0, nBytes);\n","\n","    // add matrix at host side for result checks\n","    iStart = seconds();\n","    sumMatrixOnHost(h_A, h_B, hostRef, nx, ny);\n","    iElaps = seconds() - iStart;\n","    printf(\"sumMatrix on host:\\t %f sec\\n\", iElaps);\n","\n","    // malloc device global memory\n","    float *d_MatA, *d_MatB, *d_MatC;\n","    CHECK(cudaMalloc((void **)&d_MatA, nBytes));\n","    CHECK(cudaMalloc((void **)&d_MatB, nBytes));\n","    CHECK(cudaMalloc((void **)&d_MatC, nBytes));\n","\n","    // invoke kernel at host side\n","    int dimx = 32;\n","    int dimy = 32;\n","    dim3 block(dimx, dimy);\n","    dim3 grid((nx + block.x - 1) / block.x, (ny + block.y - 1) / block.y);\n","\n","    // init device data to 0.0f, then warm-up kernel to obtain accurate timing\n","    // result\n","    CHECK(cudaMemset(d_MatA, 0.0f, nBytes));\n","    CHECK(cudaMemset(d_MatB, 0.0f, nBytes));\n","    sumMatrixGPU<<<grid, block>>>(d_MatA, d_MatB, d_MatC, 1, 1);\n","\n","\n","    // transfer data from host to device\n","    CHECK(cudaMemcpy(d_MatA, h_A, nBytes, cudaMemcpyHostToDevice));\n","    CHECK(cudaMemcpy(d_MatB, h_B, nBytes, cudaMemcpyHostToDevice));\n","\n","    iStart =  seconds();\n","    sumMatrixGPU<<<grid, block>>>(d_MatA, d_MatB, d_MatC, nx, ny);\n","\n","    CHECK(cudaDeviceSynchronize());\n","    iElaps = seconds() - iStart;\n","    printf(\"sumMatrix on gpu :\\t %f sec <<<(%d,%d), (%d,%d)>>> \\n\", iElaps,\n","            grid.x, grid.y, block.x, block.y);\n","\n","    CHECK(cudaMemcpy(gpuRef, d_MatC, nBytes, cudaMemcpyDeviceToHost));\n","\n","    // check kernel error\n","    CHECK(cudaGetLastError());\n","\n","    // check device results\n","    checkResult(hostRef, gpuRef, nxy);\n","\n","    // free device global memory\n","    CHECK(cudaFree(d_MatA));\n","    CHECK(cudaFree(d_MatB));\n","    CHECK(cudaFree(d_MatC));\n","\n","    // free host memory\n","    free(h_A);\n","    free(h_B);\n","    free(hostRef);\n","    free(gpuRef);\n","\n","    // reset device\n","    CHECK(cudaDeviceReset());\n","\n","    return (0);\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TJcJtD6f_IXK"},"source":["# Compilazione ed esecuzione\n","\n","!nvcc -arch=sm_60 sumMatrixGPUManual.cu  -o sumMatrixGPUManual\n","!sumMatrixGPUManual 14"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f5JMDp3t_kEI"},"source":["# profilazione (senza unified memory - d√† errore)\n","\n","!nvprof --unified-memory-profiling off ./sumMatrixGPUManual"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SOFMQZAkjlLW"},"source":["# ‚úÖ SoA vs AoS structs"]},{"cell_type":"code","metadata":{"id":"v9nRkLgeB10A"},"source":["%%writefile struct/SoA.cu\n","\n","#include <stdint.h>\n","#include \"../../utils/common.h\"\n","\n","#define N 1<<24\n","#define blocksize 1<<7\n","\n","struct SoA {\n","\tuint8_t r[N];\n","\tuint8_t g[N];\n","\tuint8_t b[N];\n","};\n","\n","\n","void initialize(SoA*, int);\n","void checkResult(SoA*, SoA*, int);\n","\n","/*\n"," * Riscala l'immagine al valore massimo [max] fissato\n"," */\n","__global__ void rescaleImg(SoA *img, const int max, const int n) {\n","\tunsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n","\tif (i < n) {\n","\t\tfloat r,g,b;\n","\t\tSoA *tmp = img;\n","\t\tr = max * (float)tmp->r[i]/255.0f;\n","\t\timg->r[i] = (uint8_t)r;\n","\t\tg = max * (float)tmp->g[i]/255.0f;\n","\t\timg->g[i] = (uint8_t)g;\n","\t\tb = max * (float)tmp->b[i]/255.0f;\n","\t\timg->b[i] = (uint8_t)b;\n","\t}\n","}\n","\n","/*\n"," * cancella un piano dell'immagine [plane = 'r' o 'g' o 'b'] fissato\n"," */\n","__global__ void deletePlane(SoA *img, const char plane, const int n) {\n","\tunsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n","\tif (i < n) {\n","\t\tswitch (plane) {\n","\t\tcase 'r':\n","\t\t\timg->r[i] = 0;\n","\t\t\tbreak;\n","\t\tcase 'g':\n","\t\t\timg->g[i] = 0;\n","\t\t\tbreak;\n","\t\tcase 'b':\n","\t\t\timg->b[i] = 0;\n","\t\t\tbreak;\n","\t\t}\n","\t}\n","}\n","\n","/*\n"," * setup device\n"," */\n","__global__ void warmup(SoA *img, const int max, const int n) {\n","\tunsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n","\tif (i < n) {\n","\t\tfloat r,g,b;\n","\t\tSoA *tmp = img;\n","\t\tr = max * (float)tmp->r[i]/255.0f;\n","\t\timg->r[i] = (uint8_t)r;\n","\t\tg = max * (float)tmp->g[i]/255.0f;\n","\t\timg->g[i] = (uint8_t)g;\n","\t\tb = max * (float)tmp->b[i]/255.0f;\n","\t\timg->b[i] = (uint8_t)b;\n","\t}\n","}\n","\n","/*\n"," * Legge da stdin quale kernel eseguire: 0 per rescaleImg, 1 per deletePlane\n"," */\n","int main(int argc, char **argv) {\n","\t// set up device\n","\tint dev = 0;\n","\tcudaDeviceProp deviceProp;\n","\tCHECK(cudaGetDeviceProperties(&deviceProp, dev));\n","\tprintf(\"%s test SoA at \", argv[0]);\n","\tprintf(\"device %d: %s \\n\", dev, deviceProp.name);\n","\tCHECK(cudaSetDevice(dev));\n","\n","\t// scelta del kernel da eseguire\n","\tint kernel = 0;\n","\tif (argc > 1) kernel = atoi(argv[1]);\n","\n","\t// allocate host memory\n","\tsize_t nBytes = sizeof(SoA);\n","\tSoA *img = (SoA *)malloc(nBytes);\n","\tSoA *new_img = (SoA *)malloc(nBytes);\n","\n","\t// initialize host array\n","\tinitialize(img, N);\n","\n","\t// allocate device memory\n","\tint n_elem = N;\n","\tSoA *d_img;\n","\tCHECK(cudaMalloc((void**)&d_img, nBytes));\n","\n","\t// copy data from host to device\n","\tCHECK(cudaMemcpy(d_img, img, nBytes, cudaMemcpyHostToDevice));\n","\n","\t// definizione max\n","\tint max = 128;\n","\tif (argc > 2) max = atoi(argv[2]);\n","\n","\t// configurazione per esecuzione\n","\tdim3 block (blocksize, 1);\n","\tdim3 grid  ((n_elem + block.x - 1) / block.x, 1);\n","\n","\t// kernel 1: warmup\n","\tdouble iStart = seconds();\n","\twarmup<<<1, 32>>>(d_img, max, 32);\n","\tCHECK(cudaDeviceSynchronize());\n","\tdouble iElaps = seconds() - iStart;\n","\tprintf(\"warmup<<< 1, 32 >>> elapsed %f sec\\n\",iElaps);\n","\tCHECK(cudaGetLastError());\n","\n","\t// kernel 2 rescaleImg o deletePlane\n","\tiStart = seconds();\n","\tif (kernel == 0)\n","\t\trescaleImg<<<grid, block>>>(d_img, max, n_elem);\n","\telse\n","\t\tdeletePlane<<<grid, block>>>(d_img, 'r', n_elem);\n","\tCHECK(cudaDeviceSynchronize());\n","\tiElaps = seconds() - iStart;\n","\tprintf(\"rescaleImg <<< %3d, %3d >>> elapsed %f sec\\n\", grid.x, block.x, iElaps);\n","\tCHECK(cudaMemcpy(new_img, d_img, nBytes, cudaMemcpyDeviceToHost));\n","\tCHECK(cudaGetLastError());\n","\n","\t//checkResult(img, new_img, n_elem);\n","\n","\t// free memories both host and device\n","\tCHECK(cudaFree(d_img));\n","\tfree(img);\n","\tfree(new_img);\n","\n","\t// reset device\n","\tCHECK(cudaDeviceReset());\n","\treturn EXIT_SUCCESS;\n","}\n","\n","void initialize(SoA *img,  int size) {\n","\tfor (int i = 0; i < size; i++) {\n","\t\timg->r[i] = rand() % 256;\n","\t\timg->g[i] = rand() % 256;\n","\t\timg->b[i] = rand() % 256;\n","\t}\n","\treturn;\n","}\n","\n","void checkResult(SoA *img, SoA *new_img, int n_elem) {\n","\tfor (int i = 0; i < n_elem; i+=1000)\n","\t\tprintf(\"img[%d] = (%d,%d,%d) -- new_img[%d] = (%d,%d,%d)\\n\",\n","\t\t\t\ti,img->r[i],img->g[i],img->b[i],i,new_img->r[i],new_img->g[i],new_img->b[i]);\n","\treturn;\n","}\n","\n","\n","void transposeHost(float *out, float *in, const int nx, const int ny) {\n","\tfor (int iy = 0; iy < ny; ++iy) {\n","\t\tfor (int ix = 0; ix < nx; ++ix) {\n","\t\t\tout[ix * ny + iy] = in[iy * nx + ix];\n","\t\t}\n","\t}\n","}\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wLxZjCx8bT3s"},"source":["# Compilazione ed esecuzione\n","!nvcc -arch=sm_37  struct/SoA.cu -o SoA\n","!./SoA"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üî¥ TODO"],"metadata":{"id":"cqQnULvlE2fu"}},{"cell_type":"code","metadata":{"id":"bwcTDn6ehJr_"},"source":["%%writefile struct/AoS.cu\n","\n","#include <stdint.h>\n","#include \"../../utils/common.h\"\n","\n","#define N 1<<24\n","#define blocksize 128\n","\n","struct AoS {\n","\tuint8_t r;\n","\tuint8_t g;\n","\tuint8_t b;\n","};\n","\n","void initialize(AoS *, int);\n","void checkResult(AoS *, AoS *, int);\n","\n","/*\n"," * Riscala l'immagine al valore massimo [max] fissato\n"," */\n","__global__ void rescaleImg(AoS *img, const int max, const int n) {\n","\tunsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n","\tif (i < n) {\n","\t\tfloat r,g,b;\n","\t\tAoS tmp = img[i];\n","\t\tr = max * (float)tmp.r/255.0f;\n","\t\ttmp.r = (uint8_t)r;\n","\t\tg = max * (float)tmp.g/255.0f;\n","\t\ttmp.g = (uint8_t)g;\n","\t\tb = max * (float)tmp.b/255.0f;\n","\t\ttmp.b = (uint8_t)b;\n","\t\timg[i] = tmp;\n","\t}\n","}\n","\n","/*\n"," * cancella un piano dell'immagine [plane = 'r' o 'g' o 'b'] fissato\n"," */\n","__global__ void deletePlane(AoS *img, const char plane, const int n) {\n","\tunsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n","\tif (i < n) {\n","\t\tswitch (plane) {\n","\t\tcase 'r':\n","\t\t\timg[i].r = 0;\n","\t\t\tbreak;\n","\t\tcase 'g':\n","\t\t\timg[i].g = 0;\n","\t\t\tbreak;\n","\t\tcase 'b':\n","\t\t\timg[i].b = 0;\n","\t\t\tbreak;\n","\t\t}\n","\t}\n","}\n","\n","/*\n"," * setup device\n"," */\n","__global__ void warmup(AoS *img, const int max, const int n) {\n","\tunsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n","\tif (i < n) {\n","\t\tfloat r,g,b;\n","\t\tAoS tmp = img[i];\n","\t\tr = max * (float)tmp.r/255.0f;\n","\t\ttmp.r = (uint8_t)r;\n","\t\tg = max * (float)tmp.g/255.0f;\n","\t\ttmp.g = (uint8_t)g;\n","\t\tb = max * (float)tmp.b/255.0f;\n","\t\ttmp.b = (uint8_t)b;\n","\t\timg[i] = tmp;\n","\t}\n","}\n","\n","/*\n"," * Legge da stdin quale kernel eseguire: 0 per rescaleImg, 1 per deletePlane\n"," */\n","int main(int argc, char **argv) {\n","\t// set up device\n","\tint dev = 0;\n","\tcudaDeviceProp deviceProp;\n","\tCHECK(cudaGetDeviceProperties(&deviceProp, dev));\n","\tprintf(\"%s test AoS at \", argv[0]);\n","\tprintf(\"device %d: %s \\n\", dev, deviceProp.name);\n","\tCHECK(cudaSetDevice(dev));\n","\n","\t// scelta del kernel da eseguire\n","\tint kernel = 0;\n","\tif (argc > 1) kernel = atoi(argv[1]);\n","\n","\t// allocate host memory\n","\tint n_elem = N;\n","\tsize_t nBytes = n_elem * sizeof(AoS);\n","\tAoS *img = (AoS *)malloc(nBytes);\n","\tAoS *new_img = (AoS *)malloc(nBytes);\n","\n","\t// initialize host array\n","\tinitialize(img, N);\n","\n","\t// allocate device memory\n","\tAoS *d_img;\n","\tCHECK(cudaMalloc((void**)&d_img, nBytes));\n","\n","\t// copy data from host to device\n","\tCHECK(cudaMemcpy(d_img, img, nBytes, cudaMemcpyHostToDevice));\n","\n","\t// definizione max\n","\tint max = 128;\n","\tif (argc > 2) max = atoi(argv[2]);\n","\n","\t// configurazione per esecuzione\n","\tdim3 block (blocksize, 1);\n","\tdim3 grid  ((n_elem + block.x - 1) / block.x, 1);\n","\n","\t// kernel 1: warmup\n","\tdouble iStart = seconds();\n","\twarmup<<<1, 32>>>(d_img, max, 32);\n","\tCHECK(cudaDeviceSynchronize());\n","\tdouble iElaps = seconds() - iStart;\n","\tprintf(\"warmup<<< 1, 32 >>> elapsed %f sec\\n\",iElaps);\n","\tCHECK(cudaGetLastError());\n","\n","\t// kernel 2 rescaleImg o deletePlane\n","\tiStart = seconds();\n","\tif (kernel == 0) {\n","\t\trescaleImg<<<grid, block>>>(d_img, max, n_elem);\n","\t\tprintf(\"rescaleImg <<< %3d, %3d >>> elapsed %f sec\\n\", grid.x, block.x, iElaps);\n","\t}\n","\telse {\n","\t\tdeletePlane<<<grid, block>>>(d_img, 'r', n_elem);\n","\t\tprintf(\"deletePlane <<< %3d, %3d >>> elapsed %f sec\\n\", grid.x, block.x, iElaps);\n","\t}\n","\tCHECK(cudaDeviceSynchronize());\n","\tiElaps = seconds() - iStart;\n","\tCHECK(cudaMemcpy(new_img, d_img, nBytes, cudaMemcpyDeviceToHost));\n","\tCHECK(cudaGetLastError());\n","\n","\t//checkResult(img, new_img, n_elem);\n","\n","\t// free memories both host and device\n","\tCHECK(cudaFree(d_img));\n","\tfree(img);\n","\tfree(new_img);\n","\n","\t// reset device\n","\tCHECK(cudaDeviceReset());\n","\treturn EXIT_SUCCESS;\n","}\n","\n","void initialize(AoS *img,  int size) {\n","\tfor (int i = 0; i < size; i++) {\n","\t\timg[i].r = rand() % 256;\n","\t\timg[i].g = rand() % 256;\n","\t\timg[i].b = rand() % 256;\n","\t}\n","\treturn;\n","}\n","\n","void checkResult(AoS *img, AoS *new_img, int n_elem) {\n","\tfor (int i = 0; i < n_elem; i+=1000)\n","\t\tprintf(\"img[%d] = (%d,%d,%d) -- new_img[%d] = (%d,%d,%d)\\n\",\n","\t\t\t\ti,img[i].r,img[i].g,img[i].b,i,new_img[i].r,new_img[i].g,new_img[i].b);\n","\treturn;\n","}\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yiun00TE2wcE"},"source":["# Compilazione ed esecuzione\n","!nvcc -arch=sm_37  struct/AoS.cu -o AoS\n","!./AoS"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SbQlRthtJTQE"},"source":["# ‚úÖ Transpose"]},{"cell_type":"code","metadata":{"id":"QmsEg15mJRLU"},"source":["%%writefile transpose/transpose.cu\n","\n","#include <stdio.h>\n","#include \"../../utils/common.h\"\n","\n","/*\n"," * Various memory access pattern optimizations applied to a matrix transpose kernel.\n"," */\n","\n","#define BDIMX 16\n","#define BDIMY 16\n","\n","void initialData(float *in,  const int size) {\n","  for (int i = 0; i < size; i++)\n","    in[i] = (float)( rand() & 0xFF ) / 10.0f; //100.0f;\n","  return;\n","}\n","\n","void printData(float *in,  const int size) {\n","  for (int i = 0; i < size; i++)\n","    printf(\"%dth element: %f\\n\", i, in[i]);\n","  return;\n","}\n","\n","void checkResult(float *hostRef, float *gpuRef, const int size, int showme) {\n","    double epsilon = 1.0E-8;\n","    bool match = 1;\n","\n","    for (int i = 0; i < size; i++) {\n","        if (abs(hostRef[i] - gpuRef[i]) > epsilon) {\n","            match = 0;\n","            printf(\"different on %dth element: host %f gpu %f\\n\", i, hostRef[i],\n","                    gpuRef[i]);\n","            break;\n","        }\n","\n","        if (showme && i > size / 2 && i < size / 2 + 5)\n","          printf(\"%dth element: host %f gpu %f\\n\",i,hostRef[i],gpuRef[i]);\n","    }\n","\n","    if (!match)  printf(\"Arrays do not match.\\n\\n\");\n","}\n","\n","void transposeHost(float *out, float *in, const int nx, const int ny) {\n","  for( int iy = 0; iy < ny; ++iy)\n","    for( int ix = 0; ix < nx; ++ix)\n","      out[ix * ny + iy] = in[iy * nx + ix];\n","}\n","\n","__global__ void warmup(float *out, float *in, const int nx, const int ny)\n","{\n","    unsigned int ix = blockDim.x * blockIdx.x + threadIdx.x;\n","    unsigned int iy = blockDim.y * blockIdx.y + threadIdx.y;\n","\n","    if (ix < nx && iy < ny)\n","    {\n","        out[iy * nx + ix] = in[iy * nx + ix];\n","    }\n","}\n","\n","// case 0 copy kernel: access data in rows\n","__global__ void copyRow(float *out, float *in, const int nx, const int ny)\n","{\n","    unsigned int ix = blockDim.x * blockIdx.x + threadIdx.x;\n","    unsigned int iy = blockDim.y * blockIdx.y + threadIdx.y;\n","\n","    if (ix < nx && iy < ny)\n","    {\n","        out[iy * nx + ix] = in[iy * nx + ix];\n","    }\n","}\n","\n","// case 1 copy kernel: access data in columns\n","__global__ void copyCol(float *out, float *in, const int nx, const int ny)\n","{\n","    unsigned int ix = blockDim.x * blockIdx.x + threadIdx.x;\n","    unsigned int iy = blockDim.y * blockIdx.y + threadIdx.y;\n","\n","    if (ix < nx && iy < ny)\n","    {\n","        out[ix * ny + iy] = in[ix * ny + iy];\n","    }\n","}\n","\n","// case 2 transpose kernel: read in rows and write in columns\n","__global__ void transposeNaiveRow(float *out, float *in, const int nx, const int ny) {\n","    unsigned int ix = blockDim.x * blockIdx.x + threadIdx.x;\n","    unsigned int iy = blockDim.y * blockIdx.y + threadIdx.y;\n","\n","    if (ix < nx && iy < ny)\n","        out[ix * ny + iy] = in[iy * nx + ix];\n","}\n","\n","// case 3 transpose kernel: read in columns and write in rows\n","__global__ void transposeNaiveCol(float *out, float *in, const int nx,\n","                                  const int ny)\n","{\n","    unsigned int ix = blockDim.x * blockIdx.x + threadIdx.x;\n","    unsigned int iy = blockDim.y * blockIdx.y + threadIdx.y;\n","\n","    if (ix < nx && iy < ny)\n","    {\n","        out[iy * nx + ix] = in[ix * ny + iy];\n","    }\n","}\n","\n","// case 4 transpose kernel: read in rows and write in columns + unroll 4 blocks\n","__global__ void transposeUnroll4Row(float *out, float *in, const int nx,\n","                                    const int ny)\n","{\n","    unsigned int ix = blockDim.x * blockIdx.x * 4 + threadIdx.x;\n","    unsigned int iy = blockDim.y * blockIdx.y + threadIdx.y;\n","\n","    unsigned int ti = iy * nx + ix; // access in rows\n","    unsigned int to = ix * ny + iy; // access in columns\n","\n","    if (ix + 3 * blockDim.x < nx && iy < ny)\n","    {\n","        out[to]                   = in[ti];\n","        out[to + ny * blockDim.x]   = in[ti + blockDim.x];\n","        out[to + ny * 2 * blockDim.x] = in[ti + 2 * blockDim.x];\n","        out[to + ny * 3 * blockDim.x] = in[ti + 3 * blockDim.x];\n","    }\n","}\n","\n","// case 5 transpose kernel: read in columns and write in rows + unroll 4 blocks\n","__global__ void transposeUnroll4Col(float *out, float *in, const int nx,\n","                                    const int ny)\n","{\n","    unsigned int ix = blockDim.x * blockIdx.x * 4 + threadIdx.x;\n","    unsigned int iy = blockDim.y * blockIdx.y + threadIdx.y;\n","\n","    unsigned int ti = iy * nx + ix; // access in rows\n","    unsigned int to = ix * ny + iy; // access in columns\n","\n","    if (ix + 3 * blockDim.x < nx && iy < ny)\n","    {\n","        out[ti]                = in[to];\n","        out[ti +   blockDim.x] = in[to +   blockDim.x * ny];\n","        out[ti + 2 * blockDim.x] = in[to + 2 * blockDim.x * ny];\n","        out[ti + 3 * blockDim.x] = in[to + 3 * blockDim.x * ny];\n","    }\n","}\n","\n","/*\n"," * case 6 :  transpose kernel: read in rows and write in colunms + diagonal\n"," * coordinate transform\n"," */\n","__global__ void transposeDiagonalRow(float *out, float *in, const int nx,\n","                                     const int ny)\n","{\n","    unsigned int blk_y = blockIdx.x;\n","    unsigned int blk_x = (blockIdx.x + blockIdx.y) % gridDim.x;\n","\n","    unsigned int ix = blockDim.x * blk_x + threadIdx.x;\n","    unsigned int iy = blockDim.y * blk_y + threadIdx.y;\n","\n","    if (ix < nx && iy < ny)\n","    {\n","        out[ix * ny + iy] = in[iy * nx + ix];\n","    }\n","}\n","\n","/*\n"," * case 7 :  transpose kernel: read in columns and write in row + diagonal\n"," * coordinate transform.\n"," */\n","__global__ void transposeDiagonalCol(float *out, float *in, const int nx,\n","                                     const int ny)\n","{\n","    unsigned int blk_y = blockIdx.x;\n","    unsigned int blk_x = (blockIdx.x + blockIdx.y) % gridDim.x;\n","\n","    unsigned int ix = blockDim.x * blk_x + threadIdx.x;\n","    unsigned int iy = blockDim.y * blk_y + threadIdx.y;\n","\n","    if (ix < nx && iy < ny)\n","    {\n","        out[iy * nx + ix] = in[ix * ny + iy];\n","    }\n","}\n","\n","// main functions\n","int main(int argc, char **argv)\n","{\n","    // set up device\n","    int dev = 0;\n","    cudaDeviceProp deviceProp;\n","    CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n","    printf(\"%s starting transpose at \", argv[0]);\n","    printf(\"device %d: %s \", dev, deviceProp.name);\n","    CHECK(cudaSetDevice(dev));\n","\n","    // set up array size 2048\n","    int nx = 1 << 12;\n","    int ny = 1 << 12;\n","\n","    // select a kernel and block size\n","    int iKernel = 0;\n","    int blockx = 16;\n","    int blocky = 16;\n","\n","    if (argc > 1) iKernel = atoi(argv[1]);\n","\n","    if (argc > 2) blockx  = atoi(argv[2]);\n","\n","    if (argc > 3) blocky  = atoi(argv[3]);\n","\n","    if (argc > 4) nx  = atoi(argv[4]);\n","\n","    if (argc > 5) ny  = atoi(argv[5]);\n","\n","    printf(\" with matrix nx %d ny %d with kernel %d\\n\", nx, ny, iKernel);\n","    size_t nBytes = nx * ny * sizeof(float);\n","\n","    // execution configuration\n","    dim3 block (blockx, blocky);\n","    dim3 grid  ((nx + block.x - 1) / block.x, (ny + block.y - 1) / block.y);\n","\n","    // allocate host memory\n","    float *h_A = (float *)malloc(nBytes);\n","    float *hostRef = (float *)malloc(nBytes);\n","    float *gpuRef  = (float *)malloc(nBytes);\n","\n","    // initialize host array\n","    initialData(h_A, nx * ny);\n","\n","    // transpose at host side\n","    transposeHost(hostRef, h_A, nx, ny);\n","\n","    // allocate device memory\n","    float *d_A, *d_C;\n","    CHECK(cudaMalloc((float**)&d_A, nBytes));\n","    CHECK(cudaMalloc((float**)&d_C, nBytes));\n","\n","    // copy data from host to device\n","    CHECK(cudaMemcpy(d_A, h_A, nBytes, cudaMemcpyHostToDevice));\n","\n","    // warmup to avoide startup overhead\n","    double iStart = seconds();\n","    warmup<<<grid, block>>>(d_C, d_A, nx, ny);\n","    CHECK(cudaDeviceSynchronize());\n","    double iElaps = seconds() - iStart;\n","    printf(\"warmup         elapsed %f sec\\n\", iElaps);\n","    CHECK(cudaGetLastError());\n","\n","    // kernel pointer and descriptor\n","    void (*kernel)(float *, float *, int, int);\n","    const char *kernelName;\n","\n","    // set up kernel\n","    switch (iKernel)\n","    {\n","    case 0:\n","        kernel = &copyRow;\n","        kernelName = \"CopyRow       \";\n","        break;\n","\n","    case 1:\n","        kernel = &copyCol;\n","        kernelName = \"CopyCol       \";\n","        break;\n","\n","    case 2:\n","        kernel = &transposeNaiveRow;\n","        kernelName = \"NaiveRow      \";\n","        break;\n","\n","    case 3:\n","        kernel = &transposeNaiveCol;\n","        kernelName = \"NaiveCol      \";\n","        break;\n","\n","    case 4:\n","        kernel = &transposeUnroll4Row;\n","        kernelName = \"Unroll4Row    \";\n","        grid.x = (nx + block.x * 4 - 1) / (block.x * 4);\n","        break;\n","\n","    case 5:\n","        kernel = &transposeUnroll4Col;\n","        kernelName = \"Unroll4Col    \";\n","        grid.x = (nx + block.x * 4 - 1) / (block.x * 4);\n","        break;\n","\n","    case 6:\n","        kernel = &transposeDiagonalRow;\n","        kernelName = \"DiagonalRow   \";\n","        break;\n","\n","    case 7:\n","        kernel = &transposeDiagonalCol;\n","        kernelName = \"DiagonalCol   \";\n","        break;\n","    }\n","\n","    // run kernel\n","    iStart = seconds();\n","    kernel<<<grid, block>>>(d_C, d_A, nx, ny);\n","    CHECK(cudaDeviceSynchronize());\n","    iElaps = seconds() - iStart;\n","\n","    // calculate effective_bandwidth\n","    float ibnd = 2 * nx * ny * sizeof(float) / 1e9 / iElaps;\n","    printf(\"%s elapsed %f sec <<< grid (%d,%d) block (%d,%d)>>> effective \"\n","           \"bandwidth %f GB\\n\", kernelName, iElaps, grid.x, grid.y, block.x,\n","           block.y, ibnd);\n","    CHECK(cudaGetLastError());\n","\n","    // check kernel results\n","    if (iKernel > 1)\n","    {\n","        CHECK(cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost));\n","        checkResult(hostRef, gpuRef, nx * ny, 1);\n","    }\n","\n","    // free host and device memory\n","    CHECK(cudaFree(d_A));\n","    CHECK(cudaFree(d_C));\n","    free(h_A);\n","    free(hostRef);\n","    free(gpuRef);\n","\n","    // reset device\n","    CHECK(cudaDeviceReset());\n","    return EXIT_SUCCESS;\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lQrHMFU_KXF5"},"source":["# Compilazione ed esecuzione\n","!nvcc -arch=sm_37  transpose/transpose.cu -o transp\n","!./transp 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LbefiJYuKk0X"},"source":["!./transp 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hTqFftynMeSd"},"source":["!./transp 2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fiH6ehEyMffv"},"source":["!./transp 3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kbVGCa8vMjZZ"},"source":["!./transp 4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2wrYsvLeMpQD"},"source":["!./transp 5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A42KS45eMqNM"},"source":["!./transp 6"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LHumN5kGMtOx"},"source":["!./transp 7"],"execution_count":null,"outputs":[]}]}