{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1-uNLOdLhEHF0EqfcWgPiq0pyQA3dRzjB","timestamp":1681709313158}],"collapsed_sections":["F9PmBZql0ow4","vKG_y-BUiE5G","iUYP4kCJhEIx"],"mount_file_id":"1mk0QXjREp-k_J-pdFfmgoC7-EVmgPyAo","authorship_tag":"ABX9TyMDmQnGhAD/IQ9fF1MMCUkT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["---\n","# **LAB 7 - CUDA Streams**\n","---"],"metadata":{"id":"fZYqN0UwVLC_"}},{"cell_type":"markdown","metadata":{"id":"F9PmBZql0ow4"},"source":["# ‚ñ∂Ô∏è CUDA setup"]},{"cell_type":"code","metadata":{"id":"p9RIwaPbVQHV"},"source":["!nvcc --version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n5YlC1IOTlNb"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## [GPU Compute Capability](https://developer.nvidia.com/cuda-gpus)"],"metadata":{"id":"tMDwal1zXNio"}},{"cell_type":"markdown","metadata":{"id":"iVV0CidyVeqU"},"source":["## NVCC Plugin for Jupyter notebook\n","\n","*Usage*:\n","\n","\n","*   Load Extension `%load_ext nvcc_plugin`\n","*   Mark a cell to be treated as cuda cell\n","`%%cuda --name example.cu --compile false`\n","\n","**NOTE**: The cell must contain either code or comments to be run successfully. It accepts 2 arguments. `-n | --name` - which is the name of either CUDA source or Header. The name parameter must have extension `.cu` or `.h`. Second argument -c | --compile; default value is false. The argument is a flag to specify if the cell will be compiled and run right away or not. It might be usefull if you're playing in the main function\n","\n","*  We are ready to run CUDA C/C++ code right in your Notebook. For this we need explicitly say to the interpreter, that we want to use the extension by adding `%%cu` at the beginning of each cell with CUDA code. \n","\n","\n"]},{"cell_type":"code","metadata":{"id":"X1EeyR1jBnWR"},"source":["!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h7HcKDuAB-CO"},"source":["%load_ext nvcc_plugin"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plugin for cpp sintax highlighting \n","\n","!wget -O cpp_plugin.py https://gist.github.com/akshaykhadse/7acc91dd41f52944c6150754e5530c4b/raw/cpp_plugin.py\n","%load_ext cpp_plugin"],"metadata":{"id":"4zGi-1hA7qdM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Clone GPUcomputing site on github..."],"metadata":{"id":"ojEBtkAGT4SU"}},{"cell_type":"code","source":["!git clone https://github.com/giulianogrossi/GPUcomputing.git"],"metadata":{"id":"2OKGY3EFT41B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ‚ñ∂Ô∏è VS Code on Colab"],"metadata":{"id":"vKG_y-BUiE5G"}},{"cell_type":"code","source":["#@title Colab-ssh tunnel\n","#@markdown Execute this cell to open the ssh tunnel. Check [colab-ssh documentation](https://github.com/WassimBenzarti/colab-ssh) for more details.\n","\n","# Install colab_ssh on google colab\n","!pip install colab_ssh --upgrade --quiet\n","\n","from colab_ssh import launch_ssh_cloudflared, init_git_cloudflared\n","ssh_tunnel_password = \"gpu\" #@param {type: \"string\"}\n","launch_ssh_cloudflared(password=ssh_tunnel_password)\n","\n","# Optional: if you want to clone a Github or Gitlab repository\n","repository_url=\"https://github.com/giulianogrossi/GPUcomputing\" #@param {type: \"string\"}\n","init_git_cloudflared(repository_url)"],"metadata":{"cellView":"form","id":"B2BcP2EciE5G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define some paths..."],"metadata":{"id":"lSqPQ40yiE5H"}},{"cell_type":"code","source":["# path setup\n","!mkdir -p /content/GPUcomputing/lab2\n","%cd /content/GPUcomputing/lab2\n","!mkdir -p src\n"],"metadata":{"id":"vpnw20UGiE5H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iUYP4kCJhEIx"},"source":["# ‚ñ∂Ô∏è DeviceQuery"]},{"cell_type":"code","metadata":{"id":"kW9b_Yuxi7id"},"source":["# DeviceQuery dell'attuale device (su Colab!)\n","!nvcc /content/GPUcomputing/utils/deviceQuery.cu -o deviceQuery\n","!./deviceQuery"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Check whether the device can transfer in both directions simultaneously"],"metadata":{"id":"J-EcbVrAYlfS"}},{"cell_type":"code","source":["%%cu\n","#include <stdio.h>\n","\n","int main(void) {\n","\n","  cudaDeviceProp dProp;\n","\tcudaGetDeviceProperties(&dProp, 0);\n","\n","  // Shows whether the device can transfer in both directions simultaneously\n","  printf(\"Device %s capable of simultaneous CPU-to-GPU and GPU-to-CPU datatransfers\\n\", dProp.deviceOverlap ? \"IS\": \"NOT\");\n","  return 0;\n","}"],"metadata":{"id":"esYDQTl-Yg3B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lAVvcKOX_DU0"},"source":["# ‚úÖ Somma array con stream"]},{"cell_type":"markdown","source":["This example demonstrates overlapping computation and communication by\n","partitioning a data set and asynchronously launching the memory copies and kernels for each subset. Launching all transfers and kernels for a given subset in the same CUDA stream ensures that computation on the device is not started until the necessary data has been transferred. However, because the work of each subset is independent of all other subsets, the communication and computation of different subsets will overlap.\n","\n","This example launches copies and kernels in breadth-first order."],"metadata":{"id":"xpJ_WqJ-kJfy"}},{"cell_type":"code","metadata":{"id":"OH6TuWnB-_0M"},"source":["%%cuda --name sumArrayStream.cu\n","\n","#include \"../GPUcomputing/utils/common.h\"\n","\n","#define NSTREAM 4\n","#define BDIM 128\n","\n","void initialData(float *ip, int size) {\n","  int i;\n","\n","  for(i = 0; i < size; i++)\n","    ip[i] = (float)(rand() & 0xFF) / 10.0f;\n","}\n","\n","void sumArraysOnHost(float *A, float *B, float *C, const int N) {\n","  for (int idx = 0; idx < N; idx++)\n","    C[idx] = A[idx] + B[idx];\n","}\n","\n","__global__ void sumArrays(float *A, float *B, float *C, const int N) {\n","  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","  if (idx < N)\n","      C[idx] = A[idx] + B[idx];\n","}\n","\n","void checkResult(float *hostRef, float *gpuRef, const int N) {\n","  double epsilon = 1.0E-8;\n","  bool match = 1;\n","\n","  for (int i = 0; i < N; i++) {\n","    if (abs(hostRef[i] - gpuRef[i]) > epsilon) {\n","      match = 0;\n","      printf(\"Arrays do not match!\\n\");\n","      printf(\"host %5.2f gpu %5.2f at %d\\n\", hostRef[i], gpuRef[i], i);\n","      break;\n","    }\n","  }\n","  if (match) \n","    printf(\"Arrays match.\\n\\n\");\n","}\n","\n","//# MAIN\n","int main(int argc, char **argv) {\n","  printf(\"> %s Starting...\\n\", argv[0]);\n","\n","  int dev = 0;\n","  cudaDeviceProp deviceProp;\n","  CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n","  printf(\"> Using Device %d: %s\\n\", dev, deviceProp.name);\n","  CHECK(cudaSetDevice(dev));\n","\n","  //# check if device support hyper-q\n","  if (deviceProp.major < 3 || (deviceProp.major == 3 && deviceProp.minor < 5)) {\n","    if (deviceProp.concurrentKernels == 0) {\n","      printf(\"> GPU does not support concurrent kernel execution (SM 3.5 or higher required)\\n\");\n","      printf(\"> CUDA kernel runs will be serialized\\n\");\n","    }\n","    else {\n","      printf(\"> GPU does not support HyperQ\\n\");\n","      printf(\"> CUDA kernel runs will have limited concurrency\\n\");\n","    }\n","  }\n","\n","  // Shows whether the device can transfer in both directions simultaneously\n","  printf(\"> Device %s capable of simultaneous CPU-to-GPU datatransfers\\n\", deviceProp.deviceOverlap ? \"IS\": \"NOT\");\n","\n","  printf(\"> Compute Capability %d.%d hardware with %d multi-processors\\n\",\n","          deviceProp.major, deviceProp.minor, deviceProp.multiProcessorCount);\n","\n","  printf (\"> with streams = %d\\n\", NSTREAM);\n","\n","  // set up data size of vectors\n","  int nElem = 1 << 18;\n","  printf(\"> vector size = %d\\n\", nElem);\n","  size_t nBytes = nElem * sizeof(float);\n","\n","  // malloc pinned host memory for async memcpy\n","  float *h_A, *h_B, *hostRef, *gpuRef;\n","  CHECK(cudaHostAlloc((void**)&h_A, nBytes, cudaHostAllocDefault));\n","  CHECK(cudaHostAlloc((void**)&h_B, nBytes, cudaHostAllocDefault));\n","  CHECK(cudaHostAlloc((void**)&gpuRef, nBytes, cudaHostAllocDefault));\n","  CHECK(cudaHostAlloc((void**)&hostRef, nBytes, cudaHostAllocDefault));\n","\n","  // initialize data at host side\n","  initialData(h_A, nElem);\n","  initialData(h_B, nElem);\n","  memset(hostRef, 0, nBytes);\n","  memset(gpuRef,  0, nBytes);\n","\n","  // add vector at host side for result checks\n","  sumArraysOnHost(h_A, h_B, hostRef, nElem);\n","\n","  // malloc device global memory\n","  float *d_A, *d_B, *d_C;\n","  CHECK(cudaMalloc((float**)&d_A, nBytes));\n","  CHECK(cudaMalloc((float**)&d_B, nBytes));\n","  CHECK(cudaMalloc((float**)&d_C, nBytes));\n","\n","  cudaEvent_t start, stop;\n","  CHECK(cudaEventCreate(&start));\n","  CHECK(cudaEventCreate(&stop));\n","\n","  // invoke kernel at host side\n","  dim3 block (BDIM);\n","  dim3 grid  ((nElem + block.x - 1) / block.x);\n","  printf(\"> grid (%d, %d) block (%d, %d)\\n\", grid.x, grid.y, block.x, block.y);\n","\n","  // sequential operation\n","  CHECK(cudaEventRecord(start, 0));\n","  CHECK(cudaMemcpy(d_A, h_A, nBytes, cudaMemcpyHostToDevice));\n","  CHECK(cudaMemcpy(d_B, h_B, nBytes, cudaMemcpyHostToDevice));\n","  CHECK(cudaEventRecord(stop, 0));\n","  CHECK(cudaEventSynchronize(stop));\n","  float memcpy_h2d_time;\n","  CHECK(cudaEventElapsedTime(&memcpy_h2d_time, start, stop));\n","\n","  CHECK(cudaEventRecord(start, 0));\n","  sumArrays<<<grid, block>>>(d_A, d_B, d_C, nElem);\n","  CHECK(cudaEventRecord(stop, 0));\n","  CHECK(cudaEventSynchronize(stop));\n","  float kernel_time;\n","  CHECK(cudaEventElapsedTime(&kernel_time, start, stop));\n","\n","  CHECK(cudaEventRecord(start, 0));\n","  CHECK(cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost));\n","  CHECK(cudaEventRecord(stop, 0));\n","  CHECK(cudaEventSynchronize(stop));\n","  float memcpy_d2h_time;\n","  CHECK(cudaEventElapsedTime(&memcpy_d2h_time, start, stop));\n","  float itotal = kernel_time + memcpy_h2d_time + memcpy_d2h_time;\n","\n","  printf(\"\\n\");\n","  printf(\"Measured timings (throughput):\\n\");\n","  printf(\" Memcpy host to device\\t: %f ms (%f GB/s)\\n\", memcpy_h2d_time, (nBytes * 1e-6) / memcpy_h2d_time);\n","  printf(\" Memcpy device to host\\t: %f ms (%f GB/s)\\n\", memcpy_d2h_time, (nBytes * 1e-6) / memcpy_d2h_time);\n","  printf(\" Kernel\\t\\t\\t: %f ms (%f GB/s)\\n\", kernel_time, (nBytes * 2e-6) / kernel_time);\n","  printf(\" Total\\t\\t\\t: %f ms (%f GB/s)\\n\", itotal, (nBytes * 2e-6) / itotal);\n","\n","  // grid parallel operation\n","  int iElem = nElem / NSTREAM;\n","  size_t iBytes = iElem * sizeof(float);\n","  grid.x = (iElem + block.x - 1) / block.x;\n","\n","  cudaStream_t stream[NSTREAM];\n","\n","  for (int i = 0; i < NSTREAM; ++i)\n","    CHECK(cudaStreamCreate(&stream[i]));\n","\n","  CHECK(cudaEventRecord(start, 0));\n","\n","  // initiate all asynchronous transfers to the device\n","  for (int i = 0; i < NSTREAM; ++i) {\n","    int ioffset = i * iElem;\n","    CHECK(cudaMemcpyAsync(&d_A[ioffset], &h_A[ioffset], iBytes, cudaMemcpyHostToDevice, stream[i]));\n","    CHECK(cudaMemcpyAsync(&d_B[ioffset], &h_B[ioffset], iBytes, cudaMemcpyHostToDevice, stream[i]));\n","  }\n","\n","  // launch a kernel in each stream\n","  for (int i = 0; i < NSTREAM; ++i) {\n","    int ioffset = i * iElem;\n","    sumArrays<<<grid, block, 0, stream[i]>>>(&d_A[ioffset], &d_B[ioffset], &d_C[ioffset], iElem);\n","  }\n","\n","  // enqueue asynchronous transfers from the device\n","  for (int i = 0; i < NSTREAM; ++i) {\n","    int ioffset = i * iElem;\n","    CHECK(cudaMemcpyAsync(&gpuRef[ioffset], &d_C[ioffset], iBytes, cudaMemcpyDeviceToHost, stream[i]));\n","  }\n","\n","  CHECK(cudaEventRecord(stop, 0));\n","  CHECK(cudaEventSynchronize(stop));\n","  float execution_time;\n","  CHECK(cudaEventElapsedTime(&execution_time, start, stop));\n","\n","  printf(\"\\n\");\n","  printf(\"Actual results from overlapped data transfers:\\n\");\n","  printf(\" overlap with %d streams : %f ms (%f GB/s)\\n\", NSTREAM, execution_time, (nBytes * 2e-6) / execution_time );\n","  printf(\" speedup                : %f \\n\", ((itotal - execution_time) * 100.0f) / itotal);\n","\n","  // check kernel error\n","  CHECK(cudaGetLastError());\n","\n","  // check device results\n","  checkResult(hostRef, gpuRef, nElem);\n","\n","  // free device global memory\n","  CHECK(cudaFree(d_A));\n","  CHECK(cudaFree(d_B));\n","  CHECK(cudaFree(d_C));\n","\n","  // free host memory\n","  CHECK(cudaFreeHost(h_A));\n","  CHECK(cudaFreeHost(h_B));\n","  CHECK(cudaFreeHost(hostRef));\n","  CHECK(cudaFreeHost(gpuRef));\n","\n","  // destroy events\n","  CHECK(cudaEventDestroy(start));\n","  CHECK(cudaEventDestroy(stop));\n","\n","  // destroy streams\n","  for (int i = 0; i < NSTREAM; ++i)\n","    CHECK(cudaStreamDestroy(stream[i]));\n","\n","  CHECK(cudaDeviceReset());\n","  return(0);\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7G91KROXBScK"},"source":["# Compilazione ed esecuzione\n","\n","!nvcc -arch=sm_75  src/sumArrayStream.cu  -o sumArray\n","!./sumArray"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ‚úÖ Tabular"],"metadata":{"id":"Cg_Y5jdVwY_j"}},{"cell_type":"markdown","metadata":{"id":"vXUIQkZLCTcG"},"source":["# üî¥ TODO\n"]},{"cell_type":"markdown","source":["1. Come modificare il kernel per usare gli stream\n","2. Gestione della memoria pinned e device\n","\n","Applicare\n","3. Schema: loop over {copy, kernel, copy}\n","4. Schema: loop over {copy H2D}, loop over {kernel}, loop over {copy D2H}\n"],"metadata":{"id":"NtEOIgt3utGz"}},{"cell_type":"code","metadata":{"id":"-Y52R0d3CA50"},"source":["%%cuda --name tabular.cu\n","\n","#include <stdio.h>\n","#include \"../GPUcomputing/utils/common.h\"\n","\n","#define PI 3.141592f\n","\n","/*\n"," * Kernel: tabular function\n"," */\n","__global__ void tabular(float *a, int n) {\n","\tint i = threadIdx.x + blockIdx.x * blockDim.x;\n","\tif (i < n) {\n","\t\tfloat x = PI * (float)i / (float)n;\n","\t\tfloat s = sinf(x);\n","\t\tfloat c = cosf(x);\n","\t\ta[i] = sqrtf(abs(s * s - c * c));\n","\t}\n","}\n","\n","/*\n"," * Kernel: tabular function using streams\n"," */\n","__global__ void tabular_streams(float *a, int n, int offset) {\n","\tint i = offset + threadIdx.x + blockIdx.x * blockDim.x;\n","  if (i < n) {\n","    float x = PI * (float)i / (float)n;\n","    float s = sinf(x);\n","    float c = cosf(x);\n","    a[i] = sqrtf(abs(s * s - c * c));\n","  }\n","}\n","\n","/*\n"," * Error measure\n"," */\n","float maxError(float *a, int n) {\n","\tfloat maxE = 0;\n","\tfor (int i = 0; i < n; i++) {\n","\t\tfloat error = fabs(a[i] - 1.0f);\n","\t\tif (error > maxE)\n","\t\t\tmaxE = error;\n","\t}\n","\treturn maxE;\n","}\n","\n","/*\n"," * Main: tabular function\n"," */\n","int main(void) {\n","\t\n","  // main params\n","  uint MB = 1024*1024; \n","  uint n = 256*MB;\n","\tint blockSize = 256;\n","\n","\t// streams\n","\t\n","\t// allocate pinned host memory and device memory\n","\t\n","\t// create events and streams\n","\t\n","\t// baseline case - sequential transfer and execute\n","\t\n","\t// asynchronous version 1: loop over {copy, kernel, copy}\n","\t\n","\t// asynchronous version 2: loop over copy, loop over kernel, loop over copy\n","\t\n","\t// cleanup\n","\t\n","\treturn 0;\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0PSc9B9PDTWt"},"source":["# Compilazione ed esecuzione\n","\n","!nvcc -arch=sm_75 src/tabular.cu  -o tabular\n","!./tabular"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mfR471rze2p-"},"source":["# profilazione (senza unified memory - d√† errore)\n","\n","!nvprof ./tabular"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SOFMQZAkjlLW"},"source":["# ‚úÖ MQDB con stream"]},{"cell_type":"markdown","source":["# üî¥ TODO"],"metadata":{"id":"8renyNfAwh4j"}},{"cell_type":"markdown","source":["- Disegnare un kernel per il prodotto tra matrici MQDB con le seguenti specifiche:\n","- Allocare spazio per matrici MQDB su CPU e GPU \n","- Confrontare uso di memoria unificata vs memoria asincrona\n","- Introdurre gli stream su cui distribuire il carico (grid parall.)\n","- Analisi di prestazioni usando i tempi ricavati con CUDA event"],"metadata":{"id":"6yomfFJusrYT"}},{"cell_type":"code","metadata":{"id":"v9nRkLgeB10A"},"source":["%%cuda --name  MQDB_stream_Unified.cu\n","\n","\n","#include \"../GPUcomputing/utils/MQDB/mqdb.h\"\n","#include \"../GPUcomputing/utils/common.h\"\n","\n","#define BLOCK_SIZE 16     // block size\n","#define TEST_CPU 0\n","\n","/*\n"," * Kernel for standard (naive) matrix product\n"," */\n","__global__ void matProdKernel(mqdb *A, mqdb *B, mqdb *C, int n) {\n","\t// row & col indexes\n","\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n","\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// each thread computes an entry of the product matrix\n","\tif ((row < n) && (col < n)) {\n","\t\tfloat val = 0;\n","\t\tfor (int k = 0; k < n; k++)\n","\t\t\tval += A->elem[row * n + k] * B->elem[k * n + col];\n","\t\tC->elem[row * n + col] = val;\n","\t}\n","}\n","\n","/*\n"," * Kernel for block sub-matrix product of mqdb\n"," */\n","__global__ void mqdbBlockProd(mqdb *A, mqdb *B, mqdb *C, uint sdim, uint d, uint n) {\n","\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n","\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// jump to the right block sub-matrix\n","\tuint  offset = (n+1)*sdim;\n","\n","\t// each thread computes an entry of the product matrix\n","\tif ((row < d) && (col < d)) {\n","\t\tfloat val = 0;\n","\t\tfor (int k = 0; k < d; k++)\n","\t\t\tval += A->elem[row * n + k + offset] * B->elem[k * n + col + offset];\n","\t\tC->elem[row * n + col + offset] = val;\n","\t}\n","}\n","\n","\n","/*\n"," * Test on MQDB kernels using Unified Memory\n"," */\n","void testKernelsMQDB_unified(uint n, uint k, cudaEvent_t start, cudaEvent_t stop) {\n","\n","\t// matrix instance generation - Unified Memory\n","\t\n","\n","  // random fill mat entries\n","  \n","\n","\t\n","\t/***********************************************************/\n","\t/*                     GPU mat product                     */\n","\t/***********************************************************/\n","\t\n","  printf(\"Kernel (naive) mat product...\\n\");\n","\t\n","\t/***********************************************************/\n","\t/*                     GPU MQDB product                    */\n","\t/***********************************************************/\n","\t\n","  printf(\"Kernel MQDB product...\\n\");\n","\t\n","\n","  /***********************************************************/\n","\t/*             GPU MQDB product using streams              */\n","\t/***********************************************************/\n","\t\n","  printf(\"Kernel MQDB product using streams...\\n\");\n","\n","  \n","\n","\t// clean up streams and events\n","\t\n","\n","}\n","\n","/*\n"," * main function\n"," */\n","int main(int argc, char *argv[]) {\n","  \n","  // set up device\n","\tint dev = 0;\n","\tcudaDeviceProp deviceProp;\n","\tCHECK(cudaGetDeviceProperties(&deviceProp, dev));\n","\tprintf(\"%s starting mqdb product at \", argv[0]);\n","\tprintf(\"device %d: %s\\n\", dev, deviceProp.name);\n","\tCHECK(cudaSetDevice(dev));\n","\n","\t// events to measure time\n","\tcudaEvent_t start, stop;\n","\tcudaEventCreate(&start);\n","\tcudaEventCreate(&stop);\n","\n","\tuint n = 8*1024;         // matrix size\n","\tuint min_k = 20;         // min num of blocks\n","\tuint max_k = 30;         // max num of blocks\n","\n","\t// multiple tests for k = # diag blocks\n","\tfor (uint k = min_k; k <= max_k; k+=5) {\n","\t\tprintf(\"\\n*****   k = %d --- (avg block size = %f)\\n\",k,(float)n/k);\n","\t\ttestKernelsMQDB_unified(n, k, start, stop);\n","\t}\n","\n","  cudaEventDestroy(start);\n","\tcudaEventDestroy(stop);\n","\treturn 0;\n","}\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wLxZjCx8bT3s"},"source":["# Compilazione ed esecuzione\n","!nvcc -arch=sm_75  src/MQDB_stream_Unified.cu GPUcomputing/utils/MQDB/mqdb.cpp -o MQDBS\n","!./MQDBS"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6fYYbj397SdK"},"source":["%%cuda --name MQDB_stream_manual.cu\n","\n","\n","#include \"../GPUcomputing/utils/MQDB/mqdb.h\"\n","#include \"../GPUcomputing/utils/common.h\"\n","\n","#define BLOCK_SIZE 16     // block size\n","\n","/*\n"," * Kernel for block sub-matrix product of mqdb\n"," */\n","__global__ void mqdbBlockProd(mqdb A, mqdb B, mqdb C, uint sdim, uint d, uint n) {\n","\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n","\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// jump to the right block sub-matrix\n","\tuint  offset = (n+1)*sdim;\n","\n","\t// each thread computes an entry of the product matrix\n","\tif ((row < d) && (col < d)) {\n","\t\tfloat val = 0;\n","\t\tfor (int k = 0; k < d; k++)\n","\t\t\tval += A.elem[row * n + k + offset] * B.elem[k * n + col + offset];\n","\t\tC.elem[row * n + col + offset] = val;\n","\t}\n","}\n","\n","/*\n"," * Test on MQDB kernels using manual async memory\n"," */\n","void testKernelsMQDB_manual_mem(uint n, uint k, cudaEvent_t start, cudaEvent_t stop) {\n","\n","\t// matrices\n","\tmqdb *A, *B, *C;         // host \n","\tmqdb d_A, d_B, d_C;      // device\n","\n","  ulong nBytes = n * n * sizeof(float);\n","  int kBytes = k * sizeof(int);\n","\tprintf(\"Memory size required = %3.4f (MB)\\n\",(float)nBytes/(1024.0*1024.0));\n","\n","\n","  // host and device Memory\n","\tCHECK(cudaMallocHost(&A, sizeof(mqdb)));\n","  CHECK(cudaMallocHost(&A->blkSize, kBytes));\n","  CHECK(cudaMallocHost(&A->elem, nBytes));\n","  CHECK(cudaMalloc(&d_A.blkSize, kBytes));\n","  CHECK(cudaMalloc(&d_A.elem, nBytes));\n","\n","  CHECK(cudaMallocHost(&B, sizeof(mqdb)));\n","  CHECK(cudaMallocHost(&B->blkSize, kBytes));\n","  CHECK(cudaMallocHost(&B->elem, nBytes));\n","\tCHECK(cudaMalloc(&d_B.blkSize, kBytes));\n","  CHECK(cudaMalloc(&d_B.elem, nBytes));\n","\t\n","  CHECK(cudaMallocHost(&C, sizeof(mqdb)));\n","  CHECK(cudaMallocHost(&C->blkSize, kBytes));\n","  CHECK(cudaMallocHost(&C->elem, nBytes));\n","\tCHECK(cudaMalloc(&d_C.blkSize, kBytes));\n","  CHECK(cudaMalloc(&d_C.elem, nBytes));\n","\n","  // random fill mat entries\n","  int seed = 1;\n","\tgenRandDims(A, n, k, seed);\n","\tgenRandDims(B, n, k, seed);\n","\tgenRandDims(C, n, k, seed);\n","\tfillBlocks(A, n, k, 'C', 1);\n","\tfillBlocks(B, n, k, 'C', 2);\n","\tfillBlocks(C, n, k, 'C', 0);\n","\n","\t// copy blk sizes on device memory\n","\t//CHECK(cudaMemcpy(d_A->blkSize, A->blkSize, kBytes, cudaMemcpyHostToDevice));\n","\t//CHECK(cudaMemcpy(d_B->blkSize, B->blkSize, kBytes, cudaMemcpyHostToDevice));\n","\t//CHECK(cudaMemcpy(d_C->blkSize, C->blkSize, kBytes, cudaMemcpyHostToDevice));\n","\t\n","  /***********************************************************/\n","\t/*       GPU MQDB product using streams & async copy       */\n","\t/***********************************************************/\n","\tprintf(\"GPU MQDB product using streams...\\n\");\n","\n","\tdim3 block(BLOCK_SIZE, BLOCK_SIZE);\n","  int nstreams = A->nBlocks;\n","\tcudaStream_t streams[nstreams];\n","\tuint dsum = 0;  // bound dx\n","\tCHECK(cudaEventRecord(start));\n","\tfor (int i = 0; i < nstreams; i++) {\n","\t\tCHECK(cudaStreamCreate(&streams[i]));\n","\t\tuint d = A->blkSize[i];\n","    int offset = dsum*n;\n","    int streamBytes = d*n;\n","\t\tCHECK(cudaMemcpyAsync(&d_A.elem[offset], &A->elem[offset], streamBytes, cudaMemcpyHostToDevice, streams[i]));\n","    CHECK(cudaMemcpyAsync(&d_B.elem[offset], &B->elem[offset], streamBytes, cudaMemcpyHostToDevice, streams[i]));\n","\t\tdim3 grid((d + block.x - 1) / block.x, (d + block.y - 1) / block.y);\n","\t\tmqdbBlockProd<<<grid, block, 0, streams[i]>>>(d_A, d_B, d_C, dsum, d, n);\n","    CHECK(cudaMemcpyAsync(&C->elem[offset], &d_C.elem[offset], streamBytes, cudaMemcpyDeviceToHost, streams[i]));\n","\t\tdsum += d;\n","\t}\n","\tCHECK(cudaEventRecord(stop));\n","\tCHECK(cudaEventSynchronize(stop));\n","  float milliseconds;\n","\tCHECK(cudaEventElapsedTime(&milliseconds, start, stop));\n","\tfloat GPUtime3 = milliseconds / 1000.0;\n","\tprintf(\"   elapsed time                  : %.5f (sec)\\n\", GPUtime3);\n","\n","\t// clean up streams and events\n","\tfor (int i = 0; i < nstreams; i++)\n","\t\tcudaStreamDestroy(streams[i]);\n","\n","} \n","\n","/*\n"," * main function\n"," */\n","int main(int argc, char *argv[]) {\n","  \n","  // set up device\n","\tint dev = 0;\n","\tcudaDeviceProp deviceProp;\n","\tCHECK(cudaGetDeviceProperties(&deviceProp, dev));\n","\tprintf(\"%s starting mqdb product at \", argv[0]);\n","\tprintf(\"device %d: %s\\n\", dev, deviceProp.name);\n","\tCHECK(cudaSetDevice(dev));\n","\n","\t// events to measure time\n","\tcudaEvent_t start, stop;\n","\tcudaEventCreate(&start);\n","\tcudaEventCreate(&stop);\n","\n","\tuint n = 16*1024;         // matrix size\n","\tuint min_k = 20;       // max num of blocks\n","\tuint max_k = 30;       // max num of blocks\n","\n","\t// multiple tests for k = # diag blocks\n","\tfor (uint k = min_k; k <= max_k; k+=5) {\n","\t\tprintf(\"\\n*****   k = %d --- (avg block size = %f)\\n\",k,(float)n/k);\n","\t\ttestKernelsMQDB_manual_mem(n, k, start, stop);\n","\t}\n","\n","  cudaEventDestroy(start);\n","\tcudaEventDestroy(stop);\n","\treturn 0;\n","}\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HmMmj5KDnteq"},"source":["# Compilazione ed esecuzione\n","!nvcc -arch=sm_75  src/MQDB_stream_manual.cu GPUcomputing/utils/MQDB/mqdb.cpp -o MQDBS\n","!./MQDBS"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pwd\n"],"metadata":{"id":"YQ-T_7OJVP6x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-jV4PRaBVQo-"},"execution_count":null,"outputs":[]}]}