{"cells":[{"cell_type":"markdown","metadata":{"id":"fZYqN0UwVLC_"},"source":["---\n","# **LAB 6 - Global memory (GMEM)**\n","---"]},{"cell_type":"markdown","metadata":{"id":"WoJbB3T5Vkw-"},"source":["# ‚ñ∂Ô∏è CUDA setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fht2Wy8wVkxJ","vscode":{"languageId":"python"}},"outputs":[],"source":["!nvcc --version"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0jP2H_YJVkxJ","vscode":{"languageId":"python"}},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"VKbaxH9wWosO"},"source":["## [GPU Compute Capability](https://developer.nvidia.com/cuda-gpus)"]},{"cell_type":"markdown","metadata":{"id":"_cGSqZovVkxK"},"source":["## NVCC Plugin for Jupyter notebook\n","\n","*Usage*:\n","\n","\n","*   Load Extension `%load_ext nvcc_plugin`\n","*   Mark a cell to be treated as cuda cell\n","`%%cuda --name example.cu --compile false`\n","\n","**NOTE**: The cell must contain either code or comments to be run successfully. It accepts 2 arguments. `-n | --name` - which is the name of either CUDA source or Header. The name parameter must have extension `.cu` or `.h`. Second argument -c | --compile; default value is false. The argument is a flag to specify if the cell will be compiled and run right away or not. It might be usefull if you're playing in the main function\n","\n","*  We are ready to run CUDA C/C++ code right in your Notebook. For this we need explicitly say to the interpreter, that we want to use the extension by adding `%%cu` at the beginning of each cell with CUDA code. \n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RCVhMkqYVkxK","vscode":{"languageId":"python"}},"outputs":[],"source":["!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e6PDOytTVkxK","vscode":{"languageId":"python"}},"outputs":[],"source":["%load_ext nvcc_plugin"]},{"cell_type":"markdown","metadata":{"id":"IYG8Cv4bTzyI"},"source":["Clone GPUcomputing site on github..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E7jZmHjCT0vu","vscode":{"languageId":"python"}},"outputs":[],"source":["!git clone https://github.com/giulianogrossi/GPUcomputing.git"]},{"cell_type":"markdown","metadata":{"id":"zs_a5Vuimily"},"source":["# ‚ñ∂Ô∏è VS Code on Colab"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"BCf9JxqphHAp","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title Colab-ssh tunnel\n","#@markdown Execute this cell to open the ssh tunnel. Check [colab-ssh documentation](https://github.com/WassimBenzarti/colab-ssh) for more details.\n","\n","# Install colab_ssh on google colab\n","!pip install colab_ssh --upgrade\n","\n","from colab_ssh import launch_ssh_cloudflared, init_git_cloudflared\n","ssh_tunnel_password = \"gpu\" #@param {type: \"string\"}\n","launch_ssh_cloudflared(password=ssh_tunnel_password)\n","\n","# Optional: if you want to clone a Github or Gitlab repository\n","repository_url=\"https://github.com/giulianogrossi/GPUcomputing\" #@param {type: \"string\"}\n","init_git_cloudflared(repository_url)"]},{"cell_type":"markdown","metadata":{"id":"iUYP4kCJhEIx"},"source":["# ‚ñ∂Ô∏è DeviceQuery"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kW9b_Yuxi7id","vscode":{"languageId":"python"}},"outputs":[],"source":["# DeviceQuery dell'attuale device (su Colab!)\n","!nvcc /content/GPUcomputing/utils/deviceQuery.cu -o deviceQuery\n","!./deviceQuery"]},{"cell_type":"markdown","metadata":{"id":"DvmApCF76YD0"},"source":["# ‚úÖ Static and pinned memory"]},{"cell_type":"markdown","metadata":{"id":"4ApFq6kF6Olh"},"source":["**Pinned memory**\n","\n","An example of using CUDA's memory copy API to transfer data to and from the device. In this case, `cudaMalloc` is used to allocate memory on the GPU and `cudaMemcpy` is used to transfer the contents of host memory to an array allocated using `cudaMalloc`. Host memory is allocated using `cudaMallocHost` to create a page-locked host array."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XObjkBa16Wx2","vscode":{"languageId":"python"}},"outputs":[],"source":["%%cuda --name pinMemTransfer.cu\n","\n","#include \"../GPUcomputing/utils/common.h\"\n","#include <cuda_runtime.h>\n","#include <stdio.h>\n","\n","int main(int argc, char **argv) {\n","  //# set up device\n","  int dev = 0;\n","  CHECK(cudaSetDevice(dev));\n","\n","  //# memory size\n","  unsigned long isize = 1 << 30;\n","  unsigned long nbytes = isize * sizeof(float);\n","\n","  //# get device information\n","  cudaDeviceProp deviceProp;\n","  CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n","\n","  //# check if device support pinned memory\n","  if (!deviceProp.canMapHostMemory) {\n","      printf(\"Device %d does not support mapping CPU host memory!\\n\", dev);\n","      CHECK(cudaDeviceReset());\n","      exit(EXIT_SUCCESS);\n","  }\n","\n","  printf(\"device %d: %s memory size = %lu byte (%5.2f MB) and canMapHostMemory = %d\\n\", dev,\n","          deviceProp.name, isize, nbytes / (1024.0f * 1024.0f),\n","          deviceProp.canMapHostMemory);\n","\n","  float *h_a;\n","  //# allocate the host memory   \n","  //h_a = (float *)malloc(nbytes);\n","\n","  //# allocate pinned host memory\n","  CHECK(cudaMallocHost ((float **)&h_a, nbytes));\n","\n","  //# allocate device memory\n","  float *p_a;\n","  CHECK(cudaMalloc((float **)&p_a, nbytes));\n","\n","  for (int i = 0; i < isize; i++) \n","    h_a[i] = 100.10f;\n","\n","  //# transfer data from the host to the device\n","  CHECK(cudaMemcpy(p_a, h_a, nbytes, cudaMemcpyHostToDevice));\n","\n","  //# transfer data from the device to the host\n","  CHECK(cudaMemcpy(h_a, p_a, nbytes, cudaMemcpyDeviceToHost));\n","\n","  //# free memory\n","  CHECK(cudaFree(p_a));\n","  CHECK(cudaFreeHost(h_a));\n","\n","  // reset device\n","  CHECK(cudaDeviceReset());\n","  return EXIT_SUCCESS;\n","}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"arU-6PkD7V4A","vscode":{"languageId":"python"}},"outputs":[],"source":["!nvcc -O3 -arch=sm_75 src/pinMemTransfer.cu -o pinMemTransfer\n","!nvprof ./pinMemTransfer"]},{"cell_type":"markdown","metadata":{"id":"vXUIQkZLCTcG"},"source":["# ‚úÖ Unified memory\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Stv28L2PU-Kg","vscode":{"languageId":"python"}},"outputs":[],"source":["%%cuda --name sumMatrix.cu\n","\n","#include <stdio.h>\n","#include \"../GPUcomputing/utils/common.h\"\n","\n","void initialData(float *ip, const int size) {\n","  int i;\n","\n","  for (i = 0; i < size; i++)\n","    ip[i] = (float)( rand() & 0xFF ) / 10.0f;\n","  return;\n","}\n","\n","void sumMatrixOnHost(float *A, float *B, float *C, const int nx, const int ny) {\n","  float *ia = A;\n","  float *ib = B;\n","  float *ic = C;\n","\n","  for (int iy = 0; iy < ny; iy++) {\n","    for (int ix = 0; ix < nx; ix++)\n","      ic[ix] = ia[ix] + ib[ix];\n","\n","    ia += nx;\n","    ib += nx;\n","    ic += nx;\n","  }\n","  return;\n","}\n","\n","void checkResult(float *hostRef, float *gpuRef, const int N) {\n","  double epsilon = 1.0E-8;\n","  bool match = 1;\n","\n","  for (int i = 0; i < N; i++) {\n","    if (abs(hostRef[i] - gpuRef[i]) > epsilon) {\n","      match = 0;\n","      printf(\"host %f gpu %f\\n\", hostRef[i], gpuRef[i]);\n","      break;\n","    }\n","  }\n","\n","  if (!match)\n","    printf(\"Arrays do not match.\\n\\n\");\n","}\n","\n","//# matrix sum with grid 2D block 2D\n","__global__ void sumMatrixGPU(float *MatA, float *MatB, float *MatC, int nx, int ny) {\n","  unsigned int ix = threadIdx.x + blockIdx.x * blockDim.x;\n","  unsigned int iy = threadIdx.y + blockIdx.y * blockDim.y;\n","  unsigned int idx = iy * nx + ix;\n","\n","  if (ix < nx && iy < ny)\n","    MatC[idx] = MatA[idx] + MatB[idx];\n","}\n","\n","//# MAIN\n","int main(int argc, char **argv) {\n","    printf(\"%s Starting \", argv[0]);\n","\n","    //# set up device\n","    int dev = 0;\n","    cudaDeviceProp deviceProp;\n","    CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n","    printf(\"using Device %d: %s\\n\", dev, deviceProp.name);\n","    CHECK(cudaSetDevice(dev));\n","\n","    //# set up data size of matrix\n","    int nx, ny;\n","    int ishift = 12;\n","    if  (argc > 1) ishift = atoi(argv[1]);\n","    nx = ny = 1 << ishift;\n","\n","    int nxy = nx * ny;\n","    int nBytes = nxy * sizeof(float);\n","    printf(\"Matrix size: nx %d ny %d\\n\", nx, ny);\n","\n","    //# malloc host memory\n","    float *h_A, *h_B, *hostRef, *gpuRef;\n","    h_A = (float *)malloc(nBytes);\n","    h_B = (float *)malloc(nBytes);\n","    hostRef = (float *)malloc(nBytes);\n","    gpuRef = (float *)malloc(nBytes);\n","\n","    // initialize data at host side\n","    double iStart = seconds();\n","    initialData(h_A, nxy);\n","    initialData(h_B, nxy);\n","    double iElaps = seconds() - iStart;\n","\n","    printf(\"initialization: \\t %f sec\\n\", iElaps);\n","\n","    memset(hostRef, 0, nBytes);\n","    memset(gpuRef, 0, nBytes);\n","\n","    //# add matrix at host side for result checks\n","    iStart = seconds();\n","    sumMatrixOnHost(h_A, h_B, hostRef, nx, ny);\n","    iElaps = seconds() - iStart;\n","    printf(\"sumMatrix on host:\\t %f sec\\n\", iElaps);\n","\n","    //# malloc device global memory\n","    float *d_MatA, *d_MatB, *d_MatC;\n","    CHECK(cudaMalloc((void **)&d_MatA, nBytes));\n","    CHECK(cudaMalloc((void **)&d_MatB, nBytes));\n","    CHECK(cudaMalloc((void **)&d_MatC, nBytes));\n","\n","    //# transfer data from host to device\n","    CHECK(cudaMemcpy(d_MatA, h_A, nBytes, cudaMemcpyHostToDevice));\n","    CHECK(cudaMemcpy(d_MatB, h_B, nBytes, cudaMemcpyHostToDevice));\n","    \n","    //# invoke kernel at host side\n","    int dimx = 32;\n","    int dimy = 32;\n","    dim3 block(dimx, dimy);\n","    dim3 grid((nx + block.x - 1) / block.x, (ny + block.y - 1) / block.y);\n","\n","    iStart =  seconds();\n","    sumMatrixGPU<<<grid, block>>>(d_MatA, d_MatB, d_MatC, nx, ny);\n","    CHECK(cudaDeviceSynchronize());\n","    iElaps = seconds() - iStart;\n","    printf(\"sumMatrix on gpu :\\t %f sec <<<(%d,%d), (%d,%d)>>> \\n\", iElaps, grid.x, grid.y, block.x, block.y);\n","    CHECK(cudaMemcpy(gpuRef, d_MatC, nBytes, cudaMemcpyDeviceToHost));\n","    //# check kernel error\n","    CHECK(cudaGetLastError());\n","    //# check device results\n","    checkResult(hostRef, gpuRef, nxy);\n","\n","    // free device global memory\n","    CHECK(cudaFree(d_MatA));\n","    CHECK(cudaFree(d_MatB));\n","    CHECK(cudaFree(d_MatC));\n","\n","    // free host memory\n","    free(h_A);\n","    free(h_B);\n","    free(hostRef);\n","    free(gpuRef);\n","\n","    // reset device\n","    CHECK(cudaDeviceReset());\n","\n","    return (0);\n","}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LezrfqF5VPEw","vscode":{"languageId":"python"}},"outputs":[],"source":["# Compilazione ed esecuzione\n","\n","!nvcc -arch=sm_70 src/sumMatrix.cu  -o sumMatrix\n","!./sumMatrix 14"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iRqExSDZViHG","vscode":{"languageId":"python"}},"outputs":[],"source":["# profilazione\n","\n","!nvprof ./sumMatrix 14"]},{"cell_type":"markdown","metadata":{"id":"R1V3b-a3RjL3"},"source":["# üî¥ TODO"]},{"cell_type":"markdown","source":["1. Definire la UMEM per ogni matrice\n","2. Effettuare la somma invocando il kernel\n","3. Analizzare i tempi e prestazioni con nvprof\n"],"metadata":{"id":"AWOfDng2kprv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Y52R0d3CA50","vscode":{"languageId":"python"}},"outputs":[],"source":["%%cuda --name sumMatrixUni.cu\n","\n","#include \"../GPUcomputing/utils/common.h\"\n","\n","void initialData(float *ip, const int size) {\n","  int i;\n","\n","  for (i = 0; i < size; i++)\n","    ip[i] = (float)( rand() & 0xFF ) / 10.0f;\n","  return;\n","}\n","\n","void sumMatrixOnHost(float *A, float *B, float *C, const int nx, const int ny) {\n","  float *ia = A;\n","  float *ib = B;\n","  float *ic = C;\n","\n","  for (int iy = 0; iy < ny; iy++) {\n","    for (int ix = 0; ix < nx; ix++)\n","      ic[ix] = ia[ix] + ib[ix];\n","\n","    ia += nx;\n","    ib += nx;\n","    ic += nx;\n","  }\n","  return;\n","}\n","\n","void checkResult(float *hostRef, float *gpuRef, const int N) {\n","  double epsilon = 1.0E-8;\n","  bool match = 1;\n","\n","  for (int i = 0; i < N; i++) {\n","    if (abs(hostRef[i] - gpuRef[i]) > epsilon) {\n","      match = 0;\n","      printf(\"host %f gpu %f\\n\", hostRef[i], gpuRef[i]);\n","      break;\n","    }\n","  }\n","\n","  if (!match)\n","    printf(\"Arrays do not match.\\n\\n\");\n","}\n","\n","//# grid 2D block 2D\n","__global__ void sumMatrixGPU(float *MatA, float *MatB, float *MatC, int nx, int ny) {\n","  unsigned int ix = threadIdx.x + blockIdx.x * blockDim.x;\n","  unsigned int iy = threadIdx.y + blockIdx.y * blockDim.y;\n","  unsigned int idx = iy * nx + ix;\n","\n","  if (ix < nx && iy < ny)\n","    MatC[idx] = MatA[idx] + MatB[idx];\n","}\n","\n","//# MAIN\n","int main(int argc, char **argv) {\n","  printf(\"%s Starting \", argv[0]);\n","\n","  // set up data size of matrix\n","  int nx, ny;\n","  int ishift = 14;\n","  if  (argc > 1) ishift = atoi(argv[1]);\n","  nx = ny = 1 << ishift;\n","\n","  int nxy = nx * ny;\n","  int nBytes = nxy * sizeof(float);\n","  printf(\"Matrix size: nx %d ny %d\\n\", nx, ny);\n","\n","  //# malloc unified host memory\n","  float *A, *B, *gpuRef;\n","  CHECK(cudaMallocManaged((void **)&A, nBytes));\n","  CHECK(cudaMallocManaged((void **)&B, nBytes));\n","  CHECK(cudaMallocManaged((void **)&gpuRef,  nBytes);  );\n","\n","  //# invoke kernel at host side\n","  int dimx = 32;\n","  int dimy = 32;\n","  dim3 block(dimx, dimy);\n","  dim3 grid((nx + block.x - 1) / block.x, (ny + block.y - 1) / block.y);\n","\n","  //# after warm-up, time with unified memory\n","  double iStart = seconds();\n","  sumMatrixGPU<<<grid, block>>>(A, B, gpuRef, nx, ny);\n","  CHECK(cudaDeviceSynchronize());\n","  double iElaps = seconds() - iStart;\n","  printf(\"sumMatrix on gpu :\\t %f sec <<<(%d,%d), (%d,%d)>>> \\n\", iElaps, grid.x, grid.y, block.x, block.y);\n","  //# check kernel error\n","  CHECK(cudaGetLastError());\n","\n","  //# free device global memory\n","  CHECK(cudaFree(A));\n","  CHECK(cudaFree(B));\n","  CHECK(cudaFree(gpuRef));\n","\n","  // reset device\n","  CHECK(cudaDeviceReset());\n","\n","  return (0);\n","}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0PSc9B9PDTWt","vscode":{"languageId":"python"}},"outputs":[],"source":["# Compilazione ed esecuzione\n","\n","!nvcc -arch=sm_70 src/sumMatrixUni.cu  -o sumMatrixUni\n","!./sumMatrixUni 14"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mfR471rze2p-","vscode":{"languageId":"python"}},"outputs":[],"source":["# profilazione\n","\n","!nvprof ./sumMatrixUni"]},{"cell_type":"markdown","metadata":{"id":"SOFMQZAkjlLW"},"source":["# ‚úÖ SoA vs AoS structs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v9nRkLgeB10A","vscode":{"languageId":"python"}},"outputs":[],"source":["%%cuda --name SoA.cu\n","\n","#include <stdint.h>\n","#include \"../GPUcomputing/utils/common.h\"\n","\n","#define N 1<<25\n","#define blocksize 1<<7\n","\n","struct SoA {\n","\tuint8_t r[N];\n","\tuint8_t g[N];\n","\tuint8_t b[N];\n","};\n","\n","\n","void initialize(SoA*, int);\n","void checkResult(SoA*, SoA*, int);\n","\n","/*\n"," * Riscala l'immagine al valore massimo [max] fissato\n"," */\n","__global__ void rescaleImg(SoA *img, const int max, const int n) {\n","\tunsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n","\tif (i < n) {\n","\t\tfloat r,g,b;\n","\t\tSoA *tmp = img;\n","\t\tr = max * (float)tmp->r[i]/255.0f;\n","\t\timg->r[i] = (uint8_t)r;\n","\t\tg = max * (float)tmp->g[i]/255.0f;\n","\t\timg->g[i] = (uint8_t)g;\n","\t\tb = max * (float)tmp->b[i]/255.0f;\n","\t\timg->b[i] = (uint8_t)b;\n","\t}\n","}\n","\n","/*\n"," * cancella un piano dell'immagine [plane = 'r' o 'g' o 'b'] fissato\n"," */\n","__global__ void deletePlane(SoA *img, const char plane, const int n) {\n","\tunsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n","\tif (i < n) {\n","\t\tswitch (plane) {\n","\t\tcase 'r':\n","\t\t\timg->r[i] = 0;\n","\t\t\tbreak;\n","\t\tcase 'g':\n","\t\t\timg->g[i] = 0;\n","\t\t\tbreak;\n","\t\tcase 'b':\n","\t\t\timg->b[i] = 0;\n","\t\t\tbreak;\n","\t\t}\n","\t}\n","}\n","\n","/*\n"," * setup device\n"," */\n","__global__ void warmup(SoA *img, const int max, const int n) {\n","\tunsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n","\tif (i < n) {\n","\t\tfloat r,g,b;\n","\t\tSoA *tmp = img;\n","\t\tr = max * (float)tmp->r[i]/255.0f;\n","\t\timg->r[i] = (uint8_t)r;\n","\t\tg = max * (float)tmp->g[i]/255.0f;\n","\t\timg->g[i] = (uint8_t)g;\n","\t\tb = max * (float)tmp->b[i]/255.0f;\n","\t\timg->b[i] = (uint8_t)b;\n","\t}\n","}\n","\n","/*\n"," * Legge da stdin quale kernel eseguire: 0 per rescaleImg, 1 per deletePlane\n"," */\n","int main(int argc, char **argv) {\n","\t// set up device\n","\tint dev = 0;\n","\tcudaDeviceProp deviceProp;\n","\tCHECK(cudaGetDeviceProperties(&deviceProp, dev));\n","\tprintf(\"%s test SoA at \", argv[0]);\n","\tprintf(\"device %d: %s \\n\", dev, deviceProp.name);\n","\tCHECK(cudaSetDevice(dev));\n","\n","\t// scelta del kernel da eseguire\n","\tint kernel = 0;\n","\tif (argc > 1) kernel = atoi(argv[1]);\n","\n","\t// allocate host memory\n","\tsize_t nBytes = sizeof(SoA);\n","\tSoA *img = (SoA *)malloc(nBytes);\n","\tSoA *new_img = (SoA *)malloc(nBytes);\n","\n","\t// initialize host array\n","\tinitialize(img, N);\n","\n","\t// allocate device memory\n","\tint n_elem = N;\n","\tSoA *d_img;\n","\tCHECK(cudaMalloc((void**)&d_img, nBytes));\n","\n","\t// copy data from host to device\n","\tCHECK(cudaMemcpy(d_img, img, nBytes, cudaMemcpyHostToDevice));\n","\n","\t// definizione max\n","\tint max = 128;\n","\tif (argc > 2) max = atoi(argv[2]);\n","\n","\t// configurazione per esecuzione\n","\tdim3 block (blocksize, 1);\n","\tdim3 grid  ((n_elem + block.x - 1) / block.x, 1);\n","\n","\t// kernel 1: warmup\n","\tdouble iStart = seconds();\n","\twarmup<<<1, 32>>>(d_img, max, 32);\n","\tCHECK(cudaDeviceSynchronize());\n","\tdouble iElaps = seconds() - iStart;\n","\tprintf(\"warmup<<< 1, 32 >>> elapsed %f sec\\n\",iElaps);\n","\tCHECK(cudaGetLastError());\n","\n","\t// kernel 2 rescaleImg o deletePlane\n","\tiStart = seconds();\n","\tif (kernel == 0) {\n","\t\trescaleImg<<<grid, block>>>(d_img, max, n_elem);\n","\t}\n","\telse {\n","\t\tdeletePlane<<<grid, block>>>(d_img, 'r', n_elem);\n","\t}\n","\n","\tCHECK(cudaDeviceSynchronize());\n","\tiElaps = seconds() - iStart;\n","\tCHECK(cudaMemcpy(new_img, d_img, nBytes, cudaMemcpyDeviceToHost));\n","\tCHECK(cudaGetLastError());\n","\n","\tif (kernel == 0) {\n","\t\tprintf(\"rescaleImg <<< %3d, %3d >>> elapsed %f sec\\n\", grid.x, block.x, iElaps);\n","\t}\n","\telse {\n","\t\tprintf(\"deletePlane <<< %3d, %3d >>> elapsed %f sec\\n\", grid.x, block.x, iElaps);\n","\t}\n","\n","\t//checkResult(img, new_img, n_elem);\n","\n","\t// free memories both host and device\n","\tCHECK(cudaFree(d_img));\n","\tfree(img);\n","\tfree(new_img);\n","\n","\t// reset device\n","\tCHECK(cudaDeviceReset());\n","\treturn EXIT_SUCCESS;\n","}\n","\n","void initialize(SoA *img,  int size) {\n","\tfor (int i = 0; i < size; i++) {\n","\t\timg->r[i] = rand() % 256;\n","\t\timg->g[i] = rand() % 256;\n","\t\timg->b[i] = rand() % 256;\n","\t}\n","\treturn;\n","}\n","\n","void checkResult(SoA *img, SoA *new_img, int n_elem) {\n","\tfor (int i = 0; i < n_elem; i+=1000)\n","\t\tprintf(\"img[%d] = (%d,%d,%d) -- new_img[%d] = (%d,%d,%d)\\n\",\n","\t\t\t\ti,img->r[i],img->g[i],img->b[i],i,new_img->r[i],new_img->g[i],new_img->b[i]);\n","\treturn;\n","}\n","\n","\n","void transposeHost(float *out, float *in, const int nx, const int ny) {\n","\tfor (int iy = 0; iy < ny; ++iy) {\n","\t\tfor (int ix = 0; ix < nx; ++ix) {\n","\t\t\tout[ix * ny + iy] = in[iy * nx + ix];\n","\t\t}\n","\t}\n","}\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wLxZjCx8bT3s","vscode":{"languageId":"python"}},"outputs":[],"source":["# Compilazione ed esecuzione\n","!nvcc -arch=sm_70  src/SoA.cu -o SoA\n","!./SoA 1"]},{"cell_type":"markdown","metadata":{"id":"cqQnULvlE2fu"},"source":["# üî¥ TODO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bwcTDn6ehJr_","vscode":{"languageId":"python"}},"outputs":[],"source":["%%cuda --name AoS.cu\n","\n","#include <stdint.h>\n","#include \"../GPUcomputing/utils/common.h\"\n","\n","#define N 1<<25\n","#define blocksize 128\n","\n","struct AoS {\n","\tuint8_t r;\n","\tuint8_t g;\n","\tuint8_t b;\n","};\n","\n","void initialize(AoS *, int);\n","void checkResult(AoS *, AoS *, int);\n","\n","/*\n"," * Riscala l'immagine al valore massimo [max] fissato\n"," */\n","__global__ void rescaleImg(AoS *img, const int max, const int n) {\n","\tunsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n","\tif (i < n) {\n","\t\tfloat r,g,b;\n","\t\tAoS tmp = img[i];\n","\t\tr = max * (float)tmp.r/255.0f;\n","\t\ttmp.r = (uint8_t)r;\n","\t\tg = max * (float)tmp.g/255.0f;\n","\t\ttmp.g = (uint8_t)g;\n","\t\tb = max * (float)tmp.b/255.0f;\n","\t\ttmp.b = (uint8_t)b;\n","\t\timg[i] = tmp;\n","\t}\n","}\n","\n","/*\n"," * cancella un piano dell'immagine [plane = 'r' o 'g' o 'b'] fissato\n"," */\n","__global__ void deletePlane(AoS *img, const char plane, const int n) {\n","\tunsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n","\tif (i < n) {\n","\t\tswitch (plane) {\n","\t\tcase 'r':\n","\t\t\timg[i].r = 0;\n","\t\t\tbreak;\n","\t\tcase 'g':\n","\t\t\timg[i].g = 0;\n","\t\t\tbreak;\n","\t\tcase 'b':\n","\t\t\timg[i].b = 0;\n","\t\t\tbreak;\n","\t\t}\n","\t}\n","}\n","\n","/*\n"," * setup device\n"," */\n","__global__ void warmup(AoS *img, const int max, const int n) {\n","\tunsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n","\tif (i < n) {\n","\t\tfloat r,g,b;\n","\t\tAoS tmp = img[i];\n","\t\tr = max * (float)tmp.r/255.0f;\n","\t\ttmp.r = (uint8_t)r;\n","\t\tg = max * (float)tmp.g/255.0f;\n","\t\ttmp.g = (uint8_t)g;\n","\t\tb = max * (float)tmp.b/255.0f;\n","\t\ttmp.b = (uint8_t)b;\n","\t\timg[i] = tmp;\n","\t}\n","}\n","\n","/*\n"," * Legge da stdin quale kernel eseguire: 0 per rescaleImg, 1 per deletePlane\n"," */\n","int main(int argc, char **argv) {\n","\t// set up device\n","\tint dev = 0;\n","\tcudaDeviceProp deviceProp;\n","\tCHECK(cudaGetDeviceProperties(&deviceProp, dev));\n","\tprintf(\"%s test AoS at \", argv[0]);\n","\tprintf(\"device %d: %s \\n\", dev, deviceProp.name);\n","\tCHECK(cudaSetDevice(dev));\n","\n","\t// scelta del kernel da eseguire\n","\tint kernel = 0;\n","\tif (argc > 1) kernel = atoi(argv[1]);\n","\n","\t// allocate host memory\n","\tint n_elem = N;\n","\tsize_t nBytes = n_elem * sizeof(AoS);\n","\tAoS *img = (AoS *)malloc(nBytes);\n","\tAoS *new_img = (AoS *)malloc(nBytes);\n","\n","\t// initialize host array\n","\tinitialize(img, N);\n","\n","\t// allocate device memory\n","\tAoS *d_img;\n","\tCHECK(cudaMalloc((void**)&d_img, nBytes));\n","\n","\t// copy data from host to device\n","\tCHECK(cudaMemcpy(d_img, img, nBytes, cudaMemcpyHostToDevice));\n","\n","\t// definizione max\n","\tint max = 128;\n","\tif (argc > 2) max = atoi(argv[2]);\n","\n","\t// configurazione per esecuzione\n","\tdim3 block (blocksize, 1);\n","\tdim3 grid  ((n_elem + block.x - 1) / block.x, 1);\n","\n","\t// kernel 1: warmup\n","\tdouble iStart = seconds();\n","\twarmup<<<1, 32>>>(d_img, max, 32);\n","\tCHECK(cudaDeviceSynchronize());\n","\tdouble iElaps = seconds() - iStart;\n","\tprintf(\"warmup<<< 1, 32 >>> elapsed %f sec\\n\",iElaps);\n","\tCHECK(cudaGetLastError());\n","\n","\t// kernel 2 rescaleImg o deletePlane\n","\tiStart = seconds();\n","\tif (kernel == 0) {\n","\t\trescaleImg<<<grid, block>>>(d_img, max, n_elem);\n","\t}\n","\telse {\n","\t\tdeletePlane<<<grid, block>>>(d_img, 'r', n_elem);\n","\t}\n","\tCHECK(cudaDeviceSynchronize());\n","\tiElaps = seconds() - iStart;\n","\tCHECK(cudaMemcpy(new_img, d_img, nBytes, cudaMemcpyDeviceToHost));\n","\tCHECK(cudaGetLastError());\n","\n","\tif (kernel == 0) {\n","\t\tprintf(\"rescaleImg <<< %3d, %3d >>> elapsed %f sec\\n\", grid.x, block.x, iElaps);\n","\t}\n","\telse {\n","\t\tprintf(\"deletePlane <<< %3d, %3d >>> elapsed %f sec\\n\", grid.x, block.x, iElaps);\n","\t}\n","\t//checkResult(img, new_img, n_elem);\n","\n","\t// free memories both host and device\n","\tCHECK(cudaFree(d_img));\n","\tfree(img);\n","\tfree(new_img);\n","\n","\t// reset device\n","\tCHECK(cudaDeviceReset());\n","\treturn EXIT_SUCCESS;\n","}\n","\n","void initialize(AoS *img,  int size) {\n","\tfor (int i = 0; i < size; i++) {\n","\t\timg[i].r = rand() % 256;\n","\t\timg[i].g = rand() % 256;\n","\t\timg[i].b = rand() % 256;\n","\t}\n","\treturn;\n","}\n","\n","void checkResult(AoS *img, AoS *new_img, int n_elem) {\n","\tfor (int i = 0; i < n_elem; i+=1000)\n","\t\tprintf(\"img[%d] = (%d,%d,%d) -- new_img[%d] = (%d,%d,%d)\\n\",\n","\t\t\t\ti,img[i].r,img[i].g,img[i].b,i,new_img[i].r,new_img[i].g,new_img[i].b);\n","\treturn;\n","}\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yiun00TE2wcE","vscode":{"languageId":"python"}},"outputs":[],"source":["# Compilazione ed esecuzione\n","!nvcc -arch=sm_70 src/AoS.cu -o AoS\n","!./AoS 1"]},{"cell_type":"markdown","metadata":{"id":"SbQlRthtJTQE"},"source":["# ‚úÖ Transpose"]},{"cell_type":"markdown","source":["# üî¥ TODO"],"metadata":{"id":"OdXa4C_wmR9j"}},{"cell_type":"markdown","source":["passi per la trasposizione con SMEM:\n","\n","1. definire la dim della SMEM pari alla dim del blocco\n","2. Il warp scrive i dati nella shared memory in row-major ordering evitando bank conflict sulle scritture. Ogni warp fa una letture coalescente dei dati in global memory\n","3. sincronizzare i thread\n"],"metadata":{"id":"cFH-7ue8uYNM"}},{"cell_type":"code","source":["%%cuda --name transposeSMEM.cu\n","\n","#include <stdio.h>\n","#include \"../GPUcomputing/utils/common.h\"\n","\n","\n","// Dimensione del blocco\n","#define BDIMX 32\n","#define BDIMY 32\n","\n","// macro x conversione indici lineari\n","#define INDEX(rows, cols, stride) (rows * stride + cols)\n","\n","// prototipi funzioni\n","void initialData(float*, const int);\n","void printData(float*, int, int);\n","void checkResult(float*, float*, int, int);\n","void transposeHost(float*, float*, const int, const int);\n","\n","/*\n"," * Kernel per il calcolo della matrice trasposta usando la shared memory\n"," */\n","__global__ void transposeSmem(float *out, float *in, int nrows, int ncols) {\n","\t// static shared memory\n","\t__shared__ float tile[BDIMY][BDIMX];\n","\n","\t// coordinate matrice originale\n","\t//unsigned int row = blockDim.y * blockIdx.y + threadIdx.y;\n","\t//unsigned int col = blockDim.x * blockIdx.x + threadIdx.x;\n","\n","\tunsigned int y = blockDim.y * blockIdx.y + threadIdx.y;\n","\tunsigned int x = blockDim.x * blockIdx.x + threadIdx.x;\n","\n","\t// trasferimento dati dalla global memory alla shared memory\n","\tif (y < nrows && x < ncols)\n","\t\ttile[threadIdx.y][threadIdx.x] = in[INDEX(y, x, ncols)];\n","\n","\t// thread synchronization\n","\t__syncthreads();\n","\n","\t// offset blocco trasposto\n","\ty = blockIdx.x * blockDim.x + threadIdx.y;\n","\tx = blockIdx.y * blockDim.y + threadIdx.x;\n","\n","\t// controlli invertiti nelle dim riga colonna\n","\tif (y < ncols && x < nrows)\n","\t\tout[y*nrows + x] = tile[threadIdx.x][threadIdx.y];\n","}\n","\n","//# naive: access data in rows\n","__global__ void copyRow(float *out, float *in, const int nrows,\tconst int ncols) {\n","\t// matrix coordinate (ix,iy)\n","\tunsigned int row = blockIdx.y * blockDim.y + threadIdx.y;\n","\tunsigned int col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// transpose with boundary test\n","\tif (row < nrows && col < ncols)\n","\t\tout[INDEX(col, row, nrows)] = in[INDEX(row, col, ncols)];\n","}\n","\n","//# naive: access data in cols\n","__global__ void copyCol(float *out, float *in, const int nrows,\tconst int ncols) {\n","\t// matrix coordinate (ix,iy)\n","\tunsigned int row = blockIdx.y * blockDim.y + threadIdx.y;\n","\tunsigned int col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// transpose with boundary test\n","\tif (row < nrows && col < ncols)\n","\t\tout[INDEX(row, col, ncols)] = in[INDEX(col, row, nrows)];\n","}\n","\n","//# MAIN\n","int main(int argc, char **argv) {\n","\t// set up device\n","\tint dev = 0;\n","\tcudaDeviceProp deviceProp;\n","\tCHECK(cudaGetDeviceProperties(&deviceProp, dev));\n","\tprintf(\"%s starting transpose at \", argv[0]);\n","\tprintf(\"device %d: %s \", dev, deviceProp.name);\n","\tCHECK(cudaSetDevice(dev));\n","\n","\tbool iprint = 0;\n","\n","\t// set up array size\n","\tint nrows = 1 << 14;\n","\tint ncols = 1 << 14;\n","\n","\tif (argc > 1)\n","\t\tiprint = atoi(argv[1]);\n","\tif (argc > 2)\n","\t\tnrows = atoi(argv[2]);\n","\tif (argc > 3)\n","\t\tncols = atoi(argv[3]);\n","\n","\tprintf(\"\\nMatrice con nrows = %d ncols = %d\\n\", nrows, ncols);\n","\tsize_t ncells = nrows * ncols;\n","\tsize_t nBytes = ncells * sizeof(float);\n","\n","\t// allocate host memory\n","\tfloat *A_h = (float *) malloc(nBytes);\n","\n","\t// Allocate Unified Memory ‚Äì accessible from CPU or GPU\n","\tfloat *A, *AT;\n","\tcudaMallocManaged(&A, nBytes);\n","\tcudaMallocManaged(&AT, nBytes);\n","\n","\t//  initialize host array\n","\tinitialData(A, nrows * ncols);\n","\tif (iprint)\n","\t\tprintData(A, nrows, ncols);\n","\n","\t//  transpose at host side\n","\ttransposeHost(A_h, A, nrows, ncols);\n","\n","\t\n","  printf(\"*** KERNEL: col copy  ***\\n\");\n","\t// tranpose gmem\n","  memset(AT, 0, nBytes);\n","  dim3 block(BDIMX, BDIMY, 1);\n","\tdim3 grid((ncols + block.x - 1) / block.x, (nrows + block.y - 1) / block.y, 1);\n","\tdouble iStart = seconds();\n","\tcopyCol<<<grid, block>>>(AT, A, nrows, ncols);\n","\tCHECK(cudaDeviceSynchronize());\n","\tdouble iElaps = seconds() - iStart;\n","\n","\t// check result\n","\tcheckResult(A_h, AT, nrows, ncols);\n","\n","\tdouble ibnd = 2 * ncells * sizeof(float) / 1e9 / iElaps;\n","\tprintf(\"col copy elapsed %f sec\\n <<< grid (%d,%d) block (%d,%d)>>> \"\n","\t\t\t\"effective bandwidth %f GB\\n\\n\", iElaps, grid.x, grid.y, block.x,\tblock.y, ibnd);\n","\n","  \n","  printf(\"*** KERNEL: row copy  ***\\n\");\n","\t// tranpose gmem\n","  memset(AT, 0, nBytes);\n","\n","\tiStart = seconds();\n","\tcopyRow<<<grid, block>>>(AT, A, nrows, ncols);\n","\tCHECK(cudaDeviceSynchronize());\n","\tiElaps = seconds() - iStart;\n","\n","\t// check result\n","\tcheckResult(A_h, AT, nrows, ncols);\n","\n","\tibnd = 2 * ncells * sizeof(float) / 1e9 / iElaps;\n","\tprintf(\"row copy elapsed %f sec\\n <<< grid (%d,%d) block (%d,%d)>>> \"\n","\t\t\t\"effective bandwidth %f GB\\n\\n\", iElaps, grid.x, grid.y, block.x,\tblock.y, ibnd);\n","\n","\n","\n","\tprintf(\"*** KERNEL: transposeSmem ***\\n\");\n","\t// tranpose smem\n","\tmemset(AT, 0, nBytes);\n","\n","\tiStart = seconds();\n","\ttransposeSmem<<<grid, block>>>(AT, A, nrows, ncols);\n","\tCHECK(cudaDeviceSynchronize());\n","\tdouble iElapsSMEM = seconds() - iStart;\n","\n","\tif (iprint)\n","\t\tprintData(AT, ncols, nrows);\n","\n","\tcheckResult(A_h, AT, nrows, ncols);\n","\tibnd = 2 * ncells * sizeof(float) / 1e9 / iElapsSMEM;\n","\tprintf(\"transposeSmem elapsed %f sec\\n <<< grid (%d,%d) block (%d,%d)>>> \"\n","\t\t\t\"effective bandwidth %f GB\\n\", iElapsSMEM, grid.x, grid.y, block.x,\n","\t\t\tblock.y, ibnd);\n","\n","\tprintf(\"SPEEDUP = %f\\n\", iElaps/iElapsSMEM);\n","\n","\t// free host and device memory\n","\tCHECK(cudaFree(A));\n","\tCHECK(cudaFree(AT));\n","\tfree(A_h);\n","\n","\t// reset device\n","\tCHECK(cudaDeviceReset());\n","\treturn EXIT_SUCCESS;\n","}\n","\n","void initialData(float *in, const int size) {\n","\tfor (int i = 0; i < size; i++)\n","\t\tin[i] = i; // (float)(rand()/INT_MAX) * 10.0f;\n","\treturn;\n","}\n","\n","void printData(float *in, int nrows, int ncols) {\n","\tfor (int i = 0; i < nrows; i++) {\n","\t\tfor (int j = 0; j < ncols; j++)\n","\t\t\tprintf(\"%3.0f \", in[INDEX(i, j, ncols)]);\n","\t\tprintf(\"\\n\");\n","\t}\n","}\n","\n","void transposeHost(float *out, float *in, const int nrows, const int ncols) {\n","\tfor (int iy = 0; iy < nrows; ++iy)\n","\t\tfor (int ix = 0; ix < ncols; ++ix)\n","\t\t\tout[INDEX(ix, iy, nrows)] = in[INDEX(iy, ix, ncols)];\n","}\n","\n","void checkResult(float *hostRef, float *gpuRef, int rows, int cols) {\n","\tdouble epsilon = 1.0E-8;\n","\tbool match = 1;\n","\n","\tfor (int i = 0; i < rows; i++) {\n","\t\tfor (int j = 0; j < cols; j++) {\n","\t\t\tint index = INDEX(i, j, cols);\n","\t\t\tif (abs(hostRef[index] - gpuRef[index]) > epsilon) {\n","\t\t\t\tmatch = 0;\n","\t\t\t\tprintf(\"different on (%d, %d) (offset=%d) element in \"\n","\t\t\t\t\t\t\"transposed matrix: host %f gpu %f\\n\", i, j, index,\n","\t\t\t\t\t\thostRef[index], gpuRef[index]);\n","\t\t\t\tbreak;\n","\t\t\t}\n","\t\t}\n","\t\tif (!match)\n","\t\t\tbreak;\n","\t}\n","\n","\tif (!match)\n","\t\tprintf(\"Arrays do not match.\\n\");\n","}\n"],"metadata":{"id":"HW2Vpd9il9m2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/transposeSMEM.cu -o transposeSMEM\n","!./transposeSMEM"],"metadata":{"id":"Jlc1ucK5mXWJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%cuda --name transposeSMEM.cu\n","\n","#include <stdio.h>\n","#include \"../GPUcomputing/utils/common.h\"\n","\n","\n","// Dimensione del blocco\n","#define BDIMX 32\n","#define BDIMY 32\n","\n","// macro x conversione indici lineari\n","#define INDEX(rows, cols, stride) (rows * stride + cols)\n","\n","// prototipi funzioni\n","void initialData(float*, const int);\n","void printData(float*, int, int);\n","void checkResult(float*, float*, int, int);\n","void transposeHost(float*, float*, const int, const int);\n","\n","/*\n"," * Kernel per il calcolo della matrice trasposta usando la shared memory\n"," */\n","__global__ void transposeSmem(float *out, float *in, int nrows, int ncols) {\n","\t// static shared memory\n","\t__shared__ float tile[BDIMY][BDIMX];\n","\n","\t// coordinate matrice originale\n","\t//unsigned int row = blockDim.y * blockIdx.y + threadIdx.y;\n","\t//unsigned int col = blockDim.x * blockIdx.x + threadIdx.x;\n","\n","\tunsigned int y = blockDim.y * blockIdx.y + threadIdx.y;\n","\tunsigned int x = blockDim.x * blockIdx.x + threadIdx.x;\n","\n","\t// trasferimento dati dalla global memory alla shared memory\n","\tif (y < nrows && x < ncols)\n","\t\ttile[threadIdx.y][threadIdx.x] = in[INDEX(y, x, ncols)];\n","\n","\t// thread synchronization\n","\t__syncthreads();\n","\n","\t// offset blocco trasposto\n","\ty = blockIdx.x * blockDim.x + threadIdx.y;\n","\tx = blockIdx.y * blockDim.y + threadIdx.x;\n","\n","\t// controlli invertiti nelle dim riga colonna\n","\tif (y < ncols && x < nrows)\n","\t\tout[y*nrows + x] = tile[threadIdx.x][threadIdx.y];\n","}\n","\n","//# naive: access data in rows\n","__global__ void copyRow(float *out, float *in, const int nrows,\tconst int ncols) {\n","\t// matrix coordinate (ix,iy)\n","\tunsigned int row = blockIdx.y * blockDim.y + threadIdx.y;\n","\tunsigned int col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// transpose with boundary test\n","\tif (row < nrows && col < ncols)\n","\t\tout[INDEX(col, row, nrows)] = in[INDEX(row, col, ncols)];\n","}\n","\n","//# naive: access data in cols\n","__global__ void copyCol(float *out, float *in, const int nrows,\tconst int ncols) {\n","\t// matrix coordinate (ix,iy)\n","\tunsigned int row = blockIdx.y * blockDim.y + threadIdx.y;\n","\tunsigned int col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// transpose with boundary test\n","\tif (row < nrows && col < ncols)\n","\t\tout[INDEX(row, col, ncols)] = in[INDEX(col, row, nrows)];\n","}\n","\n","//# MAIN\n","int main(int argc, char **argv) {\n","\t// set up device\n","\tint dev = 0;\n","\tcudaDeviceProp deviceProp;\n","\tCHECK(cudaGetDeviceProperties(&deviceProp, dev));\n","\tprintf(\"%s starting transpose at \", argv[0]);\n","\tprintf(\"device %d: %s \", dev, deviceProp.name);\n","\tCHECK(cudaSetDevice(dev));\n","\n","\tbool iprint = 0;\n","\n","\t// set up array size\n","\tint nrows = 1 << 14;\n","\tint ncols = 1 << 14;\n","\n","\tif (argc > 1)\n","\t\tiprint = atoi(argv[1]);\n","\tif (argc > 2)\n","\t\tnrows = atoi(argv[2]);\n","\tif (argc > 3)\n","\t\tncols = atoi(argv[3]);\n","\n","\tprintf(\"\\nMatrice con nrows = %d ncols = %d\\n\", nrows, ncols);\n","\tsize_t ncells = nrows * ncols;\n","\tsize_t nBytes = ncells * sizeof(float);\n","\n","\t// allocate host memory\n","\tfloat *A_h = (float *) malloc(nBytes);\n","  float *B_h = (float *) malloc(nBytes);\n","  float *AT_h = (float *) malloc(nBytes);\n","\n","\t// Allocate Unified Memory ‚Äì accessible from CPU or GPU\n","\tfloat *d_A, *d_AT;\n","\tCHECK(cudaMalloc((void** )&d_A, nBytes));\n","  CHECK(cudaMalloc((void** )&d_AT, nBytes));\n","\n","\t//  initialize host array\n","\tinitialData(A_h, nrows * ncols);\n","\tif (iprint)\n","\t\tprintData(A_h, nrows, ncols);\n","  \n","\t//  transpose at host side\n","\ttransposeHost(A_h, B_h, nrows, ncols);\n","\n","\t\n","  printf(\"*** KERNEL: col copy  ***\\n\");\n","\t// tranpose gmem\n","  CHECK(cudaMemcpy(d_A, A_h, nBytes, cudaMemcpyHostToDevice));\n","  dim3 block(BDIMX, BDIMY, 1);\n","\tdim3 grid((ncols + block.x - 1) / block.x, (nrows + block.y - 1) / block.y, 1);\n","\t\n","  double iStart = seconds();\n","\tcopyCol<<<grid, block>>>(d_AT, d_A, nrows, ncols);\n","\tCHECK(cudaDeviceSynchronize());\n","\tdouble iElaps = seconds() - iStart;\n","\n","\t// check result\n","\tCHECK(cudaMemcpy(AT_h, d_AT, nBytes, cudaMemcpyDeviceToHost));\n","\tcheckResult(A_h, B_h, nrows, ncols);\n","\n","\tdouble ibnd = 2 * ncells * sizeof(float) / 1e9 / iElaps;\n","\tprintf(\"col copy elapsed %f sec\\n <<< grid (%d,%d) block (%d,%d)>>> \"\n","\t\t\t\"effective bandwidth %f GB\\n\\n\", iElaps, grid.x, grid.y, block.x,\tblock.y, ibnd);\n","\n","  \n","  printf(\"*** KERNEL: row copy  ***\\n\");\n","\t// tranpose gmem\n","\n","\tiStart = seconds();\n","\tcopyRow<<<grid, block>>>(d_AT, d_A, nrows, ncols);\n","\tCHECK(cudaDeviceSynchronize());\n","\tiElaps = seconds() - iStart;\n","\n","\t// check result\n","  CHECK(cudaMemcpy(AT_h, d_AT, nBytes, cudaMemcpyDeviceToHost));\n","\tcheckResult(A_h, B_h, nrows, ncols);\n","\n","\tibnd = 2 * ncells * sizeof(float) / 1e9 / iElaps;\n","\tprintf(\"row copy elapsed %f sec\\n <<< grid (%d,%d) block (%d,%d)>>> \"\n","\t\t\t\"effective bandwidth %f GB\\n\\n\", iElaps, grid.x, grid.y, block.x,\tblock.y, ibnd);\n","\n","\n","\tprintf(\"*** KERNEL: transposeSmem ***\\n\");\n","\t// tranpose smem\n","\n","\tiStart = seconds();\n","\ttransposeSmem<<<grid, block>>>(d_AT, d_A, nrows, ncols);\n","\tCHECK(cudaDeviceSynchronize());\n","\tdouble iElapsSMEM = seconds() - iStart;\n","\n","\tCHECK(cudaMemcpy(AT_h, d_AT, nBytes, cudaMemcpyDeviceToHost));\n","\tcheckResult(A_h, B_h, nrows, ncols);\n","  \n","\tibnd = 2 * ncells * sizeof(float) / 1e9 / iElapsSMEM;\n","\tprintf(\"transposeSmem elapsed %f sec\\n <<< grid (%d,%d) block (%d,%d)>>> \"\n","\t\t\t\"effective bandwidth %f GB\\n\", iElapsSMEM, grid.x, grid.y, block.x,\n","\t\t\tblock.y, ibnd);\n","\n","\tprintf(\"SPEEDUP = %f\\n\", iElaps/iElapsSMEM);\n","\n","\t// free host and device memory\n","\tCHECK(cudaFree(d_A));\n","\tCHECK(cudaFree(d_AT));\n","\tfree(A_h);\n","\n","\t// reset device\n","\tCHECK(cudaDeviceReset());\n","\treturn EXIT_SUCCESS;\n","}\n","\n","void initialData(float *in, const int size) {\n","\tfor (int i = 0; i < size; i++)\n","\t\tin[i] = i; // (float)(rand()/INT_MAX) * 10.0f;\n","\treturn;\n","}\n","\n","void printData(float *in, int nrows, int ncols) {\n","\tfor (int i = 0; i < nrows; i++) {\n","\t\tfor (int j = 0; j < ncols; j++)\n","\t\t\tprintf(\"%3.0f \", in[INDEX(i, j, ncols)]);\n","\t\tprintf(\"\\n\");\n","\t}\n","}\n","\n","void transposeHost(float *out, float *in, const int nrows, const int ncols) {\n","\tfor (int iy = 0; iy < nrows; ++iy)\n","\t\tfor (int ix = 0; ix < ncols; ++ix)\n","\t\t\tout[INDEX(ix, iy, nrows)] = in[INDEX(iy, ix, ncols)];\n","}\n","\n","void checkResult(float *hostRef, float *gpuRef, int rows, int cols) {\n","\tdouble epsilon = 1.0E-8;\n","\tbool match = 1;\n","\n","\tfor (int i = 0; i < rows; i++) {\n","\t\tfor (int j = 0; j < cols; j++) {\n","\t\t\tint index = INDEX(i, j, cols);\n","\t\t\tif (abs(hostRef[index] - gpuRef[index]) > epsilon) {\n","\t\t\t\tmatch = 0;\n","\t\t\t\tprintf(\"different on (%d, %d) (offset=%d) element in \"\n","\t\t\t\t\t\t\"transposed matrix: host %f gpu %f\\n\", i, j, index,\n","\t\t\t\t\t\thostRef[index], gpuRef[index]);\n","\t\t\t\tbreak;\n","\t\t\t}\n","\t\t}\n","\t\tif (!match)\n","\t\t\tbreak;\n","\t}\n","\n","\tif (!match)\n","\t\tprintf(\"Arrays do not match.\\n\");\n","}\n"],"metadata":{"id":"xBFQxvgumfdu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvcc -arch=sm_75 src/transposeSMEM.cu -o transposeSMEM\n","!./transposeSMEM"],"metadata":{"id":"_WzHYqto-TfG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"29N1S2QUvjbm"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["WoJbB3T5Vkw-","zs_a5Vuimily","iUYP4kCJhEIx","R1V3b-a3RjL3"],"private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}